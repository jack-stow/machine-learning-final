{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are some common modules used in machine learning for importing/modifying data as well as visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3080, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import dask.dataframe as dd\n",
    "import modin.pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the encoded_df_blanks_as_na.csv file for our features dataframe. features_df contains the independent variables used to train our machine learning model. These are the characterisitcs of the data that our model will analyze to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 22:14:32,396\tINFO worker.py:1821 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- A one-hot encoded matrix of features, with 11599 person names (nam_id_XXXX)\n",
    "    and 4784 place names (geo_id_XXXX) as columns.\n",
    "- The first column contains unique text_id corresponding to individual texts.\n",
    "'''\n",
    "features_df = pd.read_csv('encoded_df_blanks_as_na.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the 20240103_texts_with_dates.csv file for our labels dataframe. We then preview the dataframe. labels_df contains the dependent variables (target values) our model is trying to predict. These are the outputs corresponding to observations in the features_df. y1 and y1 willl serve as the ground truth our model learns to predict.\n",
    "\n",
    "We later switched to the modin pandas module to leverage multithreading, as it took forever to run without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tex_id</th>\n",
       "      <th>geotex_id</th>\n",
       "      <th>written</th>\n",
       "      <th>found</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>language_text</th>\n",
       "      <th>material_text</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>remark</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12042</td>\n",
       "      <td>8388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y1 = earliest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12054</td>\n",
       "      <td>8391</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y2 = latest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12063</td>\n",
       "      <td>8393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if y1 = y2, then we are certain of the date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12064</td>\n",
       "      <td>8394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17239</td>\n",
       "      <td>9507</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>967240</td>\n",
       "      <td>1021239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-263</td>\n",
       "      <td>-229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29246</th>\n",
       "      <td>5910</td>\n",
       "      <td>5438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-148</td>\n",
       "      <td>-148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29247</th>\n",
       "      <td>3506</td>\n",
       "      <td>4066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-150</td>\n",
       "      <td>-150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29248</th>\n",
       "      <td>7491</td>\n",
       "      <td>6778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29249</th>\n",
       "      <td>7491</td>\n",
       "      <td>77875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29250 rows x 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tex_id  geotex_id  written  found  geo_id    language_text  \\\n",
       "0       12042       8388        1      1    1008            Greek   \n",
       "1       12054       8391        1      1    1008            Greek   \n",
       "2       12063       8393        1      1    1008            Greek   \n",
       "3       12064       8394        1      1    1008            Greek   \n",
       "4       17239       9507        0      1    1008            Greek   \n",
       "...       ...        ...      ...    ...     ...              ...   \n",
       "29245  967240    1021239        1      0     332            Greek   \n",
       "29246    5910       5438        1      1    1344            Greek   \n",
       "29247    3506       4066        1      1    1344  Demotic / Greek   \n",
       "29248    7491       6778        0      1     720  Demotic / Greek   \n",
       "29249    7491      77875        1      0    1780  Demotic / Greek   \n",
       "\n",
       "      material_text   y1   y2 remark  Unnamed: 10  \\\n",
       "0           papyrus  117  118    NaN          NaN   \n",
       "1           papyrus  119  119    NaN          NaN   \n",
       "2           papyrus   96   98    NaN          NaN   \n",
       "3           papyrus  131  131    NaN          NaN   \n",
       "4           papyrus  108  108    NaN          NaN   \n",
       "...             ...  ...  ...    ...          ...   \n",
       "29245       papyrus -263 -229    NaN          NaN   \n",
       "29246       papyrus -148 -148    NaN          NaN   \n",
       "29247       papyrus -150 -150    NaN          NaN   \n",
       "29248       papyrus -236 -236    new          NaN   \n",
       "29249       papyrus -236 -236    new          NaN   \n",
       "\n",
       "                                         Unnamed: 11  \n",
       "0           - y1 = earliest possible year of writing  \n",
       "1             - y2 = latest possible year of writing  \n",
       "2      [if y1 = y2, then we are certain of the date]  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "...                                              ...  \n",
       "29245                                            NaN  \n",
       "29246                                            NaN  \n",
       "29247                                            NaN  \n",
       "29248                                            NaN  \n",
       "29249                                            NaN  \n",
       "\n",
       "[29250 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- text_id: same as the text id in the features dataset\n",
    "- y1: The earliest possible date the text was written.\n",
    "- y2: The latest possible date the text was written.\n",
    "- If y1 and y2 are equal, the date of writing is known with certainty.\n",
    "'''\n",
    "\n",
    "labels_df = pd.read_csv('20240103_texts_with_dates.csv')\n",
    "\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding**\n",
    "\n",
    "Looking at the features file, it is one-hot encoded. One-hot encoding is a method used to represent categorical data as binary values. It is commonly used to convert non-numerical data into a format that algorithms can process effectively as well as avoiding implicit ordinal relationships. It works by identifying all unique categories and creating binary columns for all of them. We assign 1 to the column corresponding to the presence of that data and 0 for all other related columns.\n",
    "\n",
    "This file was not fully one-hot encoded at first. It had a 1 for all values, but a NULL for the absence of features. All NULL values should be filled with a 0 to represent the absence of that feature and clean the data more.\n",
    "\n",
    "After filling all NaN columns, we preview the first 5 columns of the features_df. We are setting the text_id as the index of the features_df because although it is a column, it is not actually a feature but rather a unique identifier for each row.\n",
    "\n",
    "Lastly, we print information about the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       text_id  nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "29069   170048            0.0             0.0           0.0           0.0   \n",
      "21831    50877            0.0             0.0           0.0           0.0   \n",
      "29892   832085            0.0             0.0           1.0           0.0   \n",
      "22718    55433            0.0             0.0           0.0           0.0   \n",
      "7071     13090            0.0             0.0           0.0           0.0   \n",
      "\n",
      "       nam_id_2227.0  nam_id_726.0  nam_id_761.0  nam_id_731.0  nam_id_1246.0  \n",
      "29069            0.0           0.0           0.0           0.0            0.0  \n",
      "21831            0.0           0.0           0.0           0.0            0.0  \n",
      "29892            0.0           0.0           0.0           0.0            0.0  \n",
      "22718            0.0           0.0           0.0           0.0            0.0  \n",
      "7071             0.0           0.0           0.0           0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Printing information about the features dataset\n",
    "features_df = features_df.fillna(0)\n",
    "\n",
    "# gets 5 random rows, only first 10 columns\n",
    "print(features_df.sample(5).iloc[:, :10])\n",
    "\n",
    "features_df.set_index('text_id', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 30324\n",
      "Number of columns: 16383\n",
      "Minimum y1: -1292.0\n",
      "Maximum y2: 1099.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `Series.hist_series` is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "Please refer to https://modin.readthedocs.io/en/stable/supported_apis/defaulting_to_pandas.html for explanation.\n",
      "UserWarning: `Series.hist_series` is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Print the number of rows and columns\n",
    "print(f'Number of rows: {features_df.shape[0]}')\n",
    "print(f'Number of columns: {features_df.shape[1]}')\n",
    "\n",
    "# Explore target labels (y1 and y2) distribution\n",
    "print(f'Minimum y1: {labels_df[\"y1\"].min()}')\n",
    "print(f'Maximum y2: {labels_df[\"y2\"].max()}')\n",
    "\n",
    "# Distribution of y1 and y2 (histograms or value counts)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "labels_df['y1'].hist(bins=20)\n",
    "plt.title('Distribution of y1 (Earliest Date)')\n",
    "plt.subplot(1, 2, 2)\n",
    "labels_df['y2'].hist(bins=20)\n",
    "plt.title('Distribution of y2 (Latest Date)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Data preprocessing is the stage in machine learning where raw data is cleaned and prepared for analysis or model training. Firstly, we cleaned the data by filling NULLS with 0's for one-hot encoding. Now, we are removing duplicate rows from features_df and labels_df to ensure the dataset is clean, consistent, and free from redundancy. Duplicates can influence bias which would negatively impact the performance of our model.\n",
    "\n",
    "<font color='red'>Remove outliers?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nam_id_1057.0     0\n",
      "nam_id_19683.0    0\n",
      "nam_id_356.0      0\n",
      "nam_id_904.0      0\n",
      "nam_id_2227.0     0\n",
      "                 ..\n",
      "nam_id_27140.0    0\n",
      "nam_id_43504.0    0\n",
      "nam_id_22369.0    0\n",
      "nam_id_26361.0    0\n",
      "geo_id_2078       0\n",
      "Length: 16383, dtype: int64\n",
      "         nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "text_id                                                              \n",
      "1                  1.0             1.0           1.0           1.0   \n",
      "2                  0.0             0.0           1.0           1.0   \n",
      "3                  0.0             0.0           1.0           0.0   \n",
      "4                  0.0             0.0           1.0           0.0   \n",
      "5                  0.0             0.0           1.0           0.0   \n",
      "\n",
      "         nam_id_2227.0  nam_id_726.0  nam_id_761.0  nam_id_731.0  \\\n",
      "text_id                                                            \n",
      "1                  1.0           1.0           1.0           1.0   \n",
      "2                  0.0           0.0           1.0           0.0   \n",
      "3                  0.0           0.0           0.0           0.0   \n",
      "4                  0.0           0.0           0.0           0.0   \n",
      "5                  0.0           0.0           0.0           0.0   \n",
      "\n",
      "         nam_id_1246.0  nam_id_8095.0  ...  nam_id_26358.0  nam_id_26392.0  \\\n",
      "text_id                                ...                                   \n",
      "1                  1.0            1.0  ...             0.0             0.0   \n",
      "2                  0.0            1.0  ...             0.0             0.0   \n",
      "3                  0.0            0.0  ...             0.0             0.0   \n",
      "4                  0.0            0.0  ...             0.0             0.0   \n",
      "5                  0.0            0.0  ...             0.0             0.0   \n",
      "\n",
      "         geo_id_7893  geo_id_7670  geo_id_7007  nam_id_27140.0  \\\n",
      "text_id                                                          \n",
      "1                0.0          0.0          0.0             0.0   \n",
      "2                0.0          0.0          0.0             0.0   \n",
      "3                0.0          0.0          0.0             0.0   \n",
      "4                0.0          0.0          0.0             0.0   \n",
      "5                0.0          0.0          0.0             0.0   \n",
      "\n",
      "         nam_id_43504.0  nam_id_22369.0  nam_id_26361.0  geo_id_2078  \n",
      "text_id                                                               \n",
      "1                   0.0             0.0             0.0          0.0  \n",
      "2                   0.0             0.0             0.0          0.0  \n",
      "3                   0.0             0.0             0.0          0.0  \n",
      "4                   0.0             0.0             0.0          0.0  \n",
      "5                   0.0             0.0             0.0          0.0  \n",
      "\n",
      "[5 rows x 16383 columns]\n",
      "   tex_id  geotex_id  written  found  geo_id language_text material_text   y1  \\\n",
      "0   12042       8388        1      1    1008         Greek       papyrus  117   \n",
      "1   12054       8391        1      1    1008         Greek       papyrus  119   \n",
      "2   12063       8393        1      1    1008         Greek       papyrus   96   \n",
      "3   12064       8394        1      1    1008         Greek       papyrus  131   \n",
      "4   17239       9507        0      1    1008         Greek       papyrus  108   \n",
      "\n",
      "    y2  remark  Unnamed: 10                                    Unnamed: 11  \n",
      "0  118     NaN          NaN       - y1 = earliest possible year of writing  \n",
      "1  119     NaN          NaN         - y2 = latest possible year of writing  \n",
      "2   98     NaN          NaN  [if y1 = y2, then we are certain of the date]  \n",
      "3  131     NaN          NaN                                            NaN  \n",
      "4  108     NaN          NaN                                            NaN  \n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining missing values after filling NaNs\n",
    "print(features_df.isnull().sum())\n",
    "\n",
    "# Inspect the first few rows of both dataframes to verify alignment\n",
    "print(features_df.head())\n",
    "print(labels_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the distribution of person and place features. \n",
    "This will help us understand how these features relate to the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "count   30324.000000    30324.000000  30324.000000  30324.000000   \n",
      "mean        0.000857        0.000758      0.088643      0.016291   \n",
      "std         0.029269        0.027530      0.284232      0.126593   \n",
      "min         0.000000        0.000000      0.000000      0.000000   \n",
      "25%         0.000000        0.000000      0.000000      0.000000   \n",
      "50%         0.000000        0.000000      0.000000      0.000000   \n",
      "75%         0.000000        0.000000      0.000000      0.000000   \n",
      "max         1.000000        1.000000      1.000000      1.000000   \n",
      "\n",
      "       nam_id_2227.0  \n",
      "count   30324.000000  \n",
      "mean        0.011377  \n",
      "std         0.106057  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "        geo_id_1704   geo_id_1628    geo_id_100   geo_id_2982   geo_id_2023\n",
      "count  30324.000000  30324.000000  30324.000000  30324.000000  30324.000000\n",
      "mean       0.024139      0.010322      0.062294      0.023513      0.007156\n",
      "std        0.153484      0.101073      0.241693      0.151528      0.084292\n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "75%        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "max        1.000000      1.000000      1.000000      1.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explore distribution of features (first few person/place columns)\n",
    "person_columns = [col for col in features_df.columns if col.startswith('nam_id')]\n",
    "place_columns = [col for col in features_df.columns if col.startswith('geo_id')]\n",
    "\n",
    "# Show summary statistics of the first few person and place features\n",
    "print(features_df[person_columns[:5]].describe())\n",
    "print(features_df[place_columns[:5]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_uncertain\n",
      "True     16698\n",
      "False    12552\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `df.groupby(categorical_by, sort=False)` implementation has mismatches with pandas:\n",
      "the groupby keys will be sorted anyway, although the 'sort=False' was passed. See the following issue for more details: https://github.com/modin-project/modin/issues/3571.\n"
     ]
    }
   ],
   "source": [
    "# Create a new column to flag if the date is uncertain (y1 != y2)\n",
    "labels_df['date_uncertain'] = labels_df['y1'] != labels_df['y2']\n",
    "\n",
    "# Check the distribution of the uncertain dates\n",
    "print(labels_df['date_uncertain'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 29250\n",
      "Number of duplicate rows: 0\n",
      "Number of rows after removing duplicates: 29250\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {labels_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {labels_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "labels_df = labels_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {labels_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are further preparing the data by removing features with low variance. Variance refers to how much the values in a feature vary. Features with very little variation don't provide value for prediction because there aren't enough differences in the data for the model to learn any patterns. By removing these low-variance features, we simplify the model, reduce overfitting, and improve computational efficiency.\n",
    "\n",
    "The threshold we chose is arbitrary since it is a hyper parameter. We tuned our models by setting various thresholds and seeing which one resulted in the lowest MAE. A threshold of 0.001 produced our best result.\n",
    "\n",
    "fit_transform calculates the variance of each feature in features_df and removes features whose variance is below our specified threshold.\n",
    "\n",
    "Printing our original and reduced shape shows how we are able to reduce our dimensions by 15,038. There were diminishing returns with feature reduction. After a certain point, it was better to have more features. Originally, we reduced to 143 columns but 1345 performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (30324, 16383)\n",
      "Reduced shape: (30324, 1309)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.001)  # Adjust threshold as needed\n",
    "features_reduced_df = selector.fit_transform(features_df)\n",
    "\n",
    "print(f\"Original shape: {features_df.shape}\")\n",
    "print(f\"Reduced shape: {features_reduced_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Singular Value Decomposition (SVD) for dimensionality reduction because it is good at reducing the number of features in high-dimensional datasets and sparse matrixes in particular. Our data was an extremely sparse matrix. SVD reduces the number of features while retaining the most important information. We chose 1000 principal components to keep because it performed well after testing various values for this hyperparameter. The explained variance ratio shows that even after reducing our features by over 15000, we still manage to capture 95% of the variance in 1000 components.\n",
    "\n",
    "<font color='red'>EXPLAIN SVD MORE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY SVD INSTEAD OF PCA?**\n",
    "\n",
    "We chose SVD over PCA for several reasons such as nonlinear relationships, sparse matrixes, and large datasets. SVD is more flexible in handling non-linear relationships and can be used as a general-purpose dimensionality reduction technique. We were able to determine our data was non-linear by comparing the results of models based on linear and nonlinear approaches. Our nonlinear models outperformed our linear models by far. SVD is also useful when dealing with sparse data, which is the case for our data. SVD seemed like a better option due to the size of our dataset as well. PCA involves calculating a covariance matrix which would take longer than SVD to process.\n",
    "\n",
    "When trying n_components significantly over 1000, often variance retention would be >0.9999, which led to worse model results due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (30324, 1309)\n",
      "Reduced shape: (30324, 1000)\n",
      "0.9577358354531391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "# svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "features_svd = svd.fit_transform(features_reduced_df)\n",
    "\n",
    "print(f\"Original shape: {features_reduced_df.shape}\")\n",
    "print(f\"Reduced shape: {features_svd.shape}\")\n",
    "\n",
    "#print(svd.explained_variance_ratio_)\n",
    "print(sum(svd.explained_variance_ratio_))  # Total variance retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell converts our features_svd array back into a dataframe features_svd_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "# Assuming reduced_features is your NumPy array from SVD or Variance Thresholding\n",
    "# features_df['text_id'] contains the IDs you need to reattach\n",
    "features_df.reset_index(inplace=True)\n",
    "features_svd_df = pd.DataFrame(features_svd, columns=[f'feature_{i}' for i in range(features_svd.shape[1])])\n",
    "features_svd_df['text_id'] = features_df['text_id'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important cells because it prepares our training set. Our original label and features files have around 20k and 30k rows respectively. Not every row of data in the features dataset has a corresponding row of data in the labels dataset and vice versa. In order to train our model, we need to give it data that it already knows the answers for - the ground truths. This is the metric we will compare our predictions against to guage the performance of our models. Although we cleaned our data for duplicates before, we wanted to ensure the data was thoroughly prepared as we inch closer to training our model.\n",
    "\n",
    "After removing duplicates and data that didn't appear in both the label and features datasets, we were left with 8565 datapoints with corresponding labels. This is the data we use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text IDs missing in labels: 21216\n",
      "Text IDs missing in features: 11838\n",
      "Updated features shape: (9108, 1001)\n",
      "Updated labels shape: (13680, 13)\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 0\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 4572\n",
      "Aligned features shape: (9108, 1001)\n",
      "Aligned labels shape: (9108, 13)\n"
     ]
    }
   ],
   "source": [
    "# Get text_ids in each dataset\n",
    "features_text_ids = set(features_svd_df['text_id'])\n",
    "labels_text_ids = set(labels_df['tex_id'])\n",
    "\n",
    "# Find unmatched text_ids\n",
    "missing_in_labels = features_text_ids - labels_text_ids\n",
    "missing_in_features = labels_text_ids - features_text_ids\n",
    "\n",
    "print(f\"Text IDs missing in labels: {len(missing_in_labels)}\")\n",
    "print(f\"Text IDs missing in features: {len(missing_in_features)}\")\n",
    "\n",
    "# Keep only common text_ids\n",
    "common_text_ids = features_text_ids & labels_text_ids\n",
    "\n",
    "# Filter features and labels datasets\n",
    "features_common_df = features_svd_df[features_svd_df['text_id'].isin(common_text_ids)]\n",
    "labels_common_df = labels_df[labels_df['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "# Check the updated shapes\n",
    "print(f\"Updated features shape: {features_common_df.shape}\")\n",
    "print(f\"Updated labels shape: {labels_common_df.shape}\")\n",
    "\n",
    "# Check for duplicated rows\n",
    "duplicate_rows = features_common_df.duplicated()\n",
    "\n",
    "# Check for duplicated text_ids\n",
    "duplicate_text_ids = features_common_df['text_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_text_ids.sum()}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_label_rows = labels_common_df.duplicated()\n",
    "\n",
    "# Check for duplicate text_ids\n",
    "duplicate_label_text_ids = labels_common_df['tex_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_label_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_label_text_ids.sum()}\")\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "features = features_common_df.drop_duplicates(subset='text_id', keep='first')\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "labels = labels_common_df.drop_duplicates(subset='tex_id', keep='first')\n",
    "\n",
    "\n",
    "# Ensure alignment of text_ids between features and labels\n",
    "common_text_ids = set(features['text_id']) & set(labels['tex_id'])\n",
    "\n",
    "# Filter again if necessary\n",
    "features_filtered_df = features[features['text_id'].isin(common_text_ids)]\n",
    "labels_filtered_df = labels[labels['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "print(f\"Aligned features shape: {features_filtered_df.shape}\")\n",
    "print(f\"Aligned labels shape: {labels_filtered_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the index of our dataframes to the text_ids since are not considered features, but rather unique identifiers of each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_df.set_index('text_id', inplace=True)\n",
    "\n",
    "labels_filtered_df.rename(columns={'tex_id': 'text_id'}, inplace=True)\n",
    "labels_filtered_df.set_index('text_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task involves predicting the year a text was written, retaining the other ground truth labels was unnecessary. Removing them avoids confusion and ensures our process is focused solely on the data relevant to our task.\n",
    "\n",
    "<font color=\"red\"> Should we convert the other labels into features before dimension reduction?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12042</th>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12064</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17239</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703104</th>\n",
       "      <td>-250</td>\n",
       "      <td>-175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703317</th>\n",
       "      <td>-263</td>\n",
       "      <td>-229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>-148</td>\n",
       "      <td>-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>-150</td>\n",
       "      <td>-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9108 rows x 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y1   y2\n",
       "text_id          \n",
       "12042    117  118\n",
       "12054    119  119\n",
       "12063     96   98\n",
       "12064    131  131\n",
       "17239    108  108\n",
       "...      ...  ...\n",
       "703104  -250 -175\n",
       "703317  -263 -229\n",
       "5910    -148 -148\n",
       "3506    -150 -150\n",
       "7491    -236 -236\n",
       "\n",
       "[9108 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_final_df = labels_filtered_df[['y1','y2']]\n",
    "labels_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this step is redundant, we wanted to ensure all of our datapoints were merged with a corresponding label via an inner join. This could be skipped altogether since we are separating the y label from the X features when training our model, but having it as one dataframe in the beginning aligned with our approaches throughout the course.\n",
    "\n",
    "<font color=\"red\">Can we replace the cell above where we filter for records in both features_df and labels_df by only using this method?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9108, 1002)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge on text_id\n",
    "merged_df = features_filtered_df.merge(labels_final_df, on='text_id', how='inner')\n",
    "\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification or Regression?</h1>\n",
    "\n",
    "Here we are creating two variables that indicate how many rows exist where y1=y2 and y1!=y2 and displaying those results.\n",
    "\n",
    "Since we have more rows where an exact year is known (5,239) we believe regression was the better approach because predicting an exact year (a continuous variable) aligns with the strengths of regression.\n",
    "\n",
    "If we had more cases where y1!=y2, then there would be more uncertainty about the exact date of our texts, justifying the use of a classifcation model where we simplify the task by binning the ranges into predifined categories (100-199AD, 200-299AD).\n",
    "\n",
    "Based on the context of our training data, we believed regression was the better approach because the majority of our ground truths were a single continous year vs a range of years.\n",
    "\n",
    "<b>We chose Regression</b>\n",
    "\n",
    "<font color=\"red\"> Should we create classification models anyway for comparison?</font>\n",
    "\n",
    "<font color=\"red\">Should we combine classifcation and regression models into one model?\n",
    "ex. classifcation followed by regression\n",
    "ex. regression followed by classification\n",
    "ex. multi-output neural network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where y1 = y2: 5568\n",
      "Number of rows where y1 != y2: 3540\n"
     ]
    }
   ],
   "source": [
    "equal_rows = merged_df[merged_df['y1'] == merged_df['y2']].shape[0]\n",
    "unequal_rows = merged_df[merged_df['y1'] != merged_df['y2']].shape[0]\n",
    "\n",
    "print(f\"Number of rows where y1 = y2: {equal_rows}\")\n",
    "print(f\"Number of rows where y1 != y2: {unequal_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose regression, we created a target column that handles both cases when y1 equals y2 (continuous) and when y1 didn't equal y2 (range of years).\n",
    "\n",
    "y_target simply became y1 if they were equal\n",
    "y_target took the midpoint of y1 and y2 if they weren't equal. The midpoint serves as a reasonable estimate for a range when an exact year isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "text_id                                                                     \n",
      "1         0.407687  -0.339652   0.577915   0.236062   0.111234  -0.021248   \n",
      "2         1.197963  -0.464093   0.788213  -0.108202   0.918461   0.902399   \n",
      "3         0.671469  -0.424639   0.356897  -0.003133   0.223691   0.077651   \n",
      "4         1.230311  -0.612838   0.707190  -0.165581   0.932456   0.654450   \n",
      "5         0.405977  -0.311168   0.482294   0.171589   0.129961  -0.034979   \n",
      "\n",
      "         feature_6  feature_7  feature_8  feature_9  ...  feature_994  \\\n",
      "text_id                                              ...                \n",
      "1        -0.153746  -0.007755   0.013452   0.273916  ...    -0.068806   \n",
      "2        -0.405444  -0.100074  -0.975100  -0.009527  ...     0.008823   \n",
      "3         0.235716  -0.357372   0.663473   0.342369  ...     0.023752   \n",
      "4        -0.259247   0.097634  -0.439642  -0.248355  ...    -0.023407   \n",
      "5        -0.137464   0.036390  -0.024269   0.261850  ...     0.004935   \n",
      "\n",
      "         feature_995  feature_996  feature_997  feature_998  feature_999  \\\n",
      "text_id                                                                    \n",
      "1           0.026521    -0.027352    -0.014985     0.023677    -0.052546   \n",
      "2          -0.017260     0.007761    -0.025403     0.028005     0.018981   \n",
      "3          -0.041222     0.011726     0.030228     0.019802    -0.023849   \n",
      "4           0.094549     0.000708     0.061064    -0.040254     0.001183   \n",
      "5          -0.012496    -0.007738    -0.005672     0.009419     0.006109   \n",
      "\n",
      "            y1     y2  y_target  date_uncertain  \n",
      "text_id                                          \n",
      "1       -124.0 -124.0    -124.0           False  \n",
      "2       -112.0 -112.0    -112.0           False  \n",
      "3       -109.0 -109.0    -109.0           False  \n",
      "4       -108.0 -108.0    -108.0           False  \n",
      "5       -106.0 -106.0    -106.0           False  \n",
      "\n",
      "[5 rows x 1004 columns]\n"
     ]
    }
   ],
   "source": [
    "merged_df['y_target'] = merged_df.apply(\n",
    "    lambda row: row['y1'] if row['y1'] == row['y2'] else (row['y1'] + row['y2']) / 2,\n",
    "    axis=1\n",
    ")\n",
    "# Now, let's add 'date_uncertain' as a feature\n",
    "merged_df['date_uncertain'] = merged_df['y1'] != merged_df['y2']\n",
    "\n",
    "# Check the updated dataframe\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our new ground truth y_target, we no longer needed the labels y1 and y2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_992</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y_target</th>\n",
       "      <th>date_uncertain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.407687</td>\n",
       "      <td>-0.339652</td>\n",
       "      <td>0.577915</td>\n",
       "      <td>0.236062</td>\n",
       "      <td>0.111234</td>\n",
       "      <td>-0.021248</td>\n",
       "      <td>-0.153746</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>0.273916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025979</td>\n",
       "      <td>-0.026186</td>\n",
       "      <td>-0.068806</td>\n",
       "      <td>0.026521</td>\n",
       "      <td>-0.027352</td>\n",
       "      <td>-0.014985</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>-0.052546</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.197963</td>\n",
       "      <td>-0.464093</td>\n",
       "      <td>0.788213</td>\n",
       "      <td>-0.108202</td>\n",
       "      <td>0.918461</td>\n",
       "      <td>0.902399</td>\n",
       "      <td>-0.405444</td>\n",
       "      <td>-0.100074</td>\n",
       "      <td>-0.975100</td>\n",
       "      <td>-0.009527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.036513</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>-0.017260</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>-0.025403</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.671469</td>\n",
       "      <td>-0.424639</td>\n",
       "      <td>0.356897</td>\n",
       "      <td>-0.003133</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.077651</td>\n",
       "      <td>0.235716</td>\n",
       "      <td>-0.357372</td>\n",
       "      <td>0.663473</td>\n",
       "      <td>0.342369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039853</td>\n",
       "      <td>-0.027543</td>\n",
       "      <td>0.023752</td>\n",
       "      <td>-0.041222</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.030228</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>-0.023849</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.230311</td>\n",
       "      <td>-0.612838</td>\n",
       "      <td>0.707190</td>\n",
       "      <td>-0.165581</td>\n",
       "      <td>0.932456</td>\n",
       "      <td>0.654450</td>\n",
       "      <td>-0.259247</td>\n",
       "      <td>0.097634</td>\n",
       "      <td>-0.439642</td>\n",
       "      <td>-0.248355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003633</td>\n",
       "      <td>0.136953</td>\n",
       "      <td>-0.023407</td>\n",
       "      <td>0.094549</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.061064</td>\n",
       "      <td>-0.040254</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.405977</td>\n",
       "      <td>-0.311168</td>\n",
       "      <td>0.482294</td>\n",
       "      <td>0.171589</td>\n",
       "      <td>0.129961</td>\n",
       "      <td>-0.034979</td>\n",
       "      <td>-0.137464</td>\n",
       "      <td>0.036390</td>\n",
       "      <td>-0.024269</td>\n",
       "      <td>0.261850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>-0.012496</td>\n",
       "      <td>-0.007738</td>\n",
       "      <td>-0.005672</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>-0.044117</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>-0.021977</td>\n",
       "      <td>-0.004083</td>\n",
       "      <td>0.015843</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.002782</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>-0.003005</td>\n",
       "      <td>-0.001328</td>\n",
       "      <td>-0.002025</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>548.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222674</td>\n",
       "      <td>0.387596</td>\n",
       "      <td>0.175491</td>\n",
       "      <td>-0.096563</td>\n",
       "      <td>-0.086935</td>\n",
       "      <td>0.119067</td>\n",
       "      <td>-0.463115</td>\n",
       "      <td>0.014110</td>\n",
       "      <td>0.505928</td>\n",
       "      <td>-0.109398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000283</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.004651</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>-0.000948</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>553.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019605</td>\n",
       "      <td>0.036823</td>\n",
       "      <td>0.038894</td>\n",
       "      <td>0.021325</td>\n",
       "      <td>-0.082492</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>-0.025609</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.009609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>-0.003265</td>\n",
       "      <td>-0.001137</td>\n",
       "      <td>549.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072067</td>\n",
       "      <td>0.107968</td>\n",
       "      <td>0.117512</td>\n",
       "      <td>0.079260</td>\n",
       "      <td>-0.256596</td>\n",
       "      <td>0.211843</td>\n",
       "      <td>-0.095737</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.066042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004424</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>-0.000752</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>549.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.327796</td>\n",
       "      <td>-0.266223</td>\n",
       "      <td>0.352387</td>\n",
       "      <td>0.215172</td>\n",
       "      <td>-0.008068</td>\n",
       "      <td>-0.141244</td>\n",
       "      <td>-0.068130</td>\n",
       "      <td>0.144283</td>\n",
       "      <td>0.086015</td>\n",
       "      <td>0.203379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012326</td>\n",
       "      <td>-0.016419</td>\n",
       "      <td>-0.008146</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>133.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9108 rows x 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.407687  -0.339652   0.577915   0.236062   0.111234  -0.021248   \n",
       "2         1.197963  -0.464093   0.788213  -0.108202   0.918461   0.902399   \n",
       "3         0.671469  -0.424639   0.356897  -0.003133   0.223691   0.077651   \n",
       "4         1.230311  -0.612838   0.707190  -0.165581   0.932456   0.654450   \n",
       "5         0.405977  -0.311168   0.482294   0.171589   0.129961  -0.034979   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008969   0.016180   0.017711   0.009963  -0.044117   0.042820   \n",
       "981644    0.222674   0.387596   0.175491  -0.096563  -0.086935   0.119067   \n",
       "981646    0.019605   0.036823   0.038894   0.021325  -0.082492   0.075968   \n",
       "981648    0.072067   0.107968   0.117512   0.079260  -0.256596   0.211843   \n",
       "981666    0.327796  -0.266223   0.352387   0.215172  -0.008068  -0.141244   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_992  \\\n",
       "text_id                                              ...                \n",
       "1        -0.153746  -0.007755   0.013452   0.273916  ...    -0.025979   \n",
       "2        -0.405444  -0.100074  -0.975100  -0.009527  ...     0.031256   \n",
       "3         0.235716  -0.357372   0.663473   0.342369  ...     0.039853   \n",
       "4        -0.259247   0.097634  -0.439642  -0.248355  ...    -0.003633   \n",
       "5        -0.137464   0.036390  -0.024269   0.261850  ...     0.011111   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643   -0.021977  -0.004083   0.015843   0.004510  ...     0.001501   \n",
       "981644   -0.463115   0.014110   0.505928  -0.109398  ...    -0.000283   \n",
       "981646   -0.025609   0.000499   0.022658   0.009609  ...     0.001046   \n",
       "981648   -0.095737   0.000916   0.054085   0.066042  ...    -0.004424   \n",
       "981666   -0.068130   0.144283   0.086015   0.203379  ...    -0.012326   \n",
       "\n",
       "         feature_993  feature_994  feature_995  feature_996  feature_997  \\\n",
       "text_id                                                                    \n",
       "1          -0.026186    -0.068806     0.026521    -0.027352    -0.014985   \n",
       "2           0.036513     0.008823    -0.017260     0.007761    -0.025403   \n",
       "3          -0.027543     0.023752    -0.041222     0.011726     0.030228   \n",
       "4           0.136953    -0.023407     0.094549     0.000708     0.061064   \n",
       "5           0.000981     0.004935    -0.012496    -0.007738    -0.005672   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002782    -0.000141     0.000533    -0.003005    -0.001328   \n",
       "981644      0.000014    -0.000176    -0.004651    -0.004013    -0.000948   \n",
       "981646      0.001083     0.000363    -0.000543     0.003878     0.002361   \n",
       "981648      0.006029     0.003331     0.000766     0.003822    -0.000752   \n",
       "981666     -0.016419    -0.008146     0.006183     0.001607     0.010076   \n",
       "\n",
       "         feature_998  feature_999  y_target  date_uncertain  \n",
       "text_id                                                      \n",
       "1           0.023677    -0.052546    -124.0           False  \n",
       "2           0.028005     0.018981    -112.0           False  \n",
       "3           0.019802    -0.023849    -109.0           False  \n",
       "4          -0.040254     0.001183    -108.0           False  \n",
       "5           0.009419     0.006109    -106.0           False  \n",
       "...              ...          ...       ...             ...  \n",
       "981643     -0.002025     0.005040     548.0           False  \n",
       "981644      0.002560     0.004841     553.0           False  \n",
       "981646     -0.003265    -0.001137     549.0           False  \n",
       "981648      0.000908     0.005346     549.5            True  \n",
       "981666      0.003191     0.001712     133.0           False  \n",
       "\n",
       "[9108 rows x 1002 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['y1','y2'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TRAINING & COMPARING OUR MODELS</h1>\n",
    "\n",
    "This code splits our dataset into training and testing subsets to prepare it for our machine learning algorithms.\n",
    "\n",
    "X contains all of our features\n",
    "y contains our label\n",
    "\n",
    "We are then splitting the data into training and testing subsets, using 20% of the data for testing and 80% for training. We set random_sate =42 for reproducibility.\n",
    "\n",
    "The purpose of splitting our data is to evaluate how well the model generalizes to unseen data. Our training sets are used to train the model while the testing set is used afterward to assess the model's performance on data it hasn't seen during training.\n",
    "\n",
    "We did not scale our data because the original data was one-hot encoded and reducced using Variance Thresholding and SVD. Variance Thresholding only removes features with low variance and doesn't change their scales. SVD produces principle components that are linear combinations of our original features, but SVD already scales them to optimize variance. The components were normalized internally in the decomposition process of the SVD class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7286, 1001)\n",
      "(1822, 1001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])\n",
    "y = merged_df['y_target']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Convert all boolean columns in the dataset to integers\n",
    "X_train = X_train.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "X_test = X_test.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LINEAR OR NONLINEAR RELATIONSHIPS?</h1>\n",
    "After training all of our models, it is apparent our the relationships in our data may be nonlinear.\n",
    "\n",
    "Our Lasso and Ridge Regression lienar models performed significantly worse. These models assume that the relationship between the features and target is linear.\n",
    "\n",
    "Our RandomForest and XGBoost ensemble nonlinear models performed far better. These models capture nonlinear relationships in the data. They work by splitting the features into smaller tress and makes decisions based on those trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LASSO</h1>\n",
    "\n",
    "We are using Lasso  linear regression model with cross validation, using 5 folds. Lasso adds an L1 regularization term to the update function. Lasso is able to shrink some coefficients to 0, basically reducing some of our features. Lasso was our poorest performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - Best alpha: 0.10065848829644768\n",
      "Train MAE: 110.6250906749479\n",
      "Test MAE: 118.26166652971969\n",
      "Selected features by Lasso:\n",
      "feature_0          29.586222\n",
      "feature_1         284.814829\n",
      "feature_2          29.551639\n",
      "feature_3          60.023492\n",
      "feature_4        -249.803620\n",
      "                     ...    \n",
      "feature_994        29.915784\n",
      "feature_995       -57.844014\n",
      "feature_997       -64.634300\n",
      "feature_998         5.194057\n",
      "date_uncertain     51.666133\n",
      "Length: 562, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Lasso with cross-validation\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Lasso - Best alpha:\", lasso.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, lasso.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, lasso.predict(X_test)))\n",
    "\n",
    "lasso_features = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\"Selected features by Lasso:\")\n",
    "print(lasso_features[lasso_features != 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RIDGE</h1>\n",
    "\n",
    "The code below performs ridge regression with cross-validation. Ridge regression adds an L2 regularization to the linear regression model, which penalizes large coefficient values and helps prevent overfitting. The strength of the regularization is controlled by the alpha parameter when instantiating the RidgeCV class. We are also performing 5-fold cross validation, which means it splits the training data into 5 parts, trains the model of 4 parts and validates on the remaing part, and repeats this process 5 times as it iterates through each fold.\n",
    "\n",
    "The mean absolute error, MAE, measures the average absolute difference between our predicted values and the ground truths. The lower the MAE, the better. After printing the MAE of our best Ridge model's best parameters, we had a Train and TEST MAE of 109.62 and 124.12. These are some of our lowest scores after comparing all of our models. The poor results on Ridge and Lasso compared to our other models suggests that our data is nonlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - Best alpha: 10.0\n",
      "Train MAE: 107.4174776997243\n",
      "Test MAE: 116.69859835174945\n",
      "Ridge coefficients:\n",
      "feature_1      283.340679\n",
      "feature_117    136.619684\n",
      "feature_668    120.557338\n",
      "feature_22     116.249011\n",
      "feature_461    114.521665\n",
      "feature_72     111.953832\n",
      "feature_434    111.309247\n",
      "feature_34     109.667555\n",
      "feature_707    104.077337\n",
      "feature_462    101.998901\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ridge with cross-validation\n",
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Ridge - Best alpha:\", ridge.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, ridge.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, ridge.predict(X_test)))\n",
    "\n",
    "ridge_features = pd.Series(ridge.coef_, index=X.columns)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_features.sort_values(ascending=False).head(10))  # Top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST</h1>\n",
    "\n",
    "This code implements a RandomForest model designed for regression tasks. It builds 100 decision tress in the \"forest\" (model).\n",
    "\n",
    "A random forest is an ensemble learning algorithm that combines multiple decision tress to improve prediction accuracy and reduce overfitting. The algorithm does something called bootstrap sampling, which randomly selects samples with replacement from the training data to create several datasets for training the individual trees we specified in the parameter. For each decision tree, the algorithm selects a random subset of features at each split. Each tree in the forest predicts a value and the final prediction is the average of all tree predictions in the forest. Random Forest is effective at solving tasks because it uses many tress instead of a single deicision tree to make an accurate prediction. This ensemble reduces variance in the predictions. It also handles complex, nonlinear relationships in the data which seems to be the case in our dataset.\n",
    "\n",
    "After 8m 15s of execution, our initial RandomForest produced results far better than our Linear and Ridge models, having a training MAE of 30.7 and test MAE of 78.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 29.255435168695872\n",
      "Random Forest - Test MAE: 76.32743520557159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, \n",
    "                                random_state=42, \n",
    "                                #Tried playing with these parameters. They get faster results, and less overfitting, but the Test MAE is worse so i've left them commented out.\n",
    "                                #max_depth=10, \n",
    "                                #min_samples_split=10, \n",
    "                                #min_samples_leaf=5,  \n",
    "                                #max_features='sqrt', \n",
    "                                n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OPTIMIZING RANDOM FOREST</h1>\n",
    "\n",
    "The RandomForest regressor was one of our highest performing models. It performed far better than Lasso and Ridge, so we wanted to improve the model by searching for better hyper parameters.\n",
    "\n",
    "We decided to use GridSearch cross validation with performs a systematic search over a range of hyperparameter combinations we assign for our RandomForestRegressor model. We are passing in parameters to control the amount of trees in the forest, the maximum depth of each tree, and the minimum number of samples required to split a node in each tree. We are also using 5 fold cross validation.\n",
    "\n",
    "After execution, we found our best parameters were:\n",
    "\n",
    "Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "\n",
    "Best Random Forest Test MAE: 92.32550495382549\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# RandomizedSearchCV doesn't like modin. convert to pandas \n",
    "X_train2 = X_train._to_pandas()\n",
    "y_train2 = y_train._to_pandas()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BEWARE!! RUNNING THIS CELL TAKES 3 HOURS TO EXECUTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "randomized_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions=param_grid, n_iter=5, cv=3, n_jobs=-1, verbose=3)\n",
    "randomized_search.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Best Parameters:\", randomized_search.best_params_)\n",
    "print(\"Best Random Forest Test MAE:\", mean_absolute_error(y_test, randomized_search.best_estimator_.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding our best hyper parameters, we reran the RandomForestRegressor with the new hyper parameters to see our results. Expectedly, they improved. However, we wanted to explore other models to see if there was even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 30.241163323492135\n",
      "Random Forest - Test MAE: 76.13762129806976\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(max_depth=20, min_samples_split=5, n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBOOST (EXTREME GRADIENT BOOSTING)</h1>\n",
    "\n",
    "This model is a gradient boosting ensemble learning algorithm. XGBoost is an efficient and accurate gradient boosintg framework for machine learning. We experimented with a bunch of different parameters manually, slowly improving our model from 81 MAE down to 71 MAE.\n",
    "\n",
    "Boosting is an ensemble learning technique that combines weak learners (like shallow decision trees) sequentially to form a strong learner. Each new tree is trained to correct errors made by the previous shallow trees. Gradient boosting minimizes the update function by computing gradients of the loss with respect to the model's predictions. The XGBoost frameowkr in particular has regularization to prevent overfitting, is optimized for speed and low memory usage, and supports sparse data.\n",
    "\n",
    "Only taking 20 seconds to run, our XGBoost model was our most successful and most efficient compared to the 18 minutes it took our RandomForest model. We were able to adjust our model to achieve an MAE of 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 68.14354441022965\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize XGBRegressor with multithreading enabled\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.7, learning_rate=0.031,\n",
    "                        max_depth=7, alpha=25, n_estimators=350, n_jobs=-1)  # -1 enables multithreading\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 value for our XGBoost model was 0.84 which indicates that 84% of the variation in our target variable was captured by the model. This indicates that the features we reduced to are highly predictive of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 68.14354441022965\n",
      "XGBoost R-squared: 0.8378113155872244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate MAE and R-squared\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])  # Replace with your feature dataframe\n",
    "y = merged_df['y_target']  # Replace with your target column\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure X_train and X_test are numeric\n",
    "X_train = X_train.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "X_test = X_test.applymap(lambda x: int(x) if isinstance(x, bool) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255002\n",
      "[LightGBM] [Info] Number of data points in the train set: 7286, number of used features: 1001\n",
      "[LightGBM] [Info] Start training from score 122.996432\n",
      "LightGBM MAE: 69.37708908152341\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize LightGBM Regressor\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=42,\n",
    "    learning_rate=0.04,\n",
    "    n_estimators=600,\n",
    "    max_depth=-1,  # No max depth\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_lgb = lgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lgb_mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "print(f\"LightGBM MAE: {lgb_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM MAE: 213.8609622578169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create a pipeline to scale the data and apply SVR\n",
    "svm_pipeline = make_pipeline(\n",
    "    StandardScaler(),  # SVM benefits from scaling the data\n",
    "    SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Radial Basis Function kernel\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "svm_mae = mean_absolute_error(y_test, y_pred_svm)\n",
    "print(f\"SVM MAE: {svm_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_0': 69632.53125,\n",
       " 'feature_1': 3289212.5,\n",
       " 'feature_2': 1288795.75,\n",
       " 'feature_3': 547318.6875,\n",
       " 'feature_4': 2206215.5,\n",
       " 'feature_5': 1091845.0,\n",
       " 'feature_6': 317597.46875,\n",
       " 'feature_7': 343053.875,\n",
       " 'feature_8': 437787.6875,\n",
       " 'feature_9': 139245.953125,\n",
       " 'feature_10': 289614.78125,\n",
       " 'feature_11': 311026.84375,\n",
       " 'feature_12': 239431.90625,\n",
       " 'feature_13': 819283.75,\n",
       " 'feature_14': 353530.75,\n",
       " 'feature_15': 189499.46875,\n",
       " 'feature_16': 447874.71875,\n",
       " 'feature_17': 377258.65625,\n",
       " 'feature_18': 588562.6875,\n",
       " 'feature_19': 306610.40625,\n",
       " 'feature_20': 272373.1875,\n",
       " 'feature_21': 257294.578125,\n",
       " 'feature_22': 391639.96875,\n",
       " 'feature_23': 315452.34375,\n",
       " 'feature_24': 149635.765625,\n",
       " 'feature_25': 176951.890625,\n",
       " 'feature_26': 166799.5625,\n",
       " 'feature_27': 254595.171875,\n",
       " 'feature_28': 110564.7734375,\n",
       " 'feature_29': 105759.5625,\n",
       " 'feature_30': 88589.6484375,\n",
       " 'feature_31': 444716.34375,\n",
       " 'feature_32': 207692.21875,\n",
       " 'feature_33': 158807.0625,\n",
       " 'feature_34': 308824.09375,\n",
       " 'feature_35': 366837.125,\n",
       " 'feature_36': 1169403.875,\n",
       " 'feature_37': 369695.625,\n",
       " 'feature_38': 108539.578125,\n",
       " 'feature_39': 101997.59375,\n",
       " 'feature_40': 140703.3125,\n",
       " 'feature_41': 281915.96875,\n",
       " 'feature_42': 136908.734375,\n",
       " 'feature_43': 154982.4375,\n",
       " 'feature_44': 59105.828125,\n",
       " 'feature_45': 72873.8515625,\n",
       " 'feature_46': 66842.0078125,\n",
       " 'feature_47': 91108.8046875,\n",
       " 'feature_48': 90004.3671875,\n",
       " 'feature_49': 137745.625,\n",
       " 'feature_50': 71458.34375,\n",
       " 'feature_51': 50363.390625,\n",
       " 'feature_52': 78956.3203125,\n",
       " 'feature_53': 167544.359375,\n",
       " 'feature_54': 191660.28125,\n",
       " 'feature_55': 32721.84765625,\n",
       " 'feature_56': 63070.3125,\n",
       " 'feature_57': 69735.875,\n",
       " 'feature_58': 68111.671875,\n",
       " 'feature_59': 39743.37890625,\n",
       " 'feature_60': 82780.34375,\n",
       " 'feature_61': 99496.2578125,\n",
       " 'feature_62': 108102.859375,\n",
       " 'feature_63': 74055.1953125,\n",
       " 'feature_64': 115298.46875,\n",
       " 'feature_65': 142406.90625,\n",
       " 'feature_66': 47931.1953125,\n",
       " 'feature_67': 123782.1015625,\n",
       " 'feature_68': 62255.55859375,\n",
       " 'feature_69': 119401.015625,\n",
       " 'feature_70': 180831.46875,\n",
       " 'feature_71': 95430.96875,\n",
       " 'feature_72': 306363.625,\n",
       " 'feature_73': 50658.55078125,\n",
       " 'feature_74': 71698.3671875,\n",
       " 'feature_75': 61800.58203125,\n",
       " 'feature_76': 142387.28125,\n",
       " 'feature_77': 27069.962890625,\n",
       " 'feature_78': 90881.2734375,\n",
       " 'feature_79': 35287.953125,\n",
       " 'feature_80': 24983.494140625,\n",
       " 'feature_81': 81889.71875,\n",
       " 'feature_82': 129733.3984375,\n",
       " 'feature_83': 90874.046875,\n",
       " 'feature_84': 29703.841796875,\n",
       " 'feature_85': 110649.8203125,\n",
       " 'feature_86': 64141.12890625,\n",
       " 'feature_87': 46746.734375,\n",
       " 'feature_88': 60034.59375,\n",
       " 'feature_89': 151659.0,\n",
       " 'feature_90': 50333.953125,\n",
       " 'feature_91': 37491.87890625,\n",
       " 'feature_92': 59367.25,\n",
       " 'feature_93': 78014.5078125,\n",
       " 'feature_94': 149430.203125,\n",
       " 'feature_95': 57769.4375,\n",
       " 'feature_96': 82540.1484375,\n",
       " 'feature_97': 35488.53515625,\n",
       " 'feature_98': 43190.2578125,\n",
       " 'feature_99': 25517.57421875,\n",
       " 'feature_100': 46901.90625,\n",
       " 'feature_101': 20480.8515625,\n",
       " 'feature_102': 27259.353515625,\n",
       " 'feature_103': 51864.55078125,\n",
       " 'feature_104': 36574.04296875,\n",
       " 'feature_105': 103618.140625,\n",
       " 'feature_106': 86456.640625,\n",
       " 'feature_107': 40048.04296875,\n",
       " 'feature_108': 43903.01171875,\n",
       " 'feature_109': 63052.11328125,\n",
       " 'feature_110': 62121.50390625,\n",
       " 'feature_111': 56667.46875,\n",
       " 'feature_112': 55350.90625,\n",
       " 'feature_113': 93330.078125,\n",
       " 'feature_114': 54308.421875,\n",
       " 'feature_115': 99989.4609375,\n",
       " 'feature_116': 24056.5390625,\n",
       " 'feature_117': 173952.4375,\n",
       " 'feature_118': 136910.25,\n",
       " 'feature_119': 64655.4609375,\n",
       " 'feature_120': 47504.65625,\n",
       " 'feature_121': 116299.03125,\n",
       " 'feature_122': 78724.5703125,\n",
       " 'feature_123': 134686.625,\n",
       " 'feature_124': 79343.453125,\n",
       " 'feature_125': 200681.21875,\n",
       " 'feature_126': 93947.2734375,\n",
       " 'feature_127': 106966.7421875,\n",
       " 'feature_128': 24107.771484375,\n",
       " 'feature_129': 34862.26953125,\n",
       " 'feature_130': 39261.0390625,\n",
       " 'feature_131': 57235.54296875,\n",
       " 'feature_132': 81941.5,\n",
       " 'feature_133': 108843.3671875,\n",
       " 'feature_134': 34377.2421875,\n",
       " 'feature_135': 54766.53125,\n",
       " 'feature_136': 36638.01171875,\n",
       " 'feature_137': 53063.48046875,\n",
       " 'feature_138': 45423.5546875,\n",
       " 'feature_139': 44408.85546875,\n",
       " 'feature_140': 29807.654296875,\n",
       " 'feature_141': 30146.474609375,\n",
       " 'feature_142': 65923.0078125,\n",
       " 'feature_143': 117712.390625,\n",
       " 'feature_144': 38629.67578125,\n",
       " 'feature_145': 45890.171875,\n",
       " 'feature_146': 84354.515625,\n",
       " 'feature_147': 39787.4453125,\n",
       " 'feature_148': 54435.81640625,\n",
       " 'feature_149': 69703.75,\n",
       " 'feature_150': 103484.3046875,\n",
       " 'feature_151': 46870.15625,\n",
       " 'feature_152': 20750.78515625,\n",
       " 'feature_153': 41202.28515625,\n",
       " 'feature_154': 46870.45703125,\n",
       " 'feature_155': 116259.265625,\n",
       " 'feature_156': 29130.61328125,\n",
       " 'feature_157': 119027.34375,\n",
       " 'feature_158': 32805.85546875,\n",
       " 'feature_159': 50695.73828125,\n",
       " 'feature_160': 12329.01953125,\n",
       " 'feature_161': 22708.171875,\n",
       " 'feature_162': 55694.99609375,\n",
       " 'feature_163': 114305.65625,\n",
       " 'feature_164': 130917.1484375,\n",
       " 'feature_165': 46862.21875,\n",
       " 'feature_166': 36915.11328125,\n",
       " 'feature_167': 57735.8984375,\n",
       " 'feature_168': 20367.271484375,\n",
       " 'feature_169': 32486.65625,\n",
       " 'feature_170': 29576.90234375,\n",
       " 'feature_171': 48745.6953125,\n",
       " 'feature_172': 21893.447265625,\n",
       " 'feature_173': 68187.46875,\n",
       " 'feature_174': 54958.546875,\n",
       " 'feature_175': 85207.5546875,\n",
       " 'feature_176': 53123.43359375,\n",
       " 'feature_177': 19139.06640625,\n",
       " 'feature_178': 261784.53125,\n",
       " 'feature_179': 27491.26171875,\n",
       " 'feature_180': 79742.90625,\n",
       " 'feature_181': 36752.35546875,\n",
       " 'feature_182': 15133.2919921875,\n",
       " 'feature_183': 40247.390625,\n",
       " 'feature_184': 34314.6015625,\n",
       " 'feature_185': 25065.662109375,\n",
       " 'feature_186': 43728.96875,\n",
       " 'feature_187': 24673.17578125,\n",
       " 'feature_188': 46129.05078125,\n",
       " 'feature_189': 75351.09375,\n",
       " 'feature_190': 25542.931640625,\n",
       " 'feature_191': 28621.740234375,\n",
       " 'feature_192': 77838.390625,\n",
       " 'feature_193': 20585.119140625,\n",
       " 'feature_194': 49317.75390625,\n",
       " 'feature_195': 53442.76171875,\n",
       " 'feature_196': 55137.671875,\n",
       " 'feature_197': 34189.13671875,\n",
       " 'feature_198': 15960.4833984375,\n",
       " 'feature_199': 27659.90625,\n",
       " 'feature_200': 26496.916015625,\n",
       " 'feature_201': 36094.76171875,\n",
       " 'feature_202': 53248.953125,\n",
       " 'feature_203': 36329.98828125,\n",
       " 'feature_204': 53594.08203125,\n",
       " 'feature_205': 62933.8984375,\n",
       " 'feature_206': 30804.0703125,\n",
       " 'feature_207': 30760.5,\n",
       " 'feature_208': 53701.26171875,\n",
       " 'feature_209': 27950.205078125,\n",
       " 'feature_210': 68498.9765625,\n",
       " 'feature_211': 19840.654296875,\n",
       " 'feature_212': 13395.455078125,\n",
       " 'feature_213': 30456.990234375,\n",
       " 'feature_214': 22500.6015625,\n",
       " 'feature_215': 32387.609375,\n",
       " 'feature_216': 37748.24609375,\n",
       " 'feature_217': 25052.65625,\n",
       " 'feature_218': 38422.30859375,\n",
       " 'feature_219': 21251.69140625,\n",
       " 'feature_220': 34120.68359375,\n",
       " 'feature_221': 53714.0625,\n",
       " 'feature_222': 30290.75390625,\n",
       " 'feature_223': 41756.33984375,\n",
       " 'feature_224': 68293.984375,\n",
       " 'feature_225': 54601.63671875,\n",
       " 'feature_226': 61430.71484375,\n",
       " 'feature_227': 55781.0,\n",
       " 'feature_228': 47101.48046875,\n",
       " 'feature_229': 51794.046875,\n",
       " 'feature_230': 34924.0234375,\n",
       " 'feature_231': 36011.9296875,\n",
       " 'feature_232': 39330.42578125,\n",
       " 'feature_233': 84231.3671875,\n",
       " 'feature_234': 77024.5625,\n",
       " 'feature_235': 22172.35546875,\n",
       " 'feature_236': 25124.466796875,\n",
       " 'feature_237': 42586.80859375,\n",
       " 'feature_238': 64445.16015625,\n",
       " 'feature_239': 86790.3828125,\n",
       " 'feature_240': 72039.125,\n",
       " 'feature_241': 17045.900390625,\n",
       " 'feature_242': 16836.0859375,\n",
       " 'feature_243': 24798.724609375,\n",
       " 'feature_244': 17665.86328125,\n",
       " 'feature_245': 37625.74609375,\n",
       " 'feature_246': 17775.501953125,\n",
       " 'feature_247': 35802.5546875,\n",
       " 'feature_248': 97108.28125,\n",
       " 'feature_249': 33489.4453125,\n",
       " 'feature_250': 23944.27734375,\n",
       " 'feature_251': 31787.509765625,\n",
       " 'feature_252': 77174.0703125,\n",
       " 'feature_253': 13695.0615234375,\n",
       " 'feature_254': 27999.744140625,\n",
       " 'feature_255': 24713.623046875,\n",
       " 'feature_256': 14317.38671875,\n",
       " 'feature_257': 39423.3125,\n",
       " 'feature_258': 74746.2109375,\n",
       " 'feature_259': 45750.51953125,\n",
       " 'feature_260': 191309.640625,\n",
       " 'feature_261': 114566.8125,\n",
       " 'feature_262': 37115.65625,\n",
       " 'feature_263': 61993.640625,\n",
       " 'feature_264': 32728.7890625,\n",
       " 'feature_265': 152798.84375,\n",
       " 'feature_266': 44694.703125,\n",
       " 'feature_267': 14961.52734375,\n",
       " 'feature_268': 44919.23828125,\n",
       " 'feature_269': 88150.1796875,\n",
       " 'feature_270': 90784.5390625,\n",
       " 'feature_271': 13581.544921875,\n",
       " 'feature_272': 43352.6953125,\n",
       " 'feature_273': 40278.62890625,\n",
       " 'feature_274': 139550.640625,\n",
       " 'feature_275': 64493.75,\n",
       " 'feature_276': 20629.064453125,\n",
       " 'feature_277': 27684.4453125,\n",
       " 'feature_278': 42502.5859375,\n",
       " 'feature_279': 28214.216796875,\n",
       " 'feature_280': 22693.1328125,\n",
       " 'feature_281': 34257.734375,\n",
       " 'feature_282': 24574.21875,\n",
       " 'feature_283': 15328.5322265625,\n",
       " 'feature_284': 16731.154296875,\n",
       " 'feature_285': 30647.208984375,\n",
       " 'feature_286': 10626.2919921875,\n",
       " 'feature_287': 15853.7177734375,\n",
       " 'feature_288': 26206.2421875,\n",
       " 'feature_289': 34502.68359375,\n",
       " 'feature_290': 50191.73046875,\n",
       " 'feature_291': 69743.46875,\n",
       " 'feature_292': 39021.80859375,\n",
       " 'feature_293': 57219.5703125,\n",
       " 'feature_294': 41903.390625,\n",
       " 'feature_295': 37558.06640625,\n",
       " 'feature_296': 18381.46875,\n",
       " 'feature_297': 12946.033203125,\n",
       " 'feature_298': 47762.30078125,\n",
       " 'feature_299': 29279.330078125,\n",
       " 'feature_300': 26988.8828125,\n",
       " 'feature_301': 33245.11328125,\n",
       " 'feature_302': 43832.30078125,\n",
       " 'feature_303': 74170.15625,\n",
       " 'feature_304': 17336.279296875,\n",
       " 'feature_305': 16340.00390625,\n",
       " 'feature_306': 30123.791015625,\n",
       " 'feature_307': 33401.80078125,\n",
       " 'feature_308': 43494.3359375,\n",
       " 'feature_309': 13320.5234375,\n",
       " 'feature_310': 24309.58984375,\n",
       " 'feature_311': 19642.388671875,\n",
       " 'feature_312': 49534.6875,\n",
       " 'feature_313': 28848.6171875,\n",
       " 'feature_314': 55355.9296875,\n",
       " 'feature_315': 66241.625,\n",
       " 'feature_316': 49923.1328125,\n",
       " 'feature_317': 56333.7890625,\n",
       " 'feature_318': 59507.49609375,\n",
       " 'feature_319': 31250.611328125,\n",
       " 'feature_320': 47897.58984375,\n",
       " 'feature_321': 36196.5859375,\n",
       " 'feature_322': 22308.134765625,\n",
       " 'feature_323': 72946.59375,\n",
       " 'feature_324': 27317.720703125,\n",
       " 'feature_325': 24771.24609375,\n",
       " 'feature_326': 15528.3515625,\n",
       " 'feature_327': 31635.30078125,\n",
       " 'feature_328': 34350.9296875,\n",
       " 'feature_329': 24179.548828125,\n",
       " 'feature_330': 35796.5078125,\n",
       " 'feature_331': 12202.1552734375,\n",
       " 'feature_332': 28399.111328125,\n",
       " 'feature_333': 16671.783203125,\n",
       " 'feature_334': 37043.06640625,\n",
       " 'feature_335': 64191.40234375,\n",
       " 'feature_336': 30725.044921875,\n",
       " 'feature_337': 21740.142578125,\n",
       " 'feature_338': 42537.6171875,\n",
       " 'feature_339': 15507.150390625,\n",
       " 'feature_340': 34373.42578125,\n",
       " 'feature_341': 58374.1953125,\n",
       " 'feature_342': 74281.140625,\n",
       " 'feature_343': 82595.65625,\n",
       " 'feature_344': 66132.7734375,\n",
       " 'feature_345': 24421.302734375,\n",
       " 'feature_346': 48061.69921875,\n",
       " 'feature_347': 26660.08984375,\n",
       " 'feature_348': 31917.560546875,\n",
       " 'feature_349': 57354.91796875,\n",
       " 'feature_350': 29451.478515625,\n",
       " 'feature_351': 31387.259765625,\n",
       " 'feature_352': 22577.42578125,\n",
       " 'feature_353': 56891.46484375,\n",
       " 'feature_354': 77494.390625,\n",
       " 'feature_355': 66516.203125,\n",
       " 'feature_356': 46477.078125,\n",
       " 'feature_357': 12532.3115234375,\n",
       " 'feature_358': 17305.201171875,\n",
       " 'feature_359': 27938.177734375,\n",
       " 'feature_360': 36176.5,\n",
       " 'feature_361': 24164.775390625,\n",
       " 'feature_362': 123333.7265625,\n",
       " 'feature_363': 30905.66015625,\n",
       " 'feature_364': 85819.390625,\n",
       " 'feature_365': 58460.5234375,\n",
       " 'feature_366': 73369.765625,\n",
       " 'feature_367': 72878.609375,\n",
       " 'feature_368': 39887.5390625,\n",
       " 'feature_369': 77612.703125,\n",
       " 'feature_370': 13207.7734375,\n",
       " 'feature_371': 31256.939453125,\n",
       " 'feature_372': 24651.54296875,\n",
       " 'feature_373': 54220.8359375,\n",
       " 'feature_374': 52697.796875,\n",
       " 'feature_375': 33411.08203125,\n",
       " 'feature_376': 23068.603515625,\n",
       " 'feature_377': 16694.08203125,\n",
       " 'feature_378': 24694.13671875,\n",
       " 'feature_379': 9754.748046875,\n",
       " 'feature_380': 78304.7109375,\n",
       " 'feature_381': 32112.583984375,\n",
       " 'feature_382': 25749.95703125,\n",
       " 'feature_383': 22631.69140625,\n",
       " 'feature_384': 26889.748046875,\n",
       " 'feature_385': 26079.23046875,\n",
       " 'feature_386': 44268.234375,\n",
       " 'feature_387': 60867.27734375,\n",
       " 'feature_388': 35272.515625,\n",
       " 'feature_389': 47410.37109375,\n",
       " 'feature_390': 25128.650390625,\n",
       " 'feature_391': 30828.833984375,\n",
       " 'feature_392': 107429.8125,\n",
       " 'feature_393': 25483.39453125,\n",
       " 'feature_394': 38969.90234375,\n",
       " 'feature_395': 59123.640625,\n",
       " 'feature_396': 33114.34375,\n",
       " 'feature_397': 40963.23046875,\n",
       " 'feature_398': 118348.265625,\n",
       " 'feature_399': 14563.501953125,\n",
       " 'feature_400': 87215.7578125,\n",
       " 'feature_401': 25461.80859375,\n",
       " 'feature_402': 87738.09375,\n",
       " 'feature_403': 27432.39453125,\n",
       " 'feature_404': 20931.5859375,\n",
       " 'feature_405': 16372.0146484375,\n",
       " 'feature_406': 39407.66015625,\n",
       " 'feature_407': 34843.21484375,\n",
       " 'feature_408': 14987.4482421875,\n",
       " 'feature_409': 54007.62109375,\n",
       " 'feature_410': 35353.1015625,\n",
       " 'feature_411': 73638.28125,\n",
       " 'feature_412': 44591.453125,\n",
       " 'feature_413': 13164.2666015625,\n",
       " 'feature_414': 32344.0703125,\n",
       " 'feature_415': 26722.806640625,\n",
       " 'feature_416': 82785.046875,\n",
       " 'feature_417': 35574.03125,\n",
       " 'feature_418': 23418.06640625,\n",
       " 'feature_419': 45262.90625,\n",
       " 'feature_420': 42279.4609375,\n",
       " 'feature_421': 30928.65625,\n",
       " 'feature_422': 65987.5390625,\n",
       " 'feature_423': 24944.33984375,\n",
       " 'feature_424': 30123.033203125,\n",
       " 'feature_425': 78866.4375,\n",
       " 'feature_426': 15575.5146484375,\n",
       " 'feature_427': 38542.8515625,\n",
       " 'feature_428': 53624.109375,\n",
       " 'feature_429': 58581.12890625,\n",
       " 'feature_430': 20635.185546875,\n",
       " 'feature_431': 48076.5078125,\n",
       " 'feature_432': 25305.859375,\n",
       " 'feature_433': 35941.71875,\n",
       " 'feature_434': 108941.1640625,\n",
       " 'feature_435': 7122.3212890625,\n",
       " 'feature_436': 8474.14453125,\n",
       " 'feature_437': 26669.146484375,\n",
       " 'feature_438': 32575.412109375,\n",
       " 'feature_439': 27179.705078125,\n",
       " 'feature_440': 74439.234375,\n",
       " 'feature_441': 39590.76171875,\n",
       " 'feature_442': 18753.9609375,\n",
       " 'feature_443': 13448.525390625,\n",
       " 'feature_444': 76742.671875,\n",
       " 'feature_445': 20464.0078125,\n",
       " 'feature_446': 28201.529296875,\n",
       " 'feature_447': 24668.20703125,\n",
       " 'feature_448': 58307.53515625,\n",
       " 'feature_449': 24505.72265625,\n",
       " 'feature_450': 23673.955078125,\n",
       " 'feature_451': 45455.05078125,\n",
       " 'feature_452': 14295.5908203125,\n",
       " 'feature_453': 17694.345703125,\n",
       " 'feature_454': 8848.6943359375,\n",
       " 'feature_455': 23508.1953125,\n",
       " 'feature_456': 34323.35546875,\n",
       " 'feature_457': 43878.69921875,\n",
       " 'feature_458': 14133.962890625,\n",
       " 'feature_459': 47688.62109375,\n",
       " 'feature_460': 21095.26171875,\n",
       " 'feature_461': 33591.3359375,\n",
       " 'feature_462': 46728.40234375,\n",
       " 'feature_463': 50444.67578125,\n",
       " 'feature_464': 73393.96875,\n",
       " 'feature_465': 27865.16015625,\n",
       " 'feature_466': 27808.48828125,\n",
       " 'feature_467': 23406.9765625,\n",
       " 'feature_468': 28990.5859375,\n",
       " 'feature_469': 38776.16015625,\n",
       " 'feature_470': 29518.755859375,\n",
       " 'feature_471': 58049.6328125,\n",
       " 'feature_472': 25723.255859375,\n",
       " 'feature_473': 63412.8671875,\n",
       " 'feature_474': 24080.087890625,\n",
       " 'feature_475': 37549.78125,\n",
       " 'feature_476': 30467.73828125,\n",
       " 'feature_477': 13103.6201171875,\n",
       " 'feature_478': 23535.7578125,\n",
       " 'feature_479': 18658.931640625,\n",
       " 'feature_480': 33076.5546875,\n",
       " 'feature_481': 93831.53125,\n",
       " 'feature_482': 55308.3515625,\n",
       " 'feature_483': 39454.00390625,\n",
       " 'feature_484': 31081.052734375,\n",
       " 'feature_485': 79840.265625,\n",
       " 'feature_486': 17510.59375,\n",
       " 'feature_487': 29471.33984375,\n",
       " 'feature_488': 54732.18359375,\n",
       " 'feature_489': 18376.248046875,\n",
       " 'feature_490': 52791.30078125,\n",
       " 'feature_491': 26018.73046875,\n",
       " 'feature_492': 26310.9765625,\n",
       " 'feature_493': 43941.75,\n",
       " 'feature_494': 21493.220703125,\n",
       " 'feature_495': 37252.11328125,\n",
       " 'feature_496': 31325.125,\n",
       " 'feature_497': 24941.509765625,\n",
       " 'feature_498': 36685.3046875,\n",
       " 'feature_499': 52506.19921875,\n",
       " 'feature_500': 10310.205078125,\n",
       " 'feature_501': 29989.224609375,\n",
       " 'feature_502': 27861.3046875,\n",
       " 'feature_503': 24179.3828125,\n",
       " 'feature_504': 45348.72265625,\n",
       " 'feature_505': 39373.39453125,\n",
       " 'feature_506': 15787.46875,\n",
       " 'feature_507': 18442.302734375,\n",
       " 'feature_508': 11662.6455078125,\n",
       " 'feature_509': 12185.0751953125,\n",
       " 'feature_510': 14723.544921875,\n",
       " 'feature_511': 50197.53125,\n",
       " 'feature_512': 27098.908203125,\n",
       " 'feature_513': 29659.10546875,\n",
       " 'feature_514': 55035.76171875,\n",
       " 'feature_515': 17310.314453125,\n",
       " 'feature_516': 9092.1142578125,\n",
       " 'feature_517': 64696.11328125,\n",
       " 'feature_518': 25657.017578125,\n",
       " 'feature_519': 28088.546875,\n",
       " 'feature_520': 23952.44921875,\n",
       " 'feature_521': 16879.39453125,\n",
       " 'feature_522': 56340.45703125,\n",
       " 'feature_523': 33288.9140625,\n",
       " 'feature_524': 24076.658203125,\n",
       " 'feature_525': 19095.51171875,\n",
       " 'feature_526': 31090.501953125,\n",
       " 'feature_527': 40977.27734375,\n",
       " 'feature_528': 43268.6484375,\n",
       " 'feature_529': 29152.47265625,\n",
       " 'feature_530': 58746.62890625,\n",
       " 'feature_531': 24552.42578125,\n",
       " 'feature_532': 32982.04296875,\n",
       " 'feature_533': 25743.994140625,\n",
       " 'feature_534': 13324.638671875,\n",
       " 'feature_535': 12727.181640625,\n",
       " 'feature_536': 14947.21484375,\n",
       " 'feature_537': 73954.2265625,\n",
       " 'feature_538': 39382.9453125,\n",
       " 'feature_539': 11758.6416015625,\n",
       " 'feature_540': 11959.166015625,\n",
       " 'feature_541': 16538.001953125,\n",
       " 'feature_542': 11917.2861328125,\n",
       " 'feature_543': 15940.47265625,\n",
       " 'feature_544': 23817.51171875,\n",
       " 'feature_545': 11295.0107421875,\n",
       " 'feature_546': 14474.650390625,\n",
       " 'feature_547': 31948.31640625,\n",
       " 'feature_548': 15768.6767578125,\n",
       " 'feature_549': 41357.296875,\n",
       " 'feature_550': 53532.22265625,\n",
       " 'feature_551': 15573.1611328125,\n",
       " 'feature_552': 24737.185546875,\n",
       " 'feature_553': 38412.0,\n",
       " 'feature_554': 32010.9453125,\n",
       " 'feature_555': 69071.828125,\n",
       " 'feature_556': 79955.0,\n",
       " 'feature_557': 21128.794921875,\n",
       " 'feature_558': 14714.5302734375,\n",
       " 'feature_559': 36489.99609375,\n",
       " 'feature_560': 27477.203125,\n",
       " 'feature_561': 14177.28515625,\n",
       " 'feature_562': 34293.4140625,\n",
       " 'feature_563': 22472.603515625,\n",
       " 'feature_564': 18822.5,\n",
       " 'feature_565': 23853.646484375,\n",
       " 'feature_566': 20669.84765625,\n",
       " 'feature_567': 24626.716796875,\n",
       " 'feature_568': 120220.640625,\n",
       " 'feature_569': 23184.708984375,\n",
       " 'feature_570': 21196.94921875,\n",
       " 'feature_571': 22982.484375,\n",
       " 'feature_572': 58490.94921875,\n",
       " 'feature_573': 33132.78125,\n",
       " 'feature_574': 64198.546875,\n",
       " 'feature_575': 10850.5791015625,\n",
       " 'feature_576': 32748.796875,\n",
       " 'feature_577': 24417.984375,\n",
       " 'feature_578': 65689.5546875,\n",
       " 'feature_579': 32768.44921875,\n",
       " 'feature_580': 51282.82421875,\n",
       " 'feature_581': 63088.6875,\n",
       " 'feature_582': 24348.640625,\n",
       " 'feature_583': 38945.32421875,\n",
       " 'feature_584': 49092.63671875,\n",
       " 'feature_585': 13754.1923828125,\n",
       " 'feature_586': 18995.80078125,\n",
       " 'feature_587': 25072.375,\n",
       " 'feature_588': 166202.6875,\n",
       " 'feature_589': 26043.4921875,\n",
       " 'feature_590': 22942.8125,\n",
       " 'feature_591': 16916.044921875,\n",
       " 'feature_592': 31999.982421875,\n",
       " 'feature_593': 38493.7734375,\n",
       " 'feature_594': 32368.693359375,\n",
       " 'feature_595': 27956.08203125,\n",
       " 'feature_596': 17075.205078125,\n",
       " 'feature_597': 20777.294921875,\n",
       " 'feature_598': 25642.13671875,\n",
       " 'feature_599': 74404.6875,\n",
       " 'feature_600': 84699.34375,\n",
       " 'feature_601': 21091.09375,\n",
       " 'feature_602': 96181.4921875,\n",
       " 'feature_603': 17971.837890625,\n",
       " 'feature_604': 12716.005859375,\n",
       " 'feature_605': 18087.001953125,\n",
       " 'feature_606': 18030.3203125,\n",
       " 'feature_607': 32522.486328125,\n",
       " 'feature_608': 63289.1015625,\n",
       " 'feature_609': 23537.984375,\n",
       " 'feature_610': 32537.6953125,\n",
       " 'feature_611': 25785.23046875,\n",
       " 'feature_612': 20428.6875,\n",
       " 'feature_613': 37536.43359375,\n",
       " 'feature_614': 31708.58984375,\n",
       " 'feature_615': 45381.71484375,\n",
       " 'feature_616': 29674.654296875,\n",
       " 'feature_617': 21144.01953125,\n",
       " 'feature_618': 21962.63671875,\n",
       " 'feature_619': 22113.10546875,\n",
       " 'feature_620': 16615.607421875,\n",
       " 'feature_621': 103704.796875,\n",
       " 'feature_622': 20028.349609375,\n",
       " 'feature_623': 38919.640625,\n",
       " 'feature_624': 25417.974609375,\n",
       " 'feature_625': 47481.9140625,\n",
       " 'feature_626': 105381.203125,\n",
       " 'feature_627': 17597.14453125,\n",
       " 'feature_628': 19011.84375,\n",
       " 'feature_629': 159385.78125,\n",
       " 'feature_630': 45566.17578125,\n",
       " 'feature_631': 14794.091796875,\n",
       " 'feature_632': 36655.0625,\n",
       " 'feature_633': 22079.69140625,\n",
       " 'feature_634': 41557.37109375,\n",
       " 'feature_635': 10561.02734375,\n",
       " 'feature_636': 23850.279296875,\n",
       " 'feature_637': 24849.505859375,\n",
       " 'feature_638': 27332.115234375,\n",
       " 'feature_639': 39344.31640625,\n",
       " 'feature_640': 39400.828125,\n",
       " 'feature_641': 27715.623046875,\n",
       " 'feature_642': 63388.59375,\n",
       " 'feature_643': 22517.06640625,\n",
       " 'feature_644': 124531.1953125,\n",
       " 'feature_645': 22898.5,\n",
       " 'feature_646': 28276.857421875,\n",
       " 'feature_647': 42453.42578125,\n",
       " 'feature_648': 48129.078125,\n",
       " 'feature_649': 24678.171875,\n",
       " 'feature_650': 44674.34765625,\n",
       " 'feature_651': 23798.19921875,\n",
       " 'feature_652': 32747.5546875,\n",
       " 'feature_653': 66639.0859375,\n",
       " 'feature_654': 39645.32421875,\n",
       " 'feature_655': 36029.1171875,\n",
       " 'feature_656': 51419.578125,\n",
       " 'feature_657': 17267.240234375,\n",
       " 'feature_658': 9927.3583984375,\n",
       " 'feature_659': 29806.318359375,\n",
       " 'feature_660': 20129.267578125,\n",
       " 'feature_661': 25328.064453125,\n",
       " 'feature_662': 33961.49609375,\n",
       " 'feature_663': 26733.03515625,\n",
       " 'feature_664': 35395.9921875,\n",
       " 'feature_665': 18399.486328125,\n",
       " 'feature_666': 35864.2421875,\n",
       " 'feature_667': 29989.626953125,\n",
       " 'feature_668': 17773.080078125,\n",
       " 'feature_669': 16963.6015625,\n",
       " 'feature_670': 24735.796875,\n",
       " 'feature_671': 17246.146484375,\n",
       " 'feature_672': 25553.40625,\n",
       " 'feature_673': 108614.09375,\n",
       " 'feature_674': 17905.6953125,\n",
       " 'feature_675': 86630.6875,\n",
       " 'feature_676': 36260.09765625,\n",
       " 'feature_677': 23225.31640625,\n",
       " 'feature_678': 36045.5390625,\n",
       " 'feature_679': 27936.828125,\n",
       " 'feature_680': 21321.693359375,\n",
       " 'feature_681': 70210.40625,\n",
       " 'feature_682': 28974.783203125,\n",
       " 'feature_683': 46138.41796875,\n",
       " 'feature_684': 17864.376953125,\n",
       " 'feature_685': 31489.46484375,\n",
       " 'feature_686': 25799.52734375,\n",
       " 'feature_687': 43174.85546875,\n",
       " 'feature_688': 21847.275390625,\n",
       " 'feature_689': 23004.595703125,\n",
       " 'feature_690': 13576.671875,\n",
       " 'feature_691': 37064.73828125,\n",
       " 'feature_692': 41709.21484375,\n",
       " 'feature_693': 21468.212890625,\n",
       " 'feature_694': 9351.2646484375,\n",
       " 'feature_695': 28096.7578125,\n",
       " 'feature_696': 23250.29296875,\n",
       " 'feature_697': 44937.17578125,\n",
       " 'feature_698': 28836.77734375,\n",
       " 'feature_699': 33774.14453125,\n",
       " 'feature_700': 19800.234375,\n",
       " 'feature_701': 20989.923828125,\n",
       " 'feature_702': 47339.47265625,\n",
       " 'feature_703': 35016.24609375,\n",
       " 'feature_704': 16627.44921875,\n",
       " 'feature_705': 20343.984375,\n",
       " 'feature_706': 41716.734375,\n",
       " 'feature_707': 117023.140625,\n",
       " 'feature_708': 40678.93359375,\n",
       " 'feature_709': 33739.63671875,\n",
       " 'feature_710': 99661.515625,\n",
       " 'feature_711': 47640.7421875,\n",
       " 'feature_712': 35237.8046875,\n",
       " 'feature_713': 14024.099609375,\n",
       " 'feature_714': 14548.970703125,\n",
       " 'feature_715': 22114.900390625,\n",
       " 'feature_716': 24483.099609375,\n",
       " 'feature_717': 34613.515625,\n",
       " 'feature_718': 9560.373046875,\n",
       " 'feature_719': 23631.470703125,\n",
       " 'feature_720': 18420.818359375,\n",
       " 'feature_721': 33209.71484375,\n",
       " 'feature_722': 72874.3125,\n",
       " 'feature_723': 31948.474609375,\n",
       " 'feature_724': 43140.1171875,\n",
       " 'feature_725': 34711.69921875,\n",
       " 'feature_726': 15949.4931640625,\n",
       " 'feature_727': 47274.36328125,\n",
       " 'feature_728': 22646.48828125,\n",
       " 'feature_729': 47009.234375,\n",
       " 'feature_730': 12163.1025390625,\n",
       " 'feature_731': 39621.57421875,\n",
       " 'feature_732': 29891.5625,\n",
       " 'feature_733': 47290.4375,\n",
       " 'feature_734': 15248.2744140625,\n",
       " 'feature_735': 34319.52734375,\n",
       " 'feature_736': 35079.515625,\n",
       " 'feature_737': 15794.51171875,\n",
       " 'feature_738': 27942.75390625,\n",
       " 'feature_739': 25197.21875,\n",
       " 'feature_740': 25410.259765625,\n",
       " 'feature_741': 28130.52734375,\n",
       " 'feature_742': 20325.814453125,\n",
       " 'feature_743': 27869.0625,\n",
       " 'feature_744': 31292.724609375,\n",
       " 'feature_745': 18951.42578125,\n",
       " 'feature_746': 37198.6875,\n",
       " 'feature_747': 21678.51953125,\n",
       " 'feature_748': 24980.150390625,\n",
       " 'feature_749': 17306.19921875,\n",
       " 'feature_750': 18289.083984375,\n",
       " 'feature_751': 21146.44140625,\n",
       " 'feature_752': 39390.9296875,\n",
       " 'feature_753': 13089.7001953125,\n",
       " 'feature_754': 25986.69140625,\n",
       " 'feature_755': 64007.4375,\n",
       " 'feature_756': 23109.625,\n",
       " 'feature_757': 40085.99609375,\n",
       " 'feature_758': 25178.57421875,\n",
       " 'feature_759': 46372.625,\n",
       " 'feature_760': 69048.234375,\n",
       " 'feature_761': 8296.8515625,\n",
       " 'feature_762': 25786.80078125,\n",
       " 'feature_763': 30574.4453125,\n",
       " 'feature_764': 15481.2705078125,\n",
       " 'feature_765': 62010.34375,\n",
       " 'feature_766': 20533.7734375,\n",
       " 'feature_767': 75736.2890625,\n",
       " 'feature_768': 16328.6640625,\n",
       " 'feature_769': 15913.109375,\n",
       " 'feature_770': 28742.166015625,\n",
       " 'feature_771': 28769.478515625,\n",
       " 'feature_772': 13564.1318359375,\n",
       " 'feature_773': 56774.73828125,\n",
       " 'feature_774': 34360.6953125,\n",
       " 'feature_775': 49689.6015625,\n",
       " 'feature_776': 23304.779296875,\n",
       " 'feature_777': 17656.46875,\n",
       " 'feature_778': 37776.3359375,\n",
       " 'feature_779': 29496.44921875,\n",
       " 'feature_780': 17761.193359375,\n",
       " 'feature_781': 27019.05859375,\n",
       " 'feature_782': 19301.658203125,\n",
       " 'feature_783': 38261.98046875,\n",
       " 'feature_784': 80908.4296875,\n",
       " 'feature_785': 24307.189453125,\n",
       " 'feature_786': 11106.0185546875,\n",
       " 'feature_787': 24270.484375,\n",
       " 'feature_788': 20864.994140625,\n",
       " 'feature_789': 48480.65234375,\n",
       " 'feature_790': 18308.279296875,\n",
       " 'feature_791': 22104.888671875,\n",
       " 'feature_792': 38845.5078125,\n",
       " 'feature_793': 20718.716796875,\n",
       " 'feature_794': 30853.689453125,\n",
       " 'feature_795': 29254.111328125,\n",
       " 'feature_796': 12572.537109375,\n",
       " 'feature_797': 35469.18359375,\n",
       " 'feature_798': 29828.3125,\n",
       " 'feature_799': 34081.796875,\n",
       " 'feature_800': 30657.50390625,\n",
       " 'feature_801': 65131.71484375,\n",
       " 'feature_802': 27559.505859375,\n",
       " 'feature_803': 24274.267578125,\n",
       " 'feature_804': 34477.734375,\n",
       " 'feature_805': 73045.703125,\n",
       " 'feature_806': 42902.07421875,\n",
       " 'feature_807': 10620.9423828125,\n",
       " 'feature_808': 54636.01171875,\n",
       " 'feature_809': 29763.078125,\n",
       " 'feature_810': 30245.5859375,\n",
       " 'feature_811': 13672.771484375,\n",
       " 'feature_812': 49630.59375,\n",
       " 'feature_813': 14921.90625,\n",
       " 'feature_814': 42138.296875,\n",
       " 'feature_815': 54628.73046875,\n",
       " 'feature_816': 24722.240234375,\n",
       " 'feature_817': 20730.078125,\n",
       " 'feature_818': 37480.5390625,\n",
       " 'feature_819': 18097.455078125,\n",
       " 'feature_820': 20868.6796875,\n",
       " 'feature_821': 16996.60546875,\n",
       " 'feature_822': 34178.1875,\n",
       " 'feature_823': 23175.150390625,\n",
       " 'feature_824': 14759.341796875,\n",
       " 'feature_825': 90525.3828125,\n",
       " 'feature_826': 30534.271484375,\n",
       " 'feature_827': 12591.310546875,\n",
       " 'feature_828': 34386.92578125,\n",
       " 'feature_829': 42307.984375,\n",
       " 'feature_830': 21863.171875,\n",
       " 'feature_831': 21034.267578125,\n",
       " 'feature_832': 33042.87890625,\n",
       " 'feature_833': 29964.0390625,\n",
       " 'feature_834': 27367.97265625,\n",
       " 'feature_835': 35072.734375,\n",
       " 'feature_836': 23351.3671875,\n",
       " 'feature_837': 20215.396484375,\n",
       " 'feature_838': 50387.08984375,\n",
       " 'feature_839': 15397.62109375,\n",
       " 'feature_840': 34023.72265625,\n",
       " 'feature_841': 64965.46484375,\n",
       " 'feature_842': 15187.26953125,\n",
       " 'feature_843': 74257.5703125,\n",
       " 'feature_844': 50696.41796875,\n",
       " 'feature_845': 50348.21484375,\n",
       " 'feature_846': 22940.8828125,\n",
       " 'feature_847': 16894.154296875,\n",
       " 'feature_848': 12066.380859375,\n",
       " 'feature_849': 90088.8984375,\n",
       " 'feature_850': 27406.3046875,\n",
       " 'feature_851': 31815.6796875,\n",
       " 'feature_852': 14828.275390625,\n",
       " 'feature_853': 51923.51171875,\n",
       " 'feature_854': 26675.4296875,\n",
       " 'feature_855': 66724.6953125,\n",
       " 'feature_856': 15789.6689453125,\n",
       " 'feature_857': 12756.7314453125,\n",
       " 'feature_858': 40239.37890625,\n",
       " 'feature_859': 55564.96484375,\n",
       " 'feature_860': 29137.6875,\n",
       " 'feature_861': 64153.640625,\n",
       " 'feature_862': 42514.14453125,\n",
       " 'feature_863': 23154.10546875,\n",
       " 'feature_864': 23386.720703125,\n",
       " 'feature_865': 26821.359375,\n",
       " 'feature_866': 48524.94921875,\n",
       " 'feature_867': 54232.35546875,\n",
       " 'feature_868': 12387.0615234375,\n",
       " 'feature_869': 19212.328125,\n",
       " 'feature_870': 13585.0732421875,\n",
       " 'feature_871': 14329.732421875,\n",
       " 'feature_872': 60623.12890625,\n",
       " 'feature_873': 35443.28125,\n",
       " 'feature_874': 23369.546875,\n",
       " 'feature_875': 17614.2890625,\n",
       " 'feature_876': 66249.046875,\n",
       " 'feature_877': 41535.35546875,\n",
       " 'feature_878': 53079.98046875,\n",
       " 'feature_879': 13917.4716796875,\n",
       " 'feature_880': 20171.4453125,\n",
       " 'feature_881': 15030.4677734375,\n",
       " 'feature_882': 23418.40625,\n",
       " 'feature_883': 52212.76171875,\n",
       " 'feature_884': 34773.71484375,\n",
       " 'feature_885': 18310.91796875,\n",
       " 'feature_886': 31639.33203125,\n",
       " 'feature_887': 30155.689453125,\n",
       " 'feature_888': 31430.462890625,\n",
       " 'feature_889': 24597.13671875,\n",
       " 'feature_890': 31393.505859375,\n",
       " 'feature_891': 32604.919921875,\n",
       " 'feature_892': 26125.017578125,\n",
       " 'feature_893': 36994.20703125,\n",
       " 'feature_894': 24644.302734375,\n",
       " 'feature_895': 39569.25390625,\n",
       " 'feature_896': 22040.99609375,\n",
       " 'feature_897': 60354.359375,\n",
       " 'feature_898': 35575.6328125,\n",
       " 'feature_899': 29954.060546875,\n",
       " 'feature_900': 30236.75,\n",
       " 'feature_901': 21229.19140625,\n",
       " 'feature_902': 49170.15234375,\n",
       " 'feature_903': 32684.404296875,\n",
       " 'feature_904': 37287.1796875,\n",
       " 'feature_905': 28837.119140625,\n",
       " 'feature_906': 51997.0546875,\n",
       " 'feature_907': 32185.982421875,\n",
       " 'feature_908': 30426.75,\n",
       " 'feature_909': 19733.462890625,\n",
       " 'feature_910': 27605.125,\n",
       " 'feature_911': 11388.0703125,\n",
       " 'feature_912': 53274.29296875,\n",
       " 'feature_913': 22169.4375,\n",
       " 'feature_914': 31308.49609375,\n",
       " 'feature_915': 38527.37890625,\n",
       " 'feature_916': 31151.103515625,\n",
       " 'feature_917': 44415.38671875,\n",
       " 'feature_918': 14391.74609375,\n",
       " 'feature_919': 26743.1171875,\n",
       " 'feature_920': 27262.923828125,\n",
       " 'feature_921': 66997.703125,\n",
       " 'feature_922': 31380.083984375,\n",
       " 'feature_923': 42927.75390625,\n",
       " 'feature_924': 42166.9296875,\n",
       " 'feature_925': 27691.73828125,\n",
       " 'feature_926': 30199.3203125,\n",
       " 'feature_927': 24078.255859375,\n",
       " 'feature_928': 29187.513671875,\n",
       " 'feature_929': 38749.01953125,\n",
       " 'feature_930': 58775.41015625,\n",
       " 'feature_931': 163690.90625,\n",
       " 'feature_932': 63463.9453125,\n",
       " 'feature_933': 12374.37890625,\n",
       " 'feature_934': 55927.9921875,\n",
       " 'feature_935': 47830.76171875,\n",
       " 'feature_936': 31809.025390625,\n",
       " 'feature_937': 77689.1875,\n",
       " 'feature_938': 38545.515625,\n",
       " 'feature_939': 10754.2587890625,\n",
       " 'feature_940': 23324.435546875,\n",
       " 'feature_941': 32605.341796875,\n",
       " 'feature_942': 8973.3310546875,\n",
       " 'feature_943': 28467.02734375,\n",
       " 'feature_944': 9352.142578125,\n",
       " 'feature_945': 25299.06640625,\n",
       " 'feature_946': 41824.86328125,\n",
       " 'feature_947': 39177.33984375,\n",
       " 'feature_948': 47585.078125,\n",
       " 'feature_949': 53115.94140625,\n",
       " 'feature_950': 28970.25,\n",
       " 'feature_951': 32022.0078125,\n",
       " 'feature_952': 32006.423828125,\n",
       " 'feature_953': 24687.1953125,\n",
       " 'feature_954': 71952.953125,\n",
       " 'feature_955': 38510.81640625,\n",
       " 'feature_956': 67659.46875,\n",
       " 'feature_957': 38256.71875,\n",
       " 'feature_958': 19588.90625,\n",
       " 'feature_959': 34295.4140625,\n",
       " 'feature_960': 18386.8359375,\n",
       " 'feature_961': 63263.97265625,\n",
       " 'feature_962': 36798.09375,\n",
       " 'feature_963': 41918.5,\n",
       " 'feature_964': 31168.802734375,\n",
       " 'feature_965': 25336.232421875,\n",
       " 'feature_966': 49355.984375,\n",
       " 'feature_967': 18316.568359375,\n",
       " 'feature_968': 39887.97265625,\n",
       " 'feature_969': 12788.060546875,\n",
       " 'feature_970': 44586.63671875,\n",
       " 'feature_971': 24433.13671875,\n",
       " 'feature_972': 21633.666015625,\n",
       " 'feature_973': 21961.568359375,\n",
       " 'feature_974': 34866.625,\n",
       " 'feature_975': 31039.701171875,\n",
       " 'feature_976': 27442.80859375,\n",
       " 'feature_977': 39554.19921875,\n",
       " 'feature_978': 56085.0078125,\n",
       " 'feature_979': 19737.501953125,\n",
       " 'feature_980': 20093.6875,\n",
       " 'feature_981': 104379.828125,\n",
       " 'feature_982': 18180.15234375,\n",
       " 'feature_983': 26300.5546875,\n",
       " 'feature_984': 30491.845703125,\n",
       " 'feature_985': 25852.099609375,\n",
       " 'feature_986': 20362.955078125,\n",
       " 'feature_987': 84325.3203125,\n",
       " 'feature_988': 89212.8203125,\n",
       " 'feature_989': 40751.23828125,\n",
       " 'feature_990': 29802.150390625,\n",
       " 'feature_991': 32850.1015625,\n",
       " 'feature_992': 18858.201171875,\n",
       " 'feature_993': 24391.552734375,\n",
       " 'feature_994': 10000.552734375,\n",
       " 'feature_995': 41836.45703125,\n",
       " 'feature_996': 21357.515625,\n",
       " 'feature_997': 24862.09375,\n",
       " 'feature_998': 59152.4296875,\n",
       " 'feature_999': 19314.146484375,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Add\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow doesn't like the modin datasets, so we convert them to pandas here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature shapes:\n",
      "X_train_scaled shape: (7286, 1001)\n",
      "X_test_scaled shape: (1822, 1001)\n",
      "Scaled target shapes:\n",
      "y_train_scaled shape: (7286,)\n",
      "y_test_scaled shape: (1822,)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy array if it's a pandas DataFrame\n",
    "X_train2 = X_train.to_numpy()\n",
    "X_test2 = X_test.to_numpy()\n",
    "y_train2 = y_train.to_numpy()\n",
    "y_test2 = y_test.to_numpy()\n",
    "\n",
    "# Create scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = MinMaxScaler()  # Using MinMaxScaler for target to bound values\n",
    "\n",
    "# Scale features (X)\n",
    "X_train_scaled = scaler_X.fit_transform(X_train2)\n",
    "X_test_scaled = scaler_X.transform(X_test2)\n",
    "\n",
    "# Scale target to [0, 1] range (y)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train2.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test2.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Scaled feature shapes:\")\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "\n",
    "print(\"Scaled target shapes:\")\n",
    "print(\"y_train_scaled shape:\", y_train_scaled.shape)\n",
    "print(\"y_test_scaled shape:\", y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, learning_rate=1e-4):\n",
    "    input_layer = Input(shape=(input_dim,))  # The input dimension will now include the new feature\n",
    "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(input_layer)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.35)(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    loss_functions = [\n",
    "        tf.keras.losses.MeanSquaredError(),\n",
    "        tf.keras.losses.MeanAbsoluteError(),\n",
    "        tf.keras.losses.Huber(delta=0.5)  # Adjust delta\n",
    "        ]\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            clipvalue=1.0  # Added clip value\n",
    "        ),\n",
    "        loss=loss_functions,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_file_name, epochs=250, learning_rate=1e-4, batch_size=32):\n",
    "    # Create and train the model\n",
    "    #model = load_model(file_name)# create_model(X_train_scaled.shape[1])\n",
    "    if model_file_name is not None and os.path.exists(model_file_name):\n",
    "        #import the model\n",
    "        model = load_model(model_file_name) # type: ignore\n",
    "    else:\n",
    "        # Create the model\n",
    "        model = create_model(X_train_scaled.shape[1], learning_rate)\n",
    "\n",
    "\n",
    "    # Early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=60, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.75, \n",
    "            patience=10, \n",
    "            min_lr=1e-10\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file_name, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        validation_split=0.2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(model_file_name)\n",
    "    \n",
    "    return model, history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history):\n",
    "    # Predict and inverse transform to get original scale\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Evaluate on original scale\n",
    "    mae = np.mean(np.abs(y_pred.flatten() - y_test2))\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 2s 64ms/step - loss: 1.3581 - mae: 0.8286 - val_loss: 1.2046 - val_mae: 0.7968 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.3419 - mae: 0.3599 - val_loss: 2.8935 - val_mae: 1.3183 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.1738 - mae: 0.2537 - val_loss: 1.0066 - val_mae: 0.7661 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.1076 - mae: 0.1907 - val_loss: 0.7001 - val_mae: 0.6081 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0961 - mae: 0.1729 - val_loss: 0.0876 - val_mae: 0.1606 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0871 - mae: 0.1597 - val_loss: 0.0798 - val_mae: 0.1464 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0835 - mae: 0.1542 - val_loss: 0.0778 - val_mae: 0.1413 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0799 - mae: 0.1499 - val_loss: 0.0736 - val_mae: 0.1351 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0789 - mae: 0.1479 - val_loss: 0.0755 - val_mae: 0.1359 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0766 - mae: 0.1440 - val_loss: 0.0721 - val_mae: 0.1291 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0762 - mae: 0.1421 - val_loss: 0.0703 - val_mae: 0.1265 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0747 - mae: 0.1399 - val_loss: 0.0689 - val_mae: 0.1229 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0728 - mae: 0.1354 - val_loss: 0.0675 - val_mae: 0.1187 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0716 - mae: 0.1325 - val_loss: 0.0661 - val_mae: 0.1166 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0703 - mae: 0.1296 - val_loss: 0.0644 - val_mae: 0.1124 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0688 - mae: 0.1243 - val_loss: 0.0631 - val_mae: 0.1077 - lr: 0.0100\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0676 - mae: 0.1219 - val_loss: 0.0615 - val_mae: 0.1025 - lr: 0.0100\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.0662 - mae: 0.1171 - val_loss: 0.0601 - val_mae: 0.0982 - lr: 0.0100\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0649 - mae: 0.1142 - val_loss: 0.0589 - val_mae: 0.0947 - lr: 0.0100\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0639 - mae: 0.1113 - val_loss: 0.0584 - val_mae: 0.0926 - lr: 0.0100\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0629 - mae: 0.1085 - val_loss: 0.0571 - val_mae: 0.0881 - lr: 0.0100\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0615 - mae: 0.1049 - val_loss: 0.0562 - val_mae: 0.0855 - lr: 0.0100\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0608 - mae: 0.1029 - val_loss: 0.0552 - val_mae: 0.0817 - lr: 0.0100\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0598 - mae: 0.1006 - val_loss: 0.0545 - val_mae: 0.0794 - lr: 0.0100\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0590 - mae: 0.0974 - val_loss: 0.0539 - val_mae: 0.0779 - lr: 0.0100\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0580 - mae: 0.0946 - val_loss: 0.0533 - val_mae: 0.0760 - lr: 0.0100\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0570 - mae: 0.0917 - val_loss: 0.0527 - val_mae: 0.0736 - lr: 0.0100\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0569 - mae: 0.0920 - val_loss: 0.0523 - val_mae: 0.0724 - lr: 0.0100\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0561 - mae: 0.0898 - val_loss: 0.0519 - val_mae: 0.0712 - lr: 0.0100\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0554 - mae: 0.0879 - val_loss: 0.0511 - val_mae: 0.0688 - lr: 0.0100\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0545 - mae: 0.0849 - val_loss: 0.0507 - val_mae: 0.0680 - lr: 0.0100\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0541 - mae: 0.0843 - val_loss: 0.0503 - val_mae: 0.0670 - lr: 0.0100\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0532 - mae: 0.0808 - val_loss: 0.0500 - val_mae: 0.0655 - lr: 0.0100\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0530 - mae: 0.0804 - val_loss: 0.0497 - val_mae: 0.0649 - lr: 0.0100\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0526 - mae: 0.0801 - val_loss: 0.0499 - val_mae: 0.0678 - lr: 0.0100\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0525 - mae: 0.0798 - val_loss: 0.0490 - val_mae: 0.0637 - lr: 0.0100\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0516 - mae: 0.0770 - val_loss: 0.0486 - val_mae: 0.0622 - lr: 0.0100\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0512 - mae: 0.0760 - val_loss: 0.0484 - val_mae: 0.0623 - lr: 0.0100\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0509 - mae: 0.0753 - val_loss: 0.0478 - val_mae: 0.0605 - lr: 0.0100\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0504 - mae: 0.0744 - val_loss: 0.0478 - val_mae: 0.0607 - lr: 0.0100\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0504 - mae: 0.0742 - val_loss: 0.0474 - val_mae: 0.0595 - lr: 0.0100\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0495 - mae: 0.0717 - val_loss: 0.0471 - val_mae: 0.0596 - lr: 0.0100\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0490 - mae: 0.0711 - val_loss: 0.0469 - val_mae: 0.0592 - lr: 0.0100\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0490 - mae: 0.0712 - val_loss: 0.0465 - val_mae: 0.0577 - lr: 0.0100\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0487 - mae: 0.0703 - val_loss: 0.0467 - val_mae: 0.0593 - lr: 0.0100\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0484 - mae: 0.0698 - val_loss: 0.0461 - val_mae: 0.0585 - lr: 0.0100\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0480 - mae: 0.0697 - val_loss: 0.0461 - val_mae: 0.0591 - lr: 0.0100\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0478 - mae: 0.0683 - val_loss: 0.0455 - val_mae: 0.0566 - lr: 0.0100\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0473 - mae: 0.0682 - val_loss: 0.0452 - val_mae: 0.0555 - lr: 0.0100\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0472 - mae: 0.0679 - val_loss: 0.0455 - val_mae: 0.0582 - lr: 0.0100\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0469 - mae: 0.0673 - val_loss: 0.0449 - val_mae: 0.0564 - lr: 0.0100\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0464 - mae: 0.0665 - val_loss: 0.0445 - val_mae: 0.0548 - lr: 0.0100\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0461 - mae: 0.0659 - val_loss: 0.0443 - val_mae: 0.0543 - lr: 0.0100\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0457 - mae: 0.0647 - val_loss: 0.0442 - val_mae: 0.0556 - lr: 0.0100\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.0457 - mae: 0.0654 - val_loss: 0.0439 - val_mae: 0.0543 - lr: 0.0100\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0454 - mae: 0.0648 - val_loss: 0.0436 - val_mae: 0.0538 - lr: 0.0100\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0450 - mae: 0.0643 - val_loss: 0.0434 - val_mae: 0.0539 - lr: 0.0100\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0449 - mae: 0.0640 - val_loss: 0.0430 - val_mae: 0.0526 - lr: 0.0100\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0445 - mae: 0.0633 - val_loss: 0.0429 - val_mae: 0.0533 - lr: 0.0100\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0441 - mae: 0.0630 - val_loss: 0.0427 - val_mae: 0.0529 - lr: 0.0100\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0437 - mae: 0.0614 - val_loss: 0.0425 - val_mae: 0.0536 - lr: 0.0100\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.0435 - mae: 0.0612 - val_loss: 0.0424 - val_mae: 0.0542 - lr: 0.0100\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0433 - mae: 0.0620 - val_loss: 0.0422 - val_mae: 0.0552 - lr: 0.0100\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0430 - mae: 0.0614 - val_loss: 0.0417 - val_mae: 0.0522 - lr: 0.0100\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0426 - mae: 0.0606 - val_loss: 0.0416 - val_mae: 0.0525 - lr: 0.0100\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0425 - mae: 0.0607 - val_loss: 0.0412 - val_mae: 0.0513 - lr: 0.0100\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0420 - mae: 0.0591 - val_loss: 0.0412 - val_mae: 0.0527 - lr: 0.0100\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0419 - mae: 0.0598 - val_loss: 0.0407 - val_mae: 0.0508 - lr: 0.0100\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0414 - mae: 0.0580 - val_loss: 0.0405 - val_mae: 0.0504 - lr: 0.0100\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0412 - mae: 0.0576 - val_loss: 0.0407 - val_mae: 0.0539 - lr: 0.0100\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0411 - mae: 0.0586 - val_loss: 0.0403 - val_mae: 0.0514 - lr: 0.0100\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0408 - mae: 0.0577 - val_loss: 0.0403 - val_mae: 0.0532 - lr: 0.0100\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0406 - mae: 0.0580 - val_loss: 0.0400 - val_mae: 0.0525 - lr: 0.0100\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0403 - mae: 0.0578 - val_loss: 0.0395 - val_mae: 0.0508 - lr: 0.0100\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0402 - mae: 0.0581 - val_loss: 0.0394 - val_mae: 0.0512 - lr: 0.0100\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0397 - mae: 0.0573 - val_loss: 0.0393 - val_mae: 0.0522 - lr: 0.0100\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0394 - mae: 0.0564 - val_loss: 0.0388 - val_mae: 0.0513 - lr: 0.0100\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0393 - mae: 0.0566 - val_loss: 0.0387 - val_mae: 0.0516 - lr: 0.0100\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0391 - mae: 0.0566 - val_loss: 0.0383 - val_mae: 0.0502 - lr: 0.0100\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0386 - mae: 0.0558 - val_loss: 0.0383 - val_mae: 0.0515 - lr: 0.0100\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0384 - mae: 0.0550 - val_loss: 0.0382 - val_mae: 0.0522 - lr: 0.0100\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0384 - mae: 0.0562 - val_loss: 0.0376 - val_mae: 0.0491 - lr: 0.0100\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0379 - mae: 0.0556 - val_loss: 0.0373 - val_mae: 0.0488 - lr: 0.0100\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0378 - mae: 0.0550 - val_loss: 0.0379 - val_mae: 0.0542 - lr: 0.0100\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0375 - mae: 0.0547 - val_loss: 0.0369 - val_mae: 0.0483 - lr: 0.0100\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0372 - mae: 0.0539 - val_loss: 0.0366 - val_mae: 0.0485 - lr: 0.0100\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0371 - mae: 0.0553 - val_loss: 0.0365 - val_mae: 0.0490 - lr: 0.0100\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0365 - mae: 0.0530 - val_loss: 0.0363 - val_mae: 0.0493 - lr: 0.0100\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0363 - mae: 0.0531 - val_loss: 0.0363 - val_mae: 0.0505 - lr: 0.0100\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0361 - mae: 0.0525 - val_loss: 0.0362 - val_mae: 0.0524 - lr: 0.0100\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0356 - mae: 0.0515 - val_loss: 0.0355 - val_mae: 0.0480 - lr: 0.0100\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0355 - mae: 0.0520 - val_loss: 0.0353 - val_mae: 0.0478 - lr: 0.0100\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0356 - mae: 0.0541 - val_loss: 0.0350 - val_mae: 0.0476 - lr: 0.0100\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0354 - mae: 0.0539 - val_loss: 0.0347 - val_mae: 0.0477 - lr: 0.0100\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0348 - mae: 0.0519 - val_loss: 0.0348 - val_mae: 0.0499 - lr: 0.0100\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0346 - mae: 0.0525 - val_loss: 0.0355 - val_mae: 0.0566 - lr: 0.0100\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0348 - mae: 0.0534 - val_loss: 0.0351 - val_mae: 0.0549 - lr: 0.0100\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0345 - mae: 0.0537 - val_loss: 0.0354 - val_mae: 0.0584 - lr: 0.0100\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0345 - mae: 0.0549 - val_loss: 0.0353 - val_mae: 0.0589 - lr: 0.0100\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0337 - mae: 0.0514 - val_loss: 0.0336 - val_mae: 0.0485 - lr: 0.0100\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0333 - mae: 0.0510 - val_loss: 0.0332 - val_mae: 0.0469 - lr: 0.0100\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0331 - mae: 0.0511 - val_loss: 0.0329 - val_mae: 0.0460 - lr: 0.0100\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0330 - mae: 0.0509 - val_loss: 0.0333 - val_mae: 0.0515 - lr: 0.0100\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0326 - mae: 0.0501 - val_loss: 0.0325 - val_mae: 0.0469 - lr: 0.0100\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0321 - mae: 0.0489 - val_loss: 0.0338 - val_mae: 0.0584 - lr: 0.0100\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0326 - mae: 0.0525 - val_loss: 0.0322 - val_mae: 0.0479 - lr: 0.0100\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0321 - mae: 0.0510 - val_loss: 0.0320 - val_mae: 0.0479 - lr: 0.0100\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0316 - mae: 0.0490 - val_loss: 0.0315 - val_mae: 0.0455 - lr: 0.0100\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0312 - mae: 0.0481 - val_loss: 0.0317 - val_mae: 0.0490 - lr: 0.0100\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0311 - mae: 0.0485 - val_loss: 0.0321 - val_mae: 0.0538 - lr: 0.0100\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0315 - mae: 0.0522 - val_loss: 0.0310 - val_mae: 0.0467 - lr: 0.0100\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0305 - mae: 0.0477 - val_loss: 0.0314 - val_mae: 0.0532 - lr: 0.0100\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0305 - mae: 0.0488 - val_loss: 0.0308 - val_mae: 0.0488 - lr: 0.0100\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0300 - mae: 0.0472 - val_loss: 0.0302 - val_mae: 0.0456 - lr: 0.0100\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0297 - mae: 0.0470 - val_loss: 0.0300 - val_mae: 0.0458 - lr: 0.0100\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0295 - mae: 0.0468 - val_loss: 0.0301 - val_mae: 0.0481 - lr: 0.0100\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0291 - mae: 0.0460 - val_loss: 0.0298 - val_mae: 0.0484 - lr: 0.0100\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0291 - mae: 0.0471 - val_loss: 0.0293 - val_mae: 0.0449 - lr: 0.0100\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0291 - mae: 0.0484 - val_loss: 0.0292 - val_mae: 0.0454 - lr: 0.0100\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0285 - mae: 0.0458 - val_loss: 0.0288 - val_mae: 0.0446 - lr: 0.0100\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0283 - mae: 0.0459 - val_loss: 0.0290 - val_mae: 0.0474 - lr: 0.0100\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0281 - mae: 0.0454 - val_loss: 0.0294 - val_mae: 0.0534 - lr: 0.0100\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0281 - mae: 0.0468 - val_loss: 0.0298 - val_mae: 0.0579 - lr: 0.0100\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0280 - mae: 0.0479 - val_loss: 0.0298 - val_mae: 0.0588 - lr: 0.0100\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0275 - mae: 0.0463 - val_loss: 0.0300 - val_mae: 0.0617 - lr: 0.0100\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0279 - mae: 0.0500 - val_loss: 0.0288 - val_mae: 0.0553 - lr: 0.0100\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0277 - mae: 0.0497 - val_loss: 0.0286 - val_mae: 0.0551 - lr: 0.0100\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0268 - mae: 0.0455 - val_loss: 0.0275 - val_mae: 0.0478 - lr: 0.0100\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0265 - mae: 0.0443 - val_loss: 0.0271 - val_mae: 0.0455 - lr: 0.0100\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0264 - mae: 0.0454 - val_loss: 0.0267 - val_mae: 0.0440 - lr: 0.0100\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0261 - mae: 0.0450 - val_loss: 0.0268 - val_mae: 0.0470 - lr: 0.0100\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0257 - mae: 0.0435 - val_loss: 0.0263 - val_mae: 0.0445 - lr: 0.0100\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0256 - mae: 0.0442 - val_loss: 0.0266 - val_mae: 0.0488 - lr: 0.0100\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0252 - mae: 0.0428 - val_loss: 0.0273 - val_mae: 0.0562 - lr: 0.0100\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0251 - mae: 0.0439 - val_loss: 0.0267 - val_mae: 0.0532 - lr: 0.0100\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0249 - mae: 0.0435 - val_loss: 0.0259 - val_mae: 0.0483 - lr: 0.0100\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0244 - mae: 0.0419 - val_loss: 0.0255 - val_mae: 0.0460 - lr: 0.0100\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0247 - mae: 0.0447 - val_loss: 0.0261 - val_mae: 0.0531 - lr: 0.0100\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0244 - mae: 0.0435 - val_loss: 0.0259 - val_mae: 0.0535 - lr: 0.0100\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0240 - mae: 0.0434 - val_loss: 0.0253 - val_mae: 0.0493 - lr: 0.0100\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0240 - mae: 0.0442 - val_loss: 0.0258 - val_mae: 0.0546 - lr: 0.0100\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0238 - mae: 0.0438 - val_loss: 0.0246 - val_mae: 0.0468 - lr: 0.0100\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0234 - mae: 0.0425 - val_loss: 0.0241 - val_mae: 0.0437 - lr: 0.0100\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0231 - mae: 0.0419 - val_loss: 0.0251 - val_mae: 0.0540 - lr: 0.0100\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0229 - mae: 0.0423 - val_loss: 0.0239 - val_mae: 0.0460 - lr: 0.0100\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0226 - mae: 0.0416 - val_loss: 0.0234 - val_mae: 0.0429 - lr: 0.0100\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0225 - mae: 0.0425 - val_loss: 0.0234 - val_mae: 0.0448 - lr: 0.0100\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0226 - mae: 0.0443 - val_loss: 0.0231 - val_mae: 0.0439 - lr: 0.0100\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0221 - mae: 0.0419 - val_loss: 0.0235 - val_mae: 0.0488 - lr: 0.0100\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0218 - mae: 0.0412 - val_loss: 0.0230 - val_mae: 0.0459 - lr: 0.0100\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0219 - mae: 0.0429 - val_loss: 0.0241 - val_mae: 0.0576 - lr: 0.0100\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0217 - mae: 0.0426 - val_loss: 0.0242 - val_mae: 0.0597 - lr: 0.0100\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0215 - mae: 0.0429 - val_loss: 0.0238 - val_mae: 0.0579 - lr: 0.0100\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0217 - mae: 0.0454 - val_loss: 0.0222 - val_mae: 0.0456 - lr: 0.0100\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0211 - mae: 0.0429 - val_loss: 0.0217 - val_mae: 0.0428 - lr: 0.0100\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0206 - mae: 0.0412 - val_loss: 0.0223 - val_mae: 0.0505 - lr: 0.0100\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0203 - mae: 0.0393 - val_loss: 0.0215 - val_mae: 0.0450 - lr: 0.0100\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0205 - mae: 0.0424 - val_loss: 0.0243 - val_mae: 0.0662 - lr: 0.0100\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0211 - mae: 0.0475 - val_loss: 0.0225 - val_mae: 0.0558 - lr: 0.0100\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0202 - mae: 0.0417 - val_loss: 0.0233 - val_mae: 0.0624 - lr: 0.0100\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0201 - mae: 0.0434 - val_loss: 0.0227 - val_mae: 0.0603 - lr: 0.0100\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0200 - mae: 0.0436 - val_loss: 0.0233 - val_mae: 0.0651 - lr: 0.0100\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0197 - mae: 0.0426 - val_loss: 0.0236 - val_mae: 0.0677 - lr: 0.0100\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0197 - mae: 0.0437 - val_loss: 0.0222 - val_mae: 0.0606 - lr: 0.0100\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0194 - mae: 0.0437 - val_loss: 0.0208 - val_mae: 0.0520 - lr: 0.0100\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0190 - mae: 0.0410 - val_loss: 0.0203 - val_mae: 0.0490 - lr: 0.0100\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0189 - mae: 0.0416 - val_loss: 0.0198 - val_mae: 0.0452 - lr: 0.0100\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0184 - mae: 0.0397 - val_loss: 0.0196 - val_mae: 0.0440 - lr: 0.0100\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0183 - mae: 0.0392 - val_loss: 0.0193 - val_mae: 0.0439 - lr: 0.0100\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0180 - mae: 0.0393 - val_loss: 0.0190 - val_mae: 0.0418 - lr: 0.0100\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0180 - mae: 0.0403 - val_loss: 0.0189 - val_mae: 0.0429 - lr: 0.0100\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0178 - mae: 0.0400 - val_loss: 0.0199 - val_mae: 0.0524 - lr: 0.0100\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0176 - mae: 0.0392 - val_loss: 0.0196 - val_mae: 0.0525 - lr: 0.0100\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0174 - mae: 0.0392 - val_loss: 0.0191 - val_mae: 0.0496 - lr: 0.0100\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0170 - mae: 0.0381 - val_loss: 0.0192 - val_mae: 0.0515 - lr: 0.0100\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0170 - mae: 0.0383 - val_loss: 0.0180 - val_mae: 0.0417 - lr: 0.0100\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0174 - mae: 0.0424 - val_loss: 0.0183 - val_mae: 0.0472 - lr: 0.0100\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0174 - mae: 0.0437 - val_loss: 0.0189 - val_mae: 0.0534 - lr: 0.0100\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0170 - mae: 0.0417 - val_loss: 0.0213 - val_mae: 0.0700 - lr: 0.0100\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0176 - mae: 0.0469 - val_loss: 0.0197 - val_mae: 0.0611 - lr: 0.0100\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0170 - mae: 0.0436 - val_loss: 0.0185 - val_mae: 0.0541 - lr: 0.0100\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0168 - mae: 0.0443 - val_loss: 0.0177 - val_mae: 0.0491 - lr: 0.0100\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0158 - mae: 0.0384 - val_loss: 0.0175 - val_mae: 0.0483 - lr: 0.0100\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0156 - mae: 0.0383 - val_loss: 0.0187 - val_mae: 0.0590 - lr: 0.0100\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0161 - mae: 0.0420 - val_loss: 0.0180 - val_mae: 0.0549 - lr: 0.0100\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0155 - mae: 0.0389 - val_loss: 0.0182 - val_mae: 0.0573 - lr: 0.0100\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0153 - mae: 0.0388 - val_loss: 0.0181 - val_mae: 0.0574 - lr: 0.0100\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0154 - mae: 0.0412 - val_loss: 0.0172 - val_mae: 0.0521 - lr: 0.0100\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0149 - mae: 0.0381 - val_loss: 0.0177 - val_mae: 0.0575 - lr: 0.0100\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0152 - mae: 0.0426 - val_loss: 0.0173 - val_mae: 0.0545 - lr: 0.0100\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0145 - mae: 0.0369 - val_loss: 0.0164 - val_mae: 0.0481 - lr: 0.0100\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0144 - mae: 0.0372 - val_loss: 0.0182 - val_mae: 0.0632 - lr: 0.0100\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0144 - mae: 0.0387 - val_loss: 0.0169 - val_mae: 0.0555 - lr: 0.0100\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0143 - mae: 0.0387 - val_loss: 0.0166 - val_mae: 0.0536 - lr: 0.0100\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0140 - mae: 0.0379 - val_loss: 0.0168 - val_mae: 0.0558 - lr: 0.0100\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0143 - mae: 0.0407 - val_loss: 0.0166 - val_mae: 0.0556 - lr: 0.0100\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0142 - mae: 0.0408 - val_loss: 0.0176 - val_mae: 0.0632 - lr: 0.0100\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0140 - mae: 0.0413 - val_loss: 0.0170 - val_mae: 0.0604 - lr: 0.0100\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0136 - mae: 0.0383 - val_loss: 0.0165 - val_mae: 0.0579 - lr: 0.0100\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0133 - mae: 0.0375 - val_loss: 0.0147 - val_mae: 0.0452 - lr: 0.0100\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0129 - mae: 0.0355 - val_loss: 0.0147 - val_mae: 0.0469 - lr: 0.0100\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0128 - mae: 0.0359 - val_loss: 0.0144 - val_mae: 0.0442 - lr: 0.0100\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0126 - mae: 0.0353 - val_loss: 0.0143 - val_mae: 0.0457 - lr: 0.0100\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0125 - mae: 0.0355 - val_loss: 0.0145 - val_mae: 0.0475 - lr: 0.0100\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0124 - mae: 0.0352 - val_loss: 0.0148 - val_mae: 0.0509 - lr: 0.0100\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0124 - mae: 0.0370 - val_loss: 0.0143 - val_mae: 0.0481 - lr: 0.0100\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0121 - mae: 0.0353 - val_loss: 0.0149 - val_mae: 0.0541 - lr: 0.0100\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0123 - mae: 0.0374 - val_loss: 0.0137 - val_mae: 0.0456 - lr: 0.0100\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0119 - mae: 0.0357 - val_loss: 0.0134 - val_mae: 0.0421 - lr: 0.0100\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0117 - mae: 0.0347 - val_loss: 0.0133 - val_mae: 0.0424 - lr: 0.0100\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0116 - mae: 0.0351 - val_loss: 0.0129 - val_mae: 0.0408 - lr: 0.0100\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0114 - mae: 0.0341 - val_loss: 0.0139 - val_mae: 0.0514 - lr: 0.0100\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0114 - mae: 0.0352 - val_loss: 0.0138 - val_mae: 0.0509 - lr: 0.0100\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0113 - mae: 0.0354 - val_loss: 0.0133 - val_mae: 0.0481 - lr: 0.0100\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0113 - mae: 0.0367 - val_loss: 0.0129 - val_mae: 0.0458 - lr: 0.0100\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0110 - mae: 0.0354 - val_loss: 0.0140 - val_mae: 0.0556 - lr: 0.0100\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0110 - mae: 0.0359 - val_loss: 0.0150 - val_mae: 0.0631 - lr: 0.0100\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0111 - mae: 0.0377 - val_loss: 0.0126 - val_mae: 0.0454 - lr: 0.0100\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0107 - mae: 0.0354 - val_loss: 0.0125 - val_mae: 0.0453 - lr: 0.0100\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0108 - mae: 0.0367 - val_loss: 0.0124 - val_mae: 0.0450 - lr: 0.0100\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0105 - mae: 0.0348 - val_loss: 0.0118 - val_mae: 0.0396 - lr: 0.0100\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0105 - mae: 0.0359 - val_loss: 0.0118 - val_mae: 0.0421 - lr: 0.0100\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0104 - mae: 0.0361 - val_loss: 0.0126 - val_mae: 0.0499 - lr: 0.0100\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0107 - mae: 0.0391 - val_loss: 0.0126 - val_mae: 0.0506 - lr: 0.0100\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0103 - mae: 0.0368 - val_loss: 0.0115 - val_mae: 0.0422 - lr: 0.0100\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0100 - mae: 0.0353 - val_loss: 0.0113 - val_mae: 0.0414 - lr: 0.0100\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0097 - mae: 0.0350 - val_loss: 0.0111 - val_mae: 0.0398 - lr: 0.0100\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0096 - mae: 0.0339 - val_loss: 0.0110 - val_mae: 0.0399 - lr: 0.0100\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0094 - mae: 0.0335 - val_loss: 0.0113 - val_mae: 0.0446 - lr: 0.0100\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0093 - mae: 0.0327 - val_loss: 0.0110 - val_mae: 0.0414 - lr: 0.0100\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0093 - mae: 0.0339 - val_loss: 0.0109 - val_mae: 0.0416 - lr: 0.0100\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0095 - mae: 0.0361 - val_loss: 0.0111 - val_mae: 0.0437 - lr: 0.0100\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0091 - mae: 0.0337 - val_loss: 0.0110 - val_mae: 0.0452 - lr: 0.0100\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0092 - mae: 0.0351 - val_loss: 0.0107 - val_mae: 0.0438 - lr: 0.0100\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0091 - mae: 0.0348 - val_loss: 0.0105 - val_mae: 0.0428 - lr: 0.0100\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0087 - mae: 0.0329 - val_loss: 0.0107 - val_mae: 0.0445 - lr: 0.0100\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0086 - mae: 0.0329 - val_loss: 0.0113 - val_mae: 0.0500 - lr: 0.0100\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0088 - mae: 0.0348 - val_loss: 0.0110 - val_mae: 0.0496 - lr: 0.0100\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0087 - mae: 0.0349 - val_loss: 0.0104 - val_mae: 0.0438 - lr: 0.0100\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0089 - mae: 0.0376 - val_loss: 0.0098 - val_mae: 0.0389 - lr: 0.0100\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0087 - mae: 0.0362 - val_loss: 0.0097 - val_mae: 0.0375 - lr: 0.0100\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0084 - mae: 0.0339 - val_loss: 0.0098 - val_mae: 0.0404 - lr: 0.0100\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0082 - mae: 0.0334 - val_loss: 0.0106 - val_mae: 0.0494 - lr: 0.0100\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0082 - mae: 0.0336 - val_loss: 0.0095 - val_mae: 0.0401 - lr: 0.0100\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0079 - mae: 0.0329 - val_loss: 0.0094 - val_mae: 0.0392 - lr: 0.0100\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0080 - mae: 0.0332 - val_loss: 0.0092 - val_mae: 0.0375 - lr: 0.0100\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0080 - mae: 0.0342 - val_loss: 0.0098 - val_mae: 0.0452 - lr: 0.0100\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0078 - mae: 0.0336 - val_loss: 0.0091 - val_mae: 0.0377 - lr: 0.0100\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0076 - mae: 0.0321 - val_loss: 0.0093 - val_mae: 0.0406 - lr: 0.0100\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0075 - mae: 0.0326 - val_loss: 0.0090 - val_mae: 0.0374 - lr: 0.0100\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0076 - mae: 0.0338 - val_loss: 0.0091 - val_mae: 0.0407 - lr: 0.0100\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0075 - mae: 0.0337 - val_loss: 0.0096 - val_mae: 0.0476 - lr: 0.0100\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0075 - mae: 0.0343 - val_loss: 0.0096 - val_mae: 0.0478 - lr: 0.0100\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0073 - mae: 0.0333 - val_loss: 0.0094 - val_mae: 0.0459 - lr: 0.0100\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0072 - mae: 0.0330 - val_loss: 0.0089 - val_mae: 0.0415 - lr: 0.0100\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0071 - mae: 0.0322 - val_loss: 0.0101 - val_mae: 0.0532 - lr: 0.0100\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0074 - mae: 0.0350 - val_loss: 0.0085 - val_mae: 0.0378 - lr: 0.0100\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0075 - mae: 0.0367 - val_loss: 0.0087 - val_mae: 0.0411 - lr: 0.0100\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0072 - mae: 0.0345 - val_loss: 0.0101 - val_mae: 0.0540 - lr: 0.0100\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0071 - mae: 0.0347 - val_loss: 0.0084 - val_mae: 0.0389 - lr: 0.0100\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0068 - mae: 0.0332 - val_loss: 0.0082 - val_mae: 0.0387 - lr: 0.0100\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0071 - mae: 0.0354 - val_loss: 0.0086 - val_mae: 0.0434 - lr: 0.0100\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0072 - mae: 0.0370 - val_loss: 0.0094 - val_mae: 0.0517 - lr: 0.0100\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0066 - mae: 0.0324 - val_loss: 0.0096 - val_mae: 0.0523 - lr: 0.0100\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0068 - mae: 0.0347 - val_loss: 0.0098 - val_mae: 0.0547 - lr: 0.0100\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0065 - mae: 0.0327 - val_loss: 0.0098 - val_mae: 0.0560 - lr: 0.0100\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0065 - mae: 0.0331 - val_loss: 0.0080 - val_mae: 0.0394 - lr: 0.0100\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0062 - mae: 0.0319 - val_loss: 0.0079 - val_mae: 0.0395 - lr: 0.0100\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0061 - mae: 0.0313 - val_loss: 0.0078 - val_mae: 0.0381 - lr: 0.0100\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0063 - mae: 0.0331 - val_loss: 0.0078 - val_mae: 0.0399 - lr: 0.0100\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0065 - mae: 0.0350 - val_loss: 0.0087 - val_mae: 0.0488 - lr: 0.0100\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0062 - mae: 0.0327 - val_loss: 0.0079 - val_mae: 0.0412 - lr: 0.0100\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0060 - mae: 0.0312 - val_loss: 0.0076 - val_mae: 0.0385 - lr: 0.0100\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0059 - mae: 0.0316 - val_loss: 0.0077 - val_mae: 0.0412 - lr: 0.0100\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0059 - mae: 0.0321 - val_loss: 0.0074 - val_mae: 0.0382 - lr: 0.0100\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0058 - mae: 0.0309 - val_loss: 0.0072 - val_mae: 0.0378 - lr: 0.0100\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0059 - mae: 0.0325 - val_loss: 0.0083 - val_mae: 0.0480 - lr: 0.0100\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0058 - mae: 0.0322 - val_loss: 0.0075 - val_mae: 0.0400 - lr: 0.0100\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0058 - mae: 0.0324 - val_loss: 0.0073 - val_mae: 0.0391 - lr: 0.0100\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0057 - mae: 0.0326 - val_loss: 0.0079 - val_mae: 0.0463 - lr: 0.0100\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0061 - mae: 0.0365 - val_loss: 0.0078 - val_mae: 0.0455 - lr: 0.0100\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0053 - mae: 0.0295 - val_loss: 0.0070 - val_mae: 0.0376 - lr: 0.0100\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0054 - mae: 0.0315 - val_loss: 0.0068 - val_mae: 0.0366 - lr: 0.0100\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0054 - mae: 0.0311 - val_loss: 0.0067 - val_mae: 0.0370 - lr: 0.0100\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0055 - mae: 0.0324 - val_loss: 0.0078 - val_mae: 0.0481 - lr: 0.0100\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0054 - mae: 0.0315 - val_loss: 0.0072 - val_mae: 0.0422 - lr: 0.0100\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0057 - mae: 0.0346 - val_loss: 0.0067 - val_mae: 0.0374 - lr: 0.0100\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0057 - mae: 0.0351 - val_loss: 0.0068 - val_mae: 0.0379 - lr: 0.0100\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0054 - mae: 0.0323 - val_loss: 0.0064 - val_mae: 0.0346 - lr: 0.0100\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0053 - mae: 0.0320 - val_loss: 0.0064 - val_mae: 0.0345 - lr: 0.0100\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0051 - mae: 0.0312 - val_loss: 0.0065 - val_mae: 0.0352 - lr: 0.0100\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0051 - mae: 0.0321 - val_loss: 0.0064 - val_mae: 0.0361 - lr: 0.0100\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0050 - mae: 0.0312 - val_loss: 0.0063 - val_mae: 0.0359 - lr: 0.0100\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0051 - mae: 0.0325 - val_loss: 0.0063 - val_mae: 0.0360 - lr: 0.0100\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0051 - mae: 0.0323 - val_loss: 0.0067 - val_mae: 0.0418 - lr: 0.0100\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0048 - mae: 0.0304 - val_loss: 0.0061 - val_mae: 0.0363 - lr: 0.0100\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0047 - mae: 0.0297 - val_loss: 0.0066 - val_mae: 0.0387 - lr: 0.0100\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0047 - mae: 0.0301 - val_loss: 0.0065 - val_mae: 0.0402 - lr: 0.0100\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0048 - mae: 0.0313 - val_loss: 0.0066 - val_mae: 0.0410 - lr: 0.0100\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0046 - mae: 0.0299 - val_loss: 0.0063 - val_mae: 0.0396 - lr: 0.0100\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0045 - mae: 0.0293 - val_loss: 0.0063 - val_mae: 0.0399 - lr: 0.0100\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0045 - mae: 0.0293 - val_loss: 0.0066 - val_mae: 0.0413 - lr: 0.0100\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0049 - mae: 0.0336 - val_loss: 0.0076 - val_mae: 0.0524 - lr: 0.0100\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0049 - mae: 0.0333 - val_loss: 0.0059 - val_mae: 0.0342 - lr: 0.0100\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0045 - mae: 0.0304 - val_loss: 0.0063 - val_mae: 0.0384 - lr: 0.0100\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0044 - mae: 0.0295 - val_loss: 0.0060 - val_mae: 0.0356 - lr: 0.0100\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0044 - mae: 0.0297 - val_loss: 0.0060 - val_mae: 0.0362 - lr: 0.0100\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0043 - mae: 0.0297 - val_loss: 0.0058 - val_mae: 0.0366 - lr: 0.0100\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0042 - mae: 0.0286 - val_loss: 0.0061 - val_mae: 0.0401 - lr: 0.0100\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0042 - mae: 0.0289 - val_loss: 0.0061 - val_mae: 0.0401 - lr: 0.0100\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0043 - mae: 0.0297 - val_loss: 0.0063 - val_mae: 0.0363 - lr: 0.0100\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0045 - mae: 0.0327 - val_loss: 0.0059 - val_mae: 0.0361 - lr: 0.0100\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0043 - mae: 0.0299 - val_loss: 0.0063 - val_mae: 0.0421 - lr: 0.0100\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0042 - mae: 0.0296 - val_loss: 0.0066 - val_mae: 0.0441 - lr: 0.0100\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0043 - mae: 0.0304 - val_loss: 0.0060 - val_mae: 0.0380 - lr: 0.0100\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0045 - mae: 0.0324 - val_loss: 0.0067 - val_mae: 0.0432 - lr: 0.0100\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0049 - mae: 0.0367 - val_loss: 0.0062 - val_mae: 0.0369 - lr: 0.0100\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0048 - mae: 0.0348 - val_loss: 0.0066 - val_mae: 0.0415 - lr: 0.0100\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0046 - mae: 0.0326 - val_loss: 0.0058 - val_mae: 0.0350 - lr: 0.0075\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0044 - mae: 0.0325 - val_loss: 0.0058 - val_mae: 0.0354 - lr: 0.0075\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0042 - mae: 0.0301 - val_loss: 0.0056 - val_mae: 0.0336 - lr: 0.0075\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0040 - mae: 0.0296 - val_loss: 0.0063 - val_mae: 0.0447 - lr: 0.0075\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0041 - mae: 0.0298 - val_loss: 0.0057 - val_mae: 0.0386 - lr: 0.0075\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0039 - mae: 0.0281 - val_loss: 0.0056 - val_mae: 0.0364 - lr: 0.0075\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0039 - mae: 0.0287 - val_loss: 0.0056 - val_mae: 0.0384 - lr: 0.0075\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0038 - mae: 0.0280 - val_loss: 0.0054 - val_mae: 0.0354 - lr: 0.0075\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0037 - mae: 0.0275 - val_loss: 0.0053 - val_mae: 0.0335 - lr: 0.0075\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0038 - mae: 0.0290 - val_loss: 0.0054 - val_mae: 0.0353 - lr: 0.0075\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0037 - mae: 0.0286 - val_loss: 0.0055 - val_mae: 0.0394 - lr: 0.0075\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0038 - mae: 0.0299 - val_loss: 0.0052 - val_mae: 0.0328 - lr: 0.0075\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0038 - mae: 0.0306 - val_loss: 0.0056 - val_mae: 0.0407 - lr: 0.0075\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0037 - mae: 0.0296 - val_loss: 0.0057 - val_mae: 0.0408 - lr: 0.0075\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0038 - mae: 0.0296 - val_loss: 0.0055 - val_mae: 0.0395 - lr: 0.0075\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0038 - mae: 0.0300 - val_loss: 0.0051 - val_mae: 0.0334 - lr: 0.0075\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0038 - mae: 0.0305 - val_loss: 0.0053 - val_mae: 0.0360 - lr: 0.0075\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0037 - mae: 0.0290 - val_loss: 0.0056 - val_mae: 0.0408 - lr: 0.0075\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0036 - mae: 0.0291 - val_loss: 0.0051 - val_mae: 0.0345 - lr: 0.0075\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0034 - mae: 0.0280 - val_loss: 0.0052 - val_mae: 0.0343 - lr: 0.0075\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0035 - mae: 0.0286 - val_loss: 0.0050 - val_mae: 0.0321 - lr: 0.0075\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0034 - mae: 0.0275 - val_loss: 0.0059 - val_mae: 0.0450 - lr: 0.0075\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0036 - mae: 0.0296 - val_loss: 0.0055 - val_mae: 0.0378 - lr: 0.0075\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0035 - mae: 0.0283 - val_loss: 0.0056 - val_mae: 0.0375 - lr: 0.0075\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0035 - mae: 0.0292 - val_loss: 0.0054 - val_mae: 0.0378 - lr: 0.0075\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0034 - mae: 0.0275 - val_loss: 0.0056 - val_mae: 0.0407 - lr: 0.0075\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0034 - mae: 0.0279 - val_loss: 0.0050 - val_mae: 0.0326 - lr: 0.0056\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0034 - mae: 0.0275 - val_loss: 0.0053 - val_mae: 0.0363 - lr: 0.0056\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0032 - mae: 0.0261 - val_loss: 0.0052 - val_mae: 0.0364 - lr: 0.0056\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0033 - mae: 0.0269 - val_loss: 0.0052 - val_mae: 0.0367 - lr: 0.0056\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0033 - mae: 0.0268 - val_loss: 0.0051 - val_mae: 0.0343 - lr: 0.0056\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0032 - mae: 0.0264 - val_loss: 0.0052 - val_mae: 0.0356 - lr: 0.0056\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0034 - mae: 0.0288 - val_loss: 0.0051 - val_mae: 0.0351 - lr: 0.0056\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0032 - mae: 0.0274 - val_loss: 0.0055 - val_mae: 0.0420 - lr: 0.0056\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0033 - mae: 0.0282 - val_loss: 0.0050 - val_mae: 0.0339 - lr: 0.0056\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0032 - mae: 0.0271 - val_loss: 0.0050 - val_mae: 0.0344 - lr: 0.0056\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0031 - mae: 0.0265 - val_loss: 0.0055 - val_mae: 0.0414 - lr: 0.0056\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0031 - mae: 0.0265 - val_loss: 0.0049 - val_mae: 0.0332 - lr: 0.0042\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0030 - mae: 0.0261 - val_loss: 0.0053 - val_mae: 0.0369 - lr: 0.0042\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0031 - mae: 0.0269 - val_loss: 0.0050 - val_mae: 0.0329 - lr: 0.0042\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0030 - mae: 0.0259 - val_loss: 0.0050 - val_mae: 0.0367 - lr: 0.0042\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0029 - mae: 0.0251 - val_loss: 0.0047 - val_mae: 0.0320 - lr: 0.0042\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0029 - mae: 0.0246 - val_loss: 0.0049 - val_mae: 0.0356 - lr: 0.0042\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0030 - mae: 0.0265 - val_loss: 0.0051 - val_mae: 0.0391 - lr: 0.0042\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0029 - mae: 0.0256 - val_loss: 0.0047 - val_mae: 0.0327 - lr: 0.0042\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0028 - mae: 0.0247 - val_loss: 0.0053 - val_mae: 0.0414 - lr: 0.0042\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0031 - mae: 0.0278 - val_loss: 0.0047 - val_mae: 0.0329 - lr: 0.0042\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0028 - mae: 0.0261 - val_loss: 0.0047 - val_mae: 0.0330 - lr: 0.0042\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0027 - mae: 0.0242 - val_loss: 0.0047 - val_mae: 0.0340 - lr: 0.0042\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0027 - mae: 0.0241 - val_loss: 0.0049 - val_mae: 0.0376 - lr: 0.0042\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0028 - mae: 0.0251 - val_loss: 0.0046 - val_mae: 0.0315 - lr: 0.0042\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0029 - mae: 0.0266 - val_loss: 0.0049 - val_mae: 0.0373 - lr: 0.0042\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0028 - mae: 0.0256 - val_loss: 0.0047 - val_mae: 0.0321 - lr: 0.0042\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0028 - mae: 0.0267 - val_loss: 0.0049 - val_mae: 0.0369 - lr: 0.0042\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0029 - mae: 0.0266 - val_loss: 0.0046 - val_mae: 0.0321 - lr: 0.0042\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0028 - mae: 0.0257 - val_loss: 0.0047 - val_mae: 0.0339 - lr: 0.0042\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0028 - mae: 0.0256 - val_loss: 0.0048 - val_mae: 0.0372 - lr: 0.0042\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0027 - mae: 0.0256 - val_loss: 0.0045 - val_mae: 0.0321 - lr: 0.0042\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0027 - mae: 0.0259 - val_loss: 0.0045 - val_mae: 0.0330 - lr: 0.0042\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0027 - mae: 0.0252 - val_loss: 0.0045 - val_mae: 0.0324 - lr: 0.0042\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0029 - mae: 0.0269 - val_loss: 0.0046 - val_mae: 0.0346 - lr: 0.0042\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0026 - mae: 0.0249 - val_loss: 0.0047 - val_mae: 0.0354 - lr: 0.0032\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0026 - mae: 0.0247 - val_loss: 0.0045 - val_mae: 0.0326 - lr: 0.0032\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0026 - mae: 0.0247 - val_loss: 0.0045 - val_mae: 0.0337 - lr: 0.0032\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0025 - mae: 0.0236 - val_loss: 0.0046 - val_mae: 0.0356 - lr: 0.0032\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0025 - mae: 0.0238 - val_loss: 0.0046 - val_mae: 0.0348 - lr: 0.0032\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0025 - mae: 0.0244 - val_loss: 0.0045 - val_mae: 0.0345 - lr: 0.0032\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0025 - mae: 0.0239 - val_loss: 0.0044 - val_mae: 0.0328 - lr: 0.0032\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 40ms/step - loss: 0.0025 - mae: 0.0241 - val_loss: 0.0043 - val_mae: 0.0310 - lr: 0.0032\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0025 - mae: 0.0250 - val_loss: 0.0044 - val_mae: 0.0326 - lr: 0.0032\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0026 - mae: 0.0253 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 0.0032\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0027 - mae: 0.0267 - val_loss: 0.0044 - val_mae: 0.0322 - lr: 0.0032\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0027 - mae: 0.0269 - val_loss: 0.0043 - val_mae: 0.0306 - lr: 0.0032\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0024 - mae: 0.0242 - val_loss: 0.0044 - val_mae: 0.0338 - lr: 0.0032\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0025 - mae: 0.0248 - val_loss: 0.0044 - val_mae: 0.0324 - lr: 0.0032\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0026 - mae: 0.0259 - val_loss: 0.0043 - val_mae: 0.0314 - lr: 0.0032\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0026 - mae: 0.0255 - val_loss: 0.0048 - val_mae: 0.0389 - lr: 0.0032\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0026 - mae: 0.0258 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 0.0032\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0025 - mae: 0.0250 - val_loss: 0.0044 - val_mae: 0.0327 - lr: 0.0032\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0025 - mae: 0.0252 - val_loss: 0.0044 - val_mae: 0.0329 - lr: 0.0032\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0025 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0322 - lr: 0.0032\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0023 - mae: 0.0236 - val_loss: 0.0043 - val_mae: 0.0342 - lr: 0.0032\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0024 - mae: 0.0244 - val_loss: 0.0042 - val_mae: 0.0336 - lr: 0.0032\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0024 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0328 - lr: 0.0024\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0024 - mae: 0.0244 - val_loss: 0.0042 - val_mae: 0.0327 - lr: 0.0024\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0024 - mae: 0.0244 - val_loss: 0.0042 - val_mae: 0.0319 - lr: 0.0024\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0024 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0312 - lr: 0.0024\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0232 - val_loss: 0.0042 - val_mae: 0.0331 - lr: 0.0024\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0233 - val_loss: 0.0042 - val_mae: 0.0334 - lr: 0.0024\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0023 - mae: 0.0237 - val_loss: 0.0041 - val_mae: 0.0314 - lr: 0.0024\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0023 - mae: 0.0239 - val_loss: 0.0043 - val_mae: 0.0346 - lr: 0.0024\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0023 - mae: 0.0236 - val_loss: 0.0042 - val_mae: 0.0331 - lr: 0.0024\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0023 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0327 - lr: 0.0024\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0022 - mae: 0.0231 - val_loss: 0.0043 - val_mae: 0.0333 - lr: 0.0024\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0023 - mae: 0.0235 - val_loss: 0.0042 - val_mae: 0.0314 - lr: 0.0024\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0237 - val_loss: 0.0043 - val_mae: 0.0335 - lr: 0.0024\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0024 - mae: 0.0242 - val_loss: 0.0045 - val_mae: 0.0358 - lr: 0.0024\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0239 - val_loss: 0.0046 - val_mae: 0.0383 - lr: 0.0018\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0024 - mae: 0.0247 - val_loss: 0.0042 - val_mae: 0.0326 - lr: 0.0018\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0024 - mae: 0.0248 - val_loss: 0.0042 - val_mae: 0.0313 - lr: 0.0018\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0240 - val_loss: 0.0041 - val_mae: 0.0307 - lr: 0.0018\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0023 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0309 - lr: 0.0018\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0023 - mae: 0.0244 - val_loss: 0.0042 - val_mae: 0.0312 - lr: 0.0018\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0023 - mae: 0.0237 - val_loss: 0.0042 - val_mae: 0.0322 - lr: 0.0018\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0022 - mae: 0.0234 - val_loss: 0.0043 - val_mae: 0.0336 - lr: 0.0018\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0022 - mae: 0.0230 - val_loss: 0.0043 - val_mae: 0.0325 - lr: 0.0018\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0022 - mae: 0.0233 - val_loss: 0.0041 - val_mae: 0.0309 - lr: 0.0018\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0240 - val_loss: 0.0041 - val_mae: 0.0318 - lr: 0.0013\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0021 - mae: 0.0226 - val_loss: 0.0041 - val_mae: 0.0324 - lr: 0.0013\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0022 - mae: 0.0230 - val_loss: 0.0041 - val_mae: 0.0315 - lr: 0.0013\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0023 - mae: 0.0241 - val_loss: 0.0041 - val_mae: 0.0316 - lr: 0.0013\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0022 - mae: 0.0225 - val_loss: 0.0042 - val_mae: 0.0336 - lr: 0.0013\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0022 - mae: 0.0229 - val_loss: 0.0041 - val_mae: 0.0312 - lr: 0.0013\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0021 - mae: 0.0229 - val_loss: 0.0042 - val_mae: 0.0332 - lr: 0.0013\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0021 - mae: 0.0230 - val_loss: 0.0043 - val_mae: 0.0353 - lr: 0.0013\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0023 - mae: 0.0242 - val_loss: 0.0042 - val_mae: 0.0334 - lr: 0.0013\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0022 - mae: 0.0234 - val_loss: 0.0041 - val_mae: 0.0327 - lr: 0.0013\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0021 - mae: 0.0232 - val_loss: 0.0041 - val_mae: 0.0314 - lr: 0.0010\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0021 - mae: 0.0231 - val_loss: 0.0042 - val_mae: 0.0338 - lr: 0.0010\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0021 - mae: 0.0228 - val_loss: 0.0040 - val_mae: 0.0310 - lr: 0.0010\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0021 - mae: 0.0228 - val_loss: 0.0041 - val_mae: 0.0310 - lr: 0.0010\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0223 - val_loss: 0.0041 - val_mae: 0.0321 - lr: 0.0010\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0224 - val_loss: 0.0041 - val_mae: 0.0318 - lr: 0.0010\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0221 - val_loss: 0.0041 - val_mae: 0.0316 - lr: 0.0010\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0020 - mae: 0.0220 - val_loss: 0.0041 - val_mae: 0.0331 - lr: 0.0010\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0224 - val_loss: 0.0041 - val_mae: 0.0319 - lr: 0.0010\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0220 - val_loss: 0.0041 - val_mae: 0.0328 - lr: 0.0010\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0021 - mae: 0.0233 - val_loss: 0.0040 - val_mae: 0.0305 - lr: 0.0010\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0021 - mae: 0.0230 - val_loss: 0.0041 - val_mae: 0.0317 - lr: 0.0010\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0021 - mae: 0.0221 - val_loss: 0.0040 - val_mae: 0.0320 - lr: 0.0010\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0021 - mae: 0.0231 - val_loss: 0.0039 - val_mae: 0.0304 - lr: 7.5085e-04\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0228 - val_loss: 0.0040 - val_mae: 0.0303 - lr: 7.5085e-04\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0020 - mae: 0.0219 - val_loss: 0.0040 - val_mae: 0.0317 - lr: 7.5085e-04\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0226 - val_loss: 0.0039 - val_mae: 0.0299 - lr: 7.5085e-04\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0040 - val_mae: 0.0313 - lr: 7.5085e-04\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0020 - mae: 0.0226 - val_loss: 0.0040 - val_mae: 0.0301 - lr: 7.5085e-04\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0215 - val_loss: 0.0040 - val_mae: 0.0307 - lr: 7.5085e-04\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0040 - val_mae: 0.0301 - lr: 7.5085e-04\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0020 - mae: 0.0219 - val_loss: 0.0040 - val_mae: 0.0316 - lr: 7.5085e-04\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0019 - mae: 0.0215 - val_loss: 0.0040 - val_mae: 0.0305 - lr: 7.5085e-04\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0222 - val_loss: 0.0040 - val_mae: 0.0309 - lr: 7.5085e-04\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0221 - val_loss: 0.0040 - val_mae: 0.0301 - lr: 5.6314e-04\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0211 - val_loss: 0.0039 - val_mae: 0.0300 - lr: 5.6314e-04\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0041 - val_mae: 0.0334 - lr: 5.6314e-04\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0040 - val_mae: 0.0298 - lr: 5.6314e-04\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0040 - val_mae: 0.0311 - lr: 5.6314e-04\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0020 - mae: 0.0227 - val_loss: 0.0040 - val_mae: 0.0314 - lr: 5.6314e-04\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0222 - val_loss: 0.0039 - val_mae: 0.0299 - lr: 5.6314e-04\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0228 - val_loss: 0.0040 - val_mae: 0.0316 - lr: 5.6314e-04\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0220 - val_loss: 0.0040 - val_mae: 0.0320 - lr: 5.6314e-04\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0019 - mae: 0.0217 - val_loss: 0.0039 - val_mae: 0.0301 - lr: 5.6314e-04\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0217 - val_loss: 0.0040 - val_mae: 0.0319 - lr: 4.2235e-04\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0020 - mae: 0.0226 - val_loss: 0.0039 - val_mae: 0.0304 - lr: 4.2235e-04\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0020 - mae: 0.0231 - val_loss: 0.0039 - val_mae: 0.0311 - lr: 4.2235e-04\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0019 - mae: 0.0213 - val_loss: 0.0039 - val_mae: 0.0299 - lr: 4.2235e-04\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0020 - mae: 0.0226 - val_loss: 0.0040 - val_mae: 0.0319 - lr: 4.2235e-04\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0019 - mae: 0.0223 - val_loss: 0.0039 - val_mae: 0.0300 - lr: 4.2235e-04\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0213 - val_loss: 0.0039 - val_mae: 0.0312 - lr: 4.2235e-04\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0217 - val_loss: 0.0039 - val_mae: 0.0305 - lr: 4.2235e-04\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0019 - mae: 0.0213 - val_loss: 0.0039 - val_mae: 0.0309 - lr: 4.2235e-04\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0018 - mae: 0.0210 - val_loss: 0.0039 - val_mae: 0.0302 - lr: 4.2235e-04\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0227 - val_loss: 0.0039 - val_mae: 0.0314 - lr: 3.1676e-04\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0226 - val_loss: 0.0039 - val_mae: 0.0302 - lr: 3.1676e-04\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0019 - mae: 0.0216 - val_loss: 0.0039 - val_mae: 0.0310 - lr: 3.1676e-04\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0019 - mae: 0.0219 - val_loss: 0.0039 - val_mae: 0.0302 - lr: 3.1676e-04\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0218 - val_loss: 0.0039 - val_mae: 0.0306 - lr: 3.1676e-04\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0018 - mae: 0.0210 - val_loss: 0.0039 - val_mae: 0.0305 - lr: 3.1676e-04\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0213 - val_loss: 0.0039 - val_mae: 0.0308 - lr: 3.1676e-04\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0216 - val_loss: 0.0039 - val_mae: 0.0305 - lr: 3.1676e-04\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0220 - val_loss: 0.0039 - val_mae: 0.0303 - lr: 3.1676e-04\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0018 - mae: 0.0214 - val_loss: 0.0039 - val_mae: 0.0304 - lr: 3.1676e-04\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0018 - mae: 0.0210 - val_loss: 0.0039 - val_mae: 0.0311 - lr: 2.3757e-04\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0018 - mae: 0.0211 - val_loss: 0.0039 - val_mae: 0.0305 - lr: 2.3757e-04\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0217 - val_loss: 0.0039 - val_mae: 0.0306 - lr: 2.3757e-04\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0213 - val_loss: 0.0039 - val_mae: 0.0307 - lr: 2.3757e-04\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0019 - mae: 0.0221 - val_loss: 0.0039 - val_mae: 0.0305 - lr: 2.3757e-04\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0019 - mae: 0.0221 - val_loss: 0.0039 - val_mae: 0.0307 - lr: 2.3757e-04\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0018 - mae: 0.0211 - val_loss: 0.0039 - val_mae: 0.0302 - lr: 2.3757e-04\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0019 - mae: 0.0217 - val_loss: 0.0039 - val_mae: 0.0302 - lr: 2.3757e-04\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 32ms/step - loss: 0.0018 - mae: 0.0214 - val_loss: 0.0039 - val_mae: 0.0307 - lr: 2.3757e-04\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0018 - mae: 0.0219 - val_loss: 0.0039 - val_mae: 0.0299 - lr: 2.3757e-04\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0019 - mae: 0.0215 - val_loss: 0.0039 - val_mae: 0.0304 - lr: 1.7818e-04\n",
      "57/57 [==============================] - 0s 4ms/step\n",
      "Mean Absolute Error: 52.30674621381037\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name=\"models/date_predictor-v27.h5\"\n",
    "model, history = train(file_name, epochs=500, learning_rate=1e-2, batch_size=512)\n",
    "evaluate_model(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "92/92 [==============================] - 3s 21ms/step - loss: 0.0040 - mae: 0.0391 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 1.3363e-04\n",
      "Epoch 2/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0039 - mae: 0.0386 - val_loss: 0.0047 - val_mae: 0.0368 - lr: 1.3363e-04\n",
      "Epoch 3/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0039 - mae: 0.0391 - val_loss: 0.0048 - val_mae: 0.0373 - lr: 1.3363e-04\n",
      "Epoch 4/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0041 - mae: 0.0401 - val_loss: 0.0048 - val_mae: 0.0376 - lr: 1.3363e-04\n",
      "Epoch 5/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0039 - mae: 0.0380 - val_loss: 0.0048 - val_mae: 0.0377 - lr: 1.3363e-04\n",
      "Epoch 6/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0038 - mae: 0.0386 - val_loss: 0.0047 - val_mae: 0.0361 - lr: 1.3363e-04\n",
      "Epoch 7/500\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.0038 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 1.3363e-04\n",
      "Epoch 8/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0040 - mae: 0.0390 - val_loss: 0.0048 - val_mae: 0.0376 - lr: 1.3363e-04\n",
      "Epoch 9/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0040 - mae: 0.0393 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.3363e-04\n",
      "Epoch 10/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 1.3363e-04\n",
      "Epoch 11/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0366 - lr: 1.3363e-04\n",
      "Epoch 12/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0042 - mae: 0.0407 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.0023e-04\n",
      "Epoch 13/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0040 - mae: 0.0396 - val_loss: 0.0048 - val_mae: 0.0377 - lr: 1.0023e-04\n",
      "Epoch 14/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0039 - mae: 0.0388 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.0023e-04\n",
      "Epoch 15/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0376 - lr: 1.0023e-04\n",
      "Epoch 16/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0039 - mae: 0.0392 - val_loss: 0.0048 - val_mae: 0.0380 - lr: 1.0023e-04\n",
      "Epoch 17/500\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.0037 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.0023e-04\n",
      "Epoch 18/500\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.0039 - mae: 0.0398 - val_loss: 0.0048 - val_mae: 0.0376 - lr: 1.0023e-04\n",
      "Epoch 19/500\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.0039 - mae: 0.0388 - val_loss: 0.0048 - val_mae: 0.0376 - lr: 1.0023e-04\n",
      "Epoch 20/500\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.0038 - mae: 0.0385 - val_loss: 0.0048 - val_mae: 0.0380 - lr: 1.0023e-04\n",
      "Epoch 21/500\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.0038 - mae: 0.0378 - val_loss: 0.0048 - val_mae: 0.0378 - lr: 1.0023e-04\n",
      "Epoch 22/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0388 - val_loss: 0.0048 - val_mae: 0.0374 - lr: 7.5169e-05\n",
      "Epoch 23/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 7.5169e-05\n",
      "Epoch 24/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 7.5169e-05\n",
      "Epoch 25/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 7.5169e-05\n",
      "Epoch 26/500\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.0036 - mae: 0.0369 - val_loss: 0.0047 - val_mae: 0.0368 - lr: 7.5169e-05\n",
      "Epoch 27/500\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.0037 - mae: 0.0377 - val_loss: 0.0047 - val_mae: 0.0368 - lr: 7.5169e-05\n",
      "Epoch 28/500\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0036 - mae: 0.0375 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 7.5169e-05\n",
      "Epoch 29/500\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.0038 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 7.5169e-05\n",
      "Epoch 30/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0379 - val_loss: 0.0048 - val_mae: 0.0380 - lr: 7.5169e-05\n",
      "Epoch 31/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 7.5169e-05\n",
      "Epoch 32/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 5.6377e-05\n",
      "Epoch 33/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 5.6377e-05\n",
      "Epoch 34/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0370 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 5.6377e-05\n",
      "Epoch 35/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 5.6377e-05\n",
      "Epoch 36/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 5.6377e-05\n",
      "Epoch 37/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 5.6377e-05\n",
      "Epoch 38/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0381 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 5.6377e-05\n",
      "Epoch 39/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0036 - mae: 0.0369 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 5.6377e-05\n",
      "Epoch 40/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0039 - mae: 0.0392 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 5.6377e-05\n",
      "Epoch 41/500\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.0039 - mae: 0.0391 - val_loss: 0.0046 - val_mae: 0.0364 - lr: 5.6377e-05\n",
      "Epoch 42/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 4.2283e-05\n",
      "Epoch 43/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0383 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 4.2283e-05\n",
      "Epoch 44/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0036 - mae: 0.0375 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 4.2283e-05\n",
      "Epoch 45/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0376 - val_loss: 0.0046 - val_mae: 0.0370 - lr: 4.2283e-05\n",
      "Epoch 46/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0035 - mae: 0.0372 - val_loss: 0.0046 - val_mae: 0.0367 - lr: 4.2283e-05\n",
      "Epoch 47/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0365 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 4.2283e-05\n",
      "Epoch 48/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0387 - val_loss: 0.0047 - val_mae: 0.0376 - lr: 4.2283e-05\n",
      "Epoch 49/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0395 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 4.2283e-05\n",
      "Epoch 50/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.0047 - val_mae: 0.0376 - lr: 4.2283e-05\n",
      "Epoch 51/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 4.2283e-05\n",
      "Epoch 52/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0388 - val_loss: 0.0046 - val_mae: 0.0369 - lr: 3.1712e-05\n",
      "Epoch 53/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0384 - val_loss: 0.0046 - val_mae: 0.0369 - lr: 3.1712e-05\n",
      "Epoch 54/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 3.1712e-05\n",
      "Epoch 55/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0370 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 3.1712e-05\n",
      "Epoch 56/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0038 - mae: 0.0381 - val_loss: 0.0047 - val_mae: 0.0368 - lr: 3.1712e-05\n",
      "Epoch 57/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0387 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 3.1712e-05\n",
      "Epoch 58/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0367 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 3.1712e-05\n",
      "Epoch 59/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0377 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 3.1712e-05\n",
      "Epoch 60/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0382 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 3.1712e-05\n",
      "Epoch 61/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0363 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 3.1712e-05\n",
      "Epoch 62/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0375 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 2.3784e-05\n",
      "Epoch 63/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0385 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 2.3784e-05\n",
      "Epoch 64/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0373 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 2.3784e-05\n",
      "Epoch 65/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 2.3784e-05\n",
      "Epoch 66/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0375 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 2.3784e-05\n",
      "Epoch 67/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0376 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 2.3784e-05\n",
      "Epoch 68/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0039 - mae: 0.0394 - val_loss: 0.0047 - val_mae: 0.0375 - lr: 2.3784e-05\n",
      "Epoch 69/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0375 - val_loss: 0.0047 - val_mae: 0.0376 - lr: 2.3784e-05\n",
      "Epoch 70/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0373 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 2.3784e-05\n",
      "Epoch 71/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0382 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 2.3784e-05\n",
      "Epoch 72/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0376 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.7838e-05\n",
      "Epoch 73/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0034 - mae: 0.0359 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.7838e-05\n",
      "Epoch 74/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0034 - mae: 0.0371 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.7838e-05\n",
      "Epoch 75/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0037 - mae: 0.0379 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 1.7838e-05\n",
      "Epoch 76/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0038 - mae: 0.0385 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 1.7838e-05\n",
      "Epoch 77/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0033 - mae: 0.0353 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.7838e-05\n",
      "Epoch 78/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0374 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 1.7838e-05\n",
      "Epoch 79/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0366 - val_loss: 0.0047 - val_mae: 0.0369 - lr: 1.7838e-05\n",
      "Epoch 80/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0381 - val_loss: 0.0047 - val_mae: 0.0368 - lr: 1.7838e-05\n",
      "Epoch 81/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0373 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.7838e-05\n",
      "Epoch 82/500\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0376 - lr: 1.3379e-05\n",
      "Epoch 83/500\n",
      "92/92 [==============================] - 1s 14ms/step - loss: 0.0035 - mae: 0.0363 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.3379e-05\n",
      "Epoch 84/500\n",
      "92/92 [==============================] - 1s 15ms/step - loss: 0.0033 - mae: 0.0358 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 1.3379e-05\n",
      "Epoch 85/500\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.0038 - mae: 0.0388 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.3379e-05\n",
      "Epoch 86/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0035 - mae: 0.0364 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 1.3379e-05\n",
      "Epoch 87/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0035 - mae: 0.0359 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.3379e-05\n",
      "Epoch 88/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0380 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.3379e-05\n",
      "Epoch 89/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0371 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 1.3379e-05\n",
      "Epoch 90/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0385 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.3379e-05\n",
      "Epoch 91/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0376 - val_loss: 0.0047 - val_mae: 0.0374 - lr: 1.3379e-05\n",
      "Epoch 92/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0382 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.0034e-05\n",
      "Epoch 93/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0368 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.0034e-05\n",
      "Epoch 94/500\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.0036 - mae: 0.0378 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 1.0034e-05\n",
      "Epoch 95/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0038 - mae: 0.0390 - val_loss: 0.0047 - val_mae: 0.0370 - lr: 1.0034e-05\n",
      "Epoch 96/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0370 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.0034e-05\n",
      "Epoch 97/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0036 - mae: 0.0377 - val_loss: 0.0047 - val_mae: 0.0372 - lr: 1.0034e-05\n",
      "Epoch 98/500\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.0037 - mae: 0.0380 - val_loss: 0.0047 - val_mae: 0.0373 - lr: 1.0034e-05\n",
      "Epoch 99/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0035 - mae: 0.0358 - val_loss: 0.0047 - val_mae: 0.0371 - lr: 1.0034e-05\n",
      "Epoch 100/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0036 - mae: 0.0372 - val_loss: 0.0047 - val_mae: 0.0377 - lr: 1.0034e-05\n",
      "Epoch 101/500\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.0037 - mae: 0.0373 - val_loss: 0.0047 - val_mae: 0.0375 - lr: 1.0034e-05\n",
      "57/57 [==============================] - 0s 4ms/step\n",
      "Mean Absolute Error: 60.04184693588776\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name=\"models/date_predictor-v30.h5\"\n",
    "model, history = train(file_name, epochs=500, learning_rate=1e-2, batch_size=16)\n",
    "evaluate_model(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Fold 1/5 ---\n",
      "Epoch 1/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 3.6131 - mae: 1.4562\n",
      "Epoch 1: val_loss improved from inf to 0.33184, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 20ms/step - loss: 3.6131 - mae: 1.4562 - val_loss: 0.3318 - val_mae: 0.3690 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.8091 - mae: 1.2837\n",
      "Epoch 2: val_loss improved from 0.33184 to 0.26230, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.8083 - mae: 1.2837 - val_loss: 0.2623 - val_mae: 0.3176 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.3537 - mae: 1.1585\n",
      "Epoch 3: val_loss improved from 0.26230 to 0.22105, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.3563 - mae: 1.1592 - val_loss: 0.2210 - val_mae: 0.2745 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.0083 - mae: 1.0832\n",
      "Epoch 4: val_loss improved from 0.22105 to 0.20183, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 2.0071 - mae: 1.0827 - val_loss: 0.2018 - val_mae: 0.2540 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.6624 - mae: 0.9774\n",
      "Epoch 5: val_loss improved from 0.20183 to 0.18652, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.6607 - mae: 0.9776 - val_loss: 0.1865 - val_mae: 0.2359 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.4753 - mae: 0.9080\n",
      "Epoch 6: val_loss improved from 0.18652 to 0.17743, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 1.4753 - mae: 0.9079 - val_loss: 0.1774 - val_mae: 0.2245 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.2916 - mae: 0.8547\n",
      "Epoch 7: val_loss improved from 0.17743 to 0.16741, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.2895 - mae: 0.8535 - val_loss: 0.1674 - val_mae: 0.2110 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.1291 - mae: 0.7883\n",
      "Epoch 8: val_loss improved from 0.16741 to 0.16025, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 1.1298 - mae: 0.7888 - val_loss: 0.1602 - val_mae: 0.2007 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9832 - mae: 0.7313\n",
      "Epoch 9: val_loss improved from 0.16025 to 0.15596, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.9832 - mae: 0.7313 - val_loss: 0.1560 - val_mae: 0.1948 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.8822 - mae: 0.6978\n",
      "Epoch 10: val_loss improved from 0.15596 to 0.15141, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.8821 - mae: 0.6978 - val_loss: 0.1514 - val_mae: 0.1874 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.7495 - mae: 0.6287\n",
      "Epoch 11: val_loss improved from 0.15141 to 0.14743, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.7492 - mae: 0.6286 - val_loss: 0.1474 - val_mae: 0.1812 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.6936 - mae: 0.6049\n",
      "Epoch 12: val_loss improved from 0.14743 to 0.14467, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.6933 - mae: 0.6047 - val_loss: 0.1447 - val_mae: 0.1763 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6254 - mae: 0.5641\n",
      "Epoch 13: val_loss improved from 0.14467 to 0.14213, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.6254 - mae: 0.5641 - val_loss: 0.1421 - val_mae: 0.1722 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5495 - mae: 0.5240\n",
      "Epoch 14: val_loss improved from 0.14213 to 0.14037, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.5494 - mae: 0.5240 - val_loss: 0.1404 - val_mae: 0.1689 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4812 - mae: 0.4826\n",
      "Epoch 15: val_loss improved from 0.14037 to 0.13883, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.4812 - mae: 0.4826 - val_loss: 0.1388 - val_mae: 0.1659 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4337 - mae: 0.4556\n",
      "Epoch 16: val_loss improved from 0.13883 to 0.13811, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.4337 - mae: 0.4556 - val_loss: 0.1381 - val_mae: 0.1643 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3898 - mae: 0.4239\n",
      "Epoch 17: val_loss improved from 0.13811 to 0.13790, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.3897 - mae: 0.4238 - val_loss: 0.1379 - val_mae: 0.1635 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3511 - mae: 0.3955\n",
      "Epoch 18: val_loss improved from 0.13790 to 0.13671, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.3512 - mae: 0.3955 - val_loss: 0.1367 - val_mae: 0.1614 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3201 - mae: 0.3739\n",
      "Epoch 19: val_loss improved from 0.13671 to 0.13534, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.3201 - mae: 0.3739 - val_loss: 0.1353 - val_mae: 0.1589 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2969 - mae: 0.3514\n",
      "Epoch 20: val_loss improved from 0.13534 to 0.13501, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.2969 - mae: 0.3514 - val_loss: 0.1350 - val_mae: 0.1582 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2649 - mae: 0.3222\n",
      "Epoch 21: val_loss improved from 0.13501 to 0.13374, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.2648 - mae: 0.3221 - val_loss: 0.1337 - val_mae: 0.1559 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2452 - mae: 0.3027\n",
      "Epoch 22: val_loss improved from 0.13374 to 0.13345, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.2452 - mae: 0.3027 - val_loss: 0.1335 - val_mae: 0.1555 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2296 - mae: 0.2875\n",
      "Epoch 23: val_loss improved from 0.13345 to 0.13315, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.2295 - mae: 0.2874 - val_loss: 0.1332 - val_mae: 0.1551 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2120 - mae: 0.2688\n",
      "Epoch 24: val_loss did not improve from 0.13315\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.2119 - mae: 0.2687 - val_loss: 0.1334 - val_mae: 0.1558 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1974 - mae: 0.2527\n",
      "Epoch 25: val_loss improved from 0.13315 to 0.13247, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1975 - mae: 0.2528 - val_loss: 0.1325 - val_mae: 0.1540 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1935 - mae: 0.2466\n",
      "Epoch 26: val_loss improved from 0.13247 to 0.13184, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1936 - mae: 0.2467 - val_loss: 0.1318 - val_mae: 0.1528 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1809 - mae: 0.2317\n",
      "Epoch 27: val_loss improved from 0.13184 to 0.13142, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1809 - mae: 0.2317 - val_loss: 0.1314 - val_mae: 0.1521 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1750 - mae: 0.2221\n",
      "Epoch 28: val_loss improved from 0.13142 to 0.13113, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1750 - mae: 0.2221 - val_loss: 0.1311 - val_mae: 0.1517 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1674 - mae: 0.2127\n",
      "Epoch 29: val_loss improved from 0.13113 to 0.13109, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1674 - mae: 0.2127 - val_loss: 0.1311 - val_mae: 0.1518 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1619 - mae: 0.2050\n",
      "Epoch 30: val_loss improved from 0.13109 to 0.13032, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1619 - mae: 0.2050 - val_loss: 0.1303 - val_mae: 0.1501 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1580 - mae: 0.1981\n",
      "Epoch 31: val_loss improved from 0.13032 to 0.13011, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1582 - mae: 0.1982 - val_loss: 0.1301 - val_mae: 0.1498 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1529 - mae: 0.1907\n",
      "Epoch 32: val_loss improved from 0.13011 to 0.12997, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1529 - mae: 0.1907 - val_loss: 0.1300 - val_mae: 0.1496 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1501 - mae: 0.1856\n",
      "Epoch 33: val_loss improved from 0.12997 to 0.12960, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1502 - mae: 0.1857 - val_loss: 0.1296 - val_mae: 0.1490 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1454 - mae: 0.1786\n",
      "Epoch 34: val_loss improved from 0.12960 to 0.12927, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1454 - mae: 0.1786 - val_loss: 0.1293 - val_mae: 0.1482 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1434 - mae: 0.1759\n",
      "Epoch 35: val_loss improved from 0.12927 to 0.12873, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1434 - mae: 0.1759 - val_loss: 0.1287 - val_mae: 0.1469 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1413 - mae: 0.1722\n",
      "Epoch 36: val_loss improved from 0.12873 to 0.12866, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1412 - mae: 0.1722 - val_loss: 0.1287 - val_mae: 0.1469 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1395 - mae: 0.1682\n",
      "Epoch 37: val_loss improved from 0.12866 to 0.12842, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1395 - mae: 0.1682 - val_loss: 0.1284 - val_mae: 0.1464 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1387 - mae: 0.1673\n",
      "Epoch 38: val_loss improved from 0.12842 to 0.12832, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1388 - mae: 0.1673 - val_loss: 0.1283 - val_mae: 0.1462 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1361 - mae: 0.1634\n",
      "Epoch 39: val_loss improved from 0.12832 to 0.12810, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1361 - mae: 0.1634 - val_loss: 0.1281 - val_mae: 0.1457 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1361 - mae: 0.1627\n",
      "Epoch 40: val_loss improved from 0.12810 to 0.12801, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1361 - mae: 0.1627 - val_loss: 0.1280 - val_mae: 0.1458 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1348 - mae: 0.1614\n",
      "Epoch 41: val_loss improved from 0.12801 to 0.12778, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1348 - mae: 0.1614 - val_loss: 0.1278 - val_mae: 0.1450 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1343 - mae: 0.1603\n",
      "Epoch 42: val_loss improved from 0.12778 to 0.12768, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1343 - mae: 0.1603 - val_loss: 0.1277 - val_mae: 0.1451 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1328 - mae: 0.1576\n",
      "Epoch 43: val_loss improved from 0.12768 to 0.12759, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1328 - mae: 0.1576 - val_loss: 0.1276 - val_mae: 0.1451 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1326 - mae: 0.1577\n",
      "Epoch 44: val_loss improved from 0.12759 to 0.12749, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1326 - mae: 0.1578 - val_loss: 0.1275 - val_mae: 0.1451 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1315 - mae: 0.1551\n",
      "Epoch 45: val_loss improved from 0.12749 to 0.12745, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1315 - mae: 0.1552 - val_loss: 0.1274 - val_mae: 0.1452 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1313 - mae: 0.1554\n",
      "Epoch 46: val_loss improved from 0.12745 to 0.12730, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1313 - mae: 0.1553 - val_loss: 0.1273 - val_mae: 0.1453 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1307 - mae: 0.1548\n",
      "Epoch 47: val_loss improved from 0.12730 to 0.12721, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1307 - mae: 0.1547 - val_loss: 0.1272 - val_mae: 0.1454 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1300 - mae: 0.1532\n",
      "Epoch 48: val_loss improved from 0.12721 to 0.12708, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1300 - mae: 0.1532 - val_loss: 0.1271 - val_mae: 0.1455 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1298 - mae: 0.1535\n",
      "Epoch 49: val_loss improved from 0.12708 to 0.12682, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1298 - mae: 0.1535 - val_loss: 0.1268 - val_mae: 0.1453 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1294 - mae: 0.1527\n",
      "Epoch 50: val_loss improved from 0.12682 to 0.12667, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1294 - mae: 0.1527 - val_loss: 0.1267 - val_mae: 0.1454 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1289 - mae: 0.1520\n",
      "Epoch 51: val_loss improved from 0.12667 to 0.12644, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1289 - mae: 0.1519 - val_loss: 0.1264 - val_mae: 0.1455 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1285 - mae: 0.1518\n",
      "Epoch 52: val_loss improved from 0.12644 to 0.12612, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1285 - mae: 0.1518 - val_loss: 0.1261 - val_mae: 0.1452 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1282 - mae: 0.1513\n",
      "Epoch 53: val_loss improved from 0.12612 to 0.12596, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1282 - mae: 0.1513 - val_loss: 0.1260 - val_mae: 0.1455 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1277 - mae: 0.1513\n",
      "Epoch 54: val_loss improved from 0.12596 to 0.12550, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1277 - mae: 0.1513 - val_loss: 0.1255 - val_mae: 0.1452 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1273 - mae: 0.1513\n",
      "Epoch 55: val_loss improved from 0.12550 to 0.12532, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1273 - mae: 0.1512 - val_loss: 0.1253 - val_mae: 0.1455 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1267 - mae: 0.1504\n",
      "Epoch 56: val_loss improved from 0.12532 to 0.12495, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.1267 - mae: 0.1504 - val_loss: 0.1250 - val_mae: 0.1457 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1262 - mae: 0.1502\n",
      "Epoch 57: val_loss improved from 0.12495 to 0.12455, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1262 - mae: 0.1502 - val_loss: 0.1246 - val_mae: 0.1458 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1255 - mae: 0.1495\n",
      "Epoch 58: val_loss improved from 0.12455 to 0.12436, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1255 - mae: 0.1495 - val_loss: 0.1244 - val_mae: 0.1466 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1250 - mae: 0.1494\n",
      "Epoch 59: val_loss improved from 0.12436 to 0.12327, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1250 - mae: 0.1493 - val_loss: 0.1233 - val_mae: 0.1447 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1247 - mae: 0.1495\n",
      "Epoch 60: val_loss improved from 0.12327 to 0.12256, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1247 - mae: 0.1495 - val_loss: 0.1226 - val_mae: 0.1443 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1238 - mae: 0.1487\n",
      "Epoch 61: val_loss improved from 0.12256 to 0.12161, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1238 - mae: 0.1487 - val_loss: 0.1216 - val_mae: 0.1433 - lr: 1.0000e-04\n",
      "Epoch 62/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1230 - mae: 0.1482\n",
      "Epoch 62: val_loss improved from 0.12161 to 0.12069, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1230 - mae: 0.1481 - val_loss: 0.1207 - val_mae: 0.1425 - lr: 1.0000e-04\n",
      "Epoch 63/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1219 - mae: 0.1468\n",
      "Epoch 63: val_loss improved from 0.12069 to 0.11959, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1219 - mae: 0.1468 - val_loss: 0.1196 - val_mae: 0.1416 - lr: 1.0000e-04\n",
      "Epoch 64/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1202 - mae: 0.1436\n",
      "Epoch 64: val_loss improved from 0.11959 to 0.11823, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1202 - mae: 0.1436 - val_loss: 0.1182 - val_mae: 0.1407 - lr: 1.0000e-04\n",
      "Epoch 65/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1182 - mae: 0.1404\n",
      "Epoch 65: val_loss improved from 0.11823 to 0.11516, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1182 - mae: 0.1404 - val_loss: 0.1152 - val_mae: 0.1346 - lr: 1.0000e-04\n",
      "Epoch 66/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1163 - mae: 0.1371\n",
      "Epoch 66: val_loss improved from 0.11516 to 0.11271, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1163 - mae: 0.1372 - val_loss: 0.1127 - val_mae: 0.1301 - lr: 1.0000e-04\n",
      "Epoch 67/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1137 - mae: 0.1320\n",
      "Epoch 67: val_loss improved from 0.11271 to 0.11026, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1137 - mae: 0.1320 - val_loss: 0.1103 - val_mae: 0.1258 - lr: 1.0000e-04\n",
      "Epoch 68/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1108 - mae: 0.1257\n",
      "Epoch 68: val_loss improved from 0.11026 to 0.10709, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1108 - mae: 0.1257 - val_loss: 0.1071 - val_mae: 0.1189 - lr: 1.0000e-04\n",
      "Epoch 69/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1083 - mae: 0.1206\n",
      "Epoch 69: val_loss improved from 0.10709 to 0.10400, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.1083 - mae: 0.1206 - val_loss: 0.1040 - val_mae: 0.1115 - lr: 1.0000e-04\n",
      "Epoch 70/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1060 - mae: 0.1155\n",
      "Epoch 70: val_loss improved from 0.10400 to 0.10071, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1060 - mae: 0.1155 - val_loss: 0.1007 - val_mae: 0.1027 - lr: 1.0000e-04\n",
      "Epoch 71/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1034 - mae: 0.1105\n",
      "Epoch 71: val_loss improved from 0.10071 to 0.09904, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.1034 - mae: 0.1105 - val_loss: 0.0990 - val_mae: 0.1000 - lr: 1.0000e-04\n",
      "Epoch 72/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1010 - mae: 0.1060\n",
      "Epoch 72: val_loss improved from 0.09904 to 0.09668, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1010 - mae: 0.1060 - val_loss: 0.0967 - val_mae: 0.0944 - lr: 1.0000e-04\n",
      "Epoch 73/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0997 - mae: 0.1035\n",
      "Epoch 73: val_loss improved from 0.09668 to 0.09529, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0997 - mae: 0.1035 - val_loss: 0.0953 - val_mae: 0.0920 - lr: 1.0000e-04\n",
      "Epoch 74/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0976 - mae: 0.0998\n",
      "Epoch 74: val_loss improved from 0.09529 to 0.09310, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0976 - mae: 0.0998 - val_loss: 0.0931 - val_mae: 0.0873 - lr: 1.0000e-04\n",
      "Epoch 75/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0964 - mae: 0.0989\n",
      "Epoch 75: val_loss improved from 0.09310 to 0.09170, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0964 - mae: 0.0990 - val_loss: 0.0917 - val_mae: 0.0849 - lr: 1.0000e-04\n",
      "Epoch 76/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0949 - mae: 0.0969\n",
      "Epoch 76: val_loss improved from 0.09170 to 0.09015, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0949 - mae: 0.0970 - val_loss: 0.0902 - val_mae: 0.0817 - lr: 1.0000e-04\n",
      "Epoch 77/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0931 - mae: 0.0932\n",
      "Epoch 77: val_loss improved from 0.09015 to 0.08882, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0930 - mae: 0.0931 - val_loss: 0.0888 - val_mae: 0.0799 - lr: 1.0000e-04\n",
      "Epoch 78/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0911 - mae: 0.0904\n",
      "Epoch 78: val_loss improved from 0.08882 to 0.08773, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0912 - mae: 0.0904 - val_loss: 0.0877 - val_mae: 0.0795 - lr: 1.0000e-04\n",
      "Epoch 79/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0896 - mae: 0.0884\n",
      "Epoch 79: val_loss improved from 0.08773 to 0.08603, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0896 - mae: 0.0884 - val_loss: 0.0860 - val_mae: 0.0756 - lr: 1.0000e-04\n",
      "Epoch 80/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0884 - mae: 0.0871\n",
      "Epoch 80: val_loss improved from 0.08603 to 0.08496, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0884 - mae: 0.0871 - val_loss: 0.0850 - val_mae: 0.0752 - lr: 1.0000e-04\n",
      "Epoch 81/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0867 - mae: 0.0855\n",
      "Epoch 81: val_loss improved from 0.08496 to 0.08358, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0867 - mae: 0.0854 - val_loss: 0.0836 - val_mae: 0.0736 - lr: 1.0000e-04\n",
      "Epoch 82/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0854 - mae: 0.0838\n",
      "Epoch 82: val_loss improved from 0.08358 to 0.08236, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0854 - mae: 0.0838 - val_loss: 0.0824 - val_mae: 0.0728 - lr: 1.0000e-04\n",
      "Epoch 83/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0839 - mae: 0.0815\n",
      "Epoch 83: val_loss improved from 0.08236 to 0.08117, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0839 - mae: 0.0815 - val_loss: 0.0812 - val_mae: 0.0718 - lr: 1.0000e-04\n",
      "Epoch 84/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0824 - mae: 0.0807\n",
      "Epoch 84: val_loss improved from 0.08117 to 0.07969, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0824 - mae: 0.0807 - val_loss: 0.0797 - val_mae: 0.0707 - lr: 1.0000e-04\n",
      "Epoch 85/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0812 - mae: 0.0802\n",
      "Epoch 85: val_loss improved from 0.07969 to 0.07821, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0811 - mae: 0.0802 - val_loss: 0.0782 - val_mae: 0.0685 - lr: 1.0000e-04\n",
      "Epoch 86/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0793 - mae: 0.0766\n",
      "Epoch 86: val_loss improved from 0.07821 to 0.07697, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0793 - mae: 0.0766 - val_loss: 0.0770 - val_mae: 0.0684 - lr: 1.0000e-04\n",
      "Epoch 87/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0781 - mae: 0.0770\n",
      "Epoch 87: val_loss improved from 0.07697 to 0.07553, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0781 - mae: 0.0770 - val_loss: 0.0755 - val_mae: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 88/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0770 - mae: 0.0776\n",
      "Epoch 88: val_loss improved from 0.07553 to 0.07422, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0770 - mae: 0.0777 - val_loss: 0.0742 - val_mae: 0.0666 - lr: 1.0000e-04\n",
      "Epoch 89/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0752 - mae: 0.0744\n",
      "Epoch 89: val_loss improved from 0.07422 to 0.07257, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0752 - mae: 0.0744 - val_loss: 0.0726 - val_mae: 0.0644 - lr: 1.0000e-04\n",
      "Epoch 90/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0739 - mae: 0.0750\n",
      "Epoch 90: val_loss improved from 0.07257 to 0.07141, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0739 - mae: 0.0749 - val_loss: 0.0714 - val_mae: 0.0652 - lr: 1.0000e-04\n",
      "Epoch 91/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0722 - mae: 0.0733\n",
      "Epoch 91: val_loss improved from 0.07141 to 0.07018, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0722 - mae: 0.0733 - val_loss: 0.0702 - val_mae: 0.0658 - lr: 1.0000e-04\n",
      "Epoch 92/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0706 - mae: 0.0724\n",
      "Epoch 92: val_loss improved from 0.07018 to 0.06844, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0706 - mae: 0.0724 - val_loss: 0.0684 - val_mae: 0.0639 - lr: 1.0000e-04\n",
      "Epoch 93/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0691 - mae: 0.0717\n",
      "Epoch 93: val_loss improved from 0.06844 to 0.06708, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0691 - mae: 0.0717 - val_loss: 0.0671 - val_mae: 0.0639 - lr: 1.0000e-04\n",
      "Epoch 94/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0678 - mae: 0.0718\n",
      "Epoch 94: val_loss improved from 0.06708 to 0.06578, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0678 - mae: 0.0719 - val_loss: 0.0658 - val_mae: 0.0641 - lr: 1.0000e-04\n",
      "Epoch 95/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0662 - mae: 0.0704\n",
      "Epoch 95: val_loss improved from 0.06578 to 0.06403, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0662 - mae: 0.0704 - val_loss: 0.0640 - val_mae: 0.0624 - lr: 1.0000e-04\n",
      "Epoch 96/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0644 - mae: 0.0696\n",
      "Epoch 96: val_loss improved from 0.06403 to 0.06250, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0644 - mae: 0.0696 - val_loss: 0.0625 - val_mae: 0.0617 - lr: 1.0000e-04\n",
      "Epoch 97/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0631 - mae: 0.0698\n",
      "Epoch 97: val_loss improved from 0.06250 to 0.06112, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0631 - mae: 0.0698 - val_loss: 0.0611 - val_mae: 0.0624 - lr: 1.0000e-04\n",
      "Epoch 98/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0614 - mae: 0.0677\n",
      "Epoch 98: val_loss improved from 0.06112 to 0.05959, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0614 - mae: 0.0677 - val_loss: 0.0596 - val_mae: 0.0612 - lr: 1.0000e-04\n",
      "Epoch 99/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0596 - mae: 0.0657\n",
      "Epoch 99: val_loss improved from 0.05959 to 0.05793, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0597 - mae: 0.0659 - val_loss: 0.0579 - val_mae: 0.0600 - lr: 1.0000e-04\n",
      "Epoch 100/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0583 - mae: 0.0673\n",
      "Epoch 100: val_loss improved from 0.05793 to 0.05654, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0583 - mae: 0.0673 - val_loss: 0.0565 - val_mae: 0.0599 - lr: 1.0000e-04\n",
      "Epoch 101/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0568 - mae: 0.0663\n",
      "Epoch 101: val_loss improved from 0.05654 to 0.05488, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0568 - mae: 0.0663 - val_loss: 0.0549 - val_mae: 0.0585 - lr: 1.0000e-04\n",
      "Epoch 102/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0549 - mae: 0.0643\n",
      "Epoch 102: val_loss improved from 0.05488 to 0.05355, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0549 - mae: 0.0643 - val_loss: 0.0536 - val_mae: 0.0590 - lr: 1.0000e-04\n",
      "Epoch 103/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0537 - mae: 0.0652\n",
      "Epoch 103: val_loss improved from 0.05355 to 0.05194, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0537 - mae: 0.0652 - val_loss: 0.0519 - val_mae: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 104/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0522 - mae: 0.0649\n",
      "Epoch 104: val_loss improved from 0.05194 to 0.05054, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0522 - mae: 0.0649 - val_loss: 0.0505 - val_mae: 0.0582 - lr: 1.0000e-04\n",
      "Epoch 105/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0507 - mae: 0.0644\n",
      "Epoch 105: val_loss improved from 0.05054 to 0.04929, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0507 - mae: 0.0644 - val_loss: 0.0493 - val_mae: 0.0589 - lr: 1.0000e-04\n",
      "Epoch 106/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0490 - mae: 0.0628\n",
      "Epoch 106: val_loss improved from 0.04929 to 0.04732, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0490 - mae: 0.0628 - val_loss: 0.0473 - val_mae: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 107/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0475 - mae: 0.0615\n",
      "Epoch 107: val_loss improved from 0.04732 to 0.04622, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0475 - mae: 0.0615 - val_loss: 0.0462 - val_mae: 0.0569 - lr: 1.0000e-04\n",
      "Epoch 108/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0461 - mae: 0.0616\n",
      "Epoch 108: val_loss improved from 0.04622 to 0.04511, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0461 - mae: 0.0617 - val_loss: 0.0451 - val_mae: 0.0583 - lr: 1.0000e-04\n",
      "Epoch 109/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0447 - mae: 0.0613\n",
      "Epoch 109: val_loss improved from 0.04511 to 0.04322, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0447 - mae: 0.0614 - val_loss: 0.0432 - val_mae: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 110/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0431 - mae: 0.0602\n",
      "Epoch 110: val_loss improved from 0.04322 to 0.04193, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0432 - mae: 0.0603 - val_loss: 0.0419 - val_mae: 0.0544 - lr: 1.0000e-04\n",
      "Epoch 111/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0419 - mae: 0.0602\n",
      "Epoch 111: val_loss improved from 0.04193 to 0.04097, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0419 - mae: 0.0602 - val_loss: 0.0410 - val_mae: 0.0565 - lr: 1.0000e-04\n",
      "Epoch 112/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0407 - mae: 0.0606\n",
      "Epoch 112: val_loss improved from 0.04097 to 0.03933, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0407 - mae: 0.0606 - val_loss: 0.0393 - val_mae: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 113/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0390 - mae: 0.0591\n",
      "Epoch 113: val_loss improved from 0.03933 to 0.03774, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0390 - mae: 0.0591 - val_loss: 0.0377 - val_mae: 0.0520 - lr: 1.0000e-04\n",
      "Epoch 114/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0379 - mae: 0.0596\n",
      "Epoch 114: val_loss improved from 0.03774 to 0.03670, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0379 - mae: 0.0596 - val_loss: 0.0367 - val_mae: 0.0535 - lr: 1.0000e-04\n",
      "Epoch 115/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0365 - mae: 0.0583\n",
      "Epoch 115: val_loss improved from 0.03670 to 0.03559, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0365 - mae: 0.0583 - val_loss: 0.0356 - val_mae: 0.0544 - lr: 1.0000e-04\n",
      "Epoch 116/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0353 - mae: 0.0577\n",
      "Epoch 116: val_loss improved from 0.03559 to 0.03436, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0353 - mae: 0.0577 - val_loss: 0.0344 - val_mae: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 117/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0342 - mae: 0.0584\n",
      "Epoch 117: val_loss improved from 0.03436 to 0.03299, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0342 - mae: 0.0584 - val_loss: 0.0330 - val_mae: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 118/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0332 - mae: 0.0582\n",
      "Epoch 118: val_loss improved from 0.03299 to 0.03178, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0332 - mae: 0.0582 - val_loss: 0.0318 - val_mae: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 119/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0321 - mae: 0.0583\n",
      "Epoch 119: val_loss improved from 0.03178 to 0.03111, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0321 - mae: 0.0584 - val_loss: 0.0311 - val_mae: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 120/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0308 - mae: 0.0571\n",
      "Epoch 120: val_loss improved from 0.03111 to 0.02970, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0308 - mae: 0.0571 - val_loss: 0.0297 - val_mae: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 121/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0300 - mae: 0.0579\n",
      "Epoch 121: val_loss improved from 0.02970 to 0.02855, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0300 - mae: 0.0579 - val_loss: 0.0285 - val_mae: 0.0484 - lr: 1.0000e-04\n",
      "Epoch 122/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0570\n",
      "Epoch 122: val_loss improved from 0.02855 to 0.02789, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0290 - mae: 0.0570 - val_loss: 0.0279 - val_mae: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 123/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0278 - mae: 0.0551\n",
      "Epoch 123: val_loss improved from 0.02789 to 0.02698, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0278 - mae: 0.0551 - val_loss: 0.0270 - val_mae: 0.0495 - lr: 1.0000e-04\n",
      "Epoch 124/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0268 - mae: 0.0549\n",
      "Epoch 124: val_loss improved from 0.02698 to 0.02609, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0268 - mae: 0.0549 - val_loss: 0.0261 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 125/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0260 - mae: 0.0554\n",
      "Epoch 125: val_loss improved from 0.02609 to 0.02556, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0260 - mae: 0.0554 - val_loss: 0.0256 - val_mae: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 126/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0252 - mae: 0.0551\n",
      "Epoch 126: val_loss improved from 0.02556 to 0.02459, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0252 - mae: 0.0551 - val_loss: 0.0246 - val_mae: 0.0506 - lr: 1.0000e-04\n",
      "Epoch 127/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0246 - mae: 0.0557\n",
      "Epoch 127: val_loss improved from 0.02459 to 0.02363, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0246 - mae: 0.0558 - val_loss: 0.0236 - val_mae: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 128/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0238 - mae: 0.0557\n",
      "Epoch 128: val_loss improved from 0.02363 to 0.02298, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0238 - mae: 0.0558 - val_loss: 0.0230 - val_mae: 0.0495 - lr: 1.0000e-04\n",
      "Epoch 129/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0520\n",
      "Epoch 129: val_loss improved from 0.02298 to 0.02232, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0224 - mae: 0.0520 - val_loss: 0.0223 - val_mae: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 130/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0221 - mae: 0.0543\n",
      "Epoch 130: val_loss improved from 0.02232 to 0.02142, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0221 - mae: 0.0543 - val_loss: 0.0214 - val_mae: 0.0479 - lr: 1.0000e-04\n",
      "Epoch 131/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0214 - mae: 0.0538\n",
      "Epoch 131: val_loss improved from 0.02142 to 0.02101, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0214 - mae: 0.0539 - val_loss: 0.0210 - val_mae: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 132/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0534\n",
      "Epoch 132: val_loss improved from 0.02101 to 0.02005, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0206 - mae: 0.0534 - val_loss: 0.0200 - val_mae: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 133/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0198 - mae: 0.0524\n",
      "Epoch 133: val_loss improved from 0.02005 to 0.01939, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0198 - mae: 0.0524 - val_loss: 0.0194 - val_mae: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 134/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0539\n",
      "Epoch 134: val_loss improved from 0.01939 to 0.01879, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0195 - mae: 0.0538 - val_loss: 0.0188 - val_mae: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 135/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0186 - mae: 0.0519\n",
      "Epoch 135: val_loss improved from 0.01879 to 0.01822, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0186 - mae: 0.0519 - val_loss: 0.0182 - val_mae: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 136/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0184 - mae: 0.0542\n",
      "Epoch 136: val_loss improved from 0.01822 to 0.01759, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0184 - mae: 0.0542 - val_loss: 0.0176 - val_mae: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 137/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0178 - mae: 0.0537\n",
      "Epoch 137: val_loss improved from 0.01759 to 0.01731, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0178 - mae: 0.0537 - val_loss: 0.0173 - val_mae: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 138/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0172 - mae: 0.0529\n",
      "Epoch 138: val_loss improved from 0.01731 to 0.01671, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0172 - mae: 0.0529 - val_loss: 0.0167 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 139/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0168 - mae: 0.0531\n",
      "Epoch 139: val_loss improved from 0.01671 to 0.01651, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0168 - mae: 0.0531 - val_loss: 0.0165 - val_mae: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 140/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0160 - mae: 0.0510\n",
      "Epoch 140: val_loss improved from 0.01651 to 0.01582, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0160 - mae: 0.0510 - val_loss: 0.0158 - val_mae: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 141/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0155 - mae: 0.0507\n",
      "Epoch 141: val_loss improved from 0.01582 to 0.01518, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0155 - mae: 0.0507 - val_loss: 0.0152 - val_mae: 0.0441 - lr: 1.0000e-04\n",
      "Epoch 142/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0153 - mae: 0.0518\n",
      "Epoch 142: val_loss improved from 0.01518 to 0.01474, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0153 - mae: 0.0519 - val_loss: 0.0147 - val_mae: 0.0445 - lr: 1.0000e-04\n",
      "Epoch 143/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0150 - mae: 0.0523\n",
      "Epoch 143: val_loss improved from 0.01474 to 0.01465, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0150 - mae: 0.0524 - val_loss: 0.0146 - val_mae: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 144/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0145 - mae: 0.0516\n",
      "Epoch 144: val_loss improved from 0.01465 to 0.01410, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0145 - mae: 0.0516 - val_loss: 0.0141 - val_mae: 0.0455 - lr: 1.0000e-04\n",
      "Epoch 145/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0141 - mae: 0.0510\n",
      "Epoch 145: val_loss improved from 0.01410 to 0.01383, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0141 - mae: 0.0510 - val_loss: 0.0138 - val_mae: 0.0451 - lr: 1.0000e-04\n",
      "Epoch 146/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0135 - mae: 0.0500\n",
      "Epoch 146: val_loss improved from 0.01383 to 0.01352, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0135 - mae: 0.0500 - val_loss: 0.0135 - val_mae: 0.0456 - lr: 1.0000e-04\n",
      "Epoch 147/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0134 - mae: 0.0513\n",
      "Epoch 147: val_loss improved from 0.01352 to 0.01334, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0134 - mae: 0.0512 - val_loss: 0.0133 - val_mae: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 148/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0502\n",
      "Epoch 148: val_loss improved from 0.01334 to 0.01309, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.0130 - mae: 0.0502 - val_loss: 0.0131 - val_mae: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 149/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0127 - mae: 0.0501\n",
      "Epoch 149: val_loss improved from 0.01309 to 0.01246, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0127 - mae: 0.0501 - val_loss: 0.0125 - val_mae: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 150/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0124 - mae: 0.0498\n",
      "Epoch 150: val_loss improved from 0.01246 to 0.01233, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.0124 - mae: 0.0498 - val_loss: 0.0123 - val_mae: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 151/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0121 - mae: 0.0501\n",
      "Epoch 151: val_loss did not improve from 0.01233\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0121 - mae: 0.0501 - val_loss: 0.0123 - val_mae: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 152/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0512\n",
      "Epoch 152: val_loss improved from 0.01233 to 0.01180, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 16ms/step - loss: 0.0120 - mae: 0.0512 - val_loss: 0.0118 - val_mae: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 153/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0117 - mae: 0.0504\n",
      "Epoch 153: val_loss improved from 0.01180 to 0.01159, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0117 - mae: 0.0504 - val_loss: 0.0116 - val_mae: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 154/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0112 - mae: 0.0489\n",
      "Epoch 154: val_loss improved from 0.01159 to 0.01133, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0112 - mae: 0.0489 - val_loss: 0.0113 - val_mae: 0.0456 - lr: 1.0000e-04\n",
      "Epoch 155/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0477\n",
      "Epoch 155: val_loss improved from 0.01133 to 0.01119, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0108 - mae: 0.0477 - val_loss: 0.0112 - val_mae: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 156/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0112 - mae: 0.0514\n",
      "Epoch 156: val_loss improved from 0.01119 to 0.01111, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0112 - mae: 0.0514 - val_loss: 0.0111 - val_mae: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 157/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0109 - mae: 0.0509\n",
      "Epoch 157: val_loss improved from 0.01111 to 0.01050, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.0109 - mae: 0.0509 - val_loss: 0.0105 - val_mae: 0.0419 - lr: 1.0000e-04\n",
      "Epoch 158/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0105 - mae: 0.0494\n",
      "Epoch 158: val_loss improved from 0.01050 to 0.01039, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0105 - mae: 0.0494 - val_loss: 0.0104 - val_mae: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 159/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0105 - mae: 0.0511\n",
      "Epoch 159: val_loss improved from 0.01039 to 0.01013, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0105 - mae: 0.0511 - val_loss: 0.0101 - val_mae: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 160/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0100 - mae: 0.0482\n",
      "Epoch 160: val_loss improved from 0.01013 to 0.01009, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0100 - mae: 0.0482 - val_loss: 0.0101 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 161/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0499\n",
      "Epoch 161: val_loss improved from 0.01009 to 0.01007, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0102 - mae: 0.0501 - val_loss: 0.0101 - val_mae: 0.0435 - lr: 1.0000e-04\n",
      "Epoch 162/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0099 - mae: 0.0490\n",
      "Epoch 162: val_loss improved from 0.01007 to 0.00987, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0099 - mae: 0.0490 - val_loss: 0.0099 - val_mae: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 163/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0098 - mae: 0.0494\n",
      "Epoch 163: val_loss improved from 0.00987 to 0.00961, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0098 - mae: 0.0495 - val_loss: 0.0096 - val_mae: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 164/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0490\n",
      "Epoch 164: val_loss improved from 0.00961 to 0.00937, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0095 - mae: 0.0489 - val_loss: 0.0094 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 165/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0497\n",
      "Epoch 165: val_loss improved from 0.00937 to 0.00924, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0095 - mae: 0.0497 - val_loss: 0.0092 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 166/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0498\n",
      "Epoch 166: val_loss improved from 0.00924 to 0.00910, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0095 - mae: 0.0498 - val_loss: 0.0091 - val_mae: 0.0412 - lr: 1.0000e-04\n",
      "Epoch 167/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0089 - mae: 0.0475\n",
      "Epoch 167: val_loss did not improve from 0.00910\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0089 - mae: 0.0475 - val_loss: 0.0091 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 168/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0089 - mae: 0.0487\n",
      "Epoch 168: val_loss improved from 0.00910 to 0.00909, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0089 - mae: 0.0487 - val_loss: 0.0091 - val_mae: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 169/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0491\n",
      "Epoch 169: val_loss improved from 0.00909 to 0.00896, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0089 - mae: 0.0491 - val_loss: 0.0090 - val_mae: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 170/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0484\n",
      "Epoch 170: val_loss improved from 0.00896 to 0.00884, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0088 - mae: 0.0485 - val_loss: 0.0088 - val_mae: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 171/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0084 - mae: 0.0470\n",
      "Epoch 171: val_loss improved from 0.00884 to 0.00884, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0084 - mae: 0.0470 - val_loss: 0.0088 - val_mae: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 172/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0087 - mae: 0.0488\n",
      "Epoch 172: val_loss improved from 0.00884 to 0.00863, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0087 - mae: 0.0488 - val_loss: 0.0086 - val_mae: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 173/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0472\n",
      "Epoch 173: val_loss improved from 0.00863 to 0.00844, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0083 - mae: 0.0473 - val_loss: 0.0084 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 174/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0082 - mae: 0.0474\n",
      "Epoch 174: val_loss did not improve from 0.00844\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0082 - mae: 0.0474 - val_loss: 0.0086 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 175/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0084 - mae: 0.0493\n",
      "Epoch 175: val_loss improved from 0.00844 to 0.00826, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0084 - mae: 0.0493 - val_loss: 0.0083 - val_mae: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 176/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0081 - mae: 0.0474\n",
      "Epoch 176: val_loss did not improve from 0.00826\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0081 - mae: 0.0474 - val_loss: 0.0083 - val_mae: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 177/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0469\n",
      "Epoch 177: val_loss improved from 0.00826 to 0.00810, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0079 - mae: 0.0469 - val_loss: 0.0081 - val_mae: 0.0403 - lr: 1.0000e-04\n",
      "Epoch 178/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0081 - mae: 0.0485\n",
      "Epoch 178: val_loss improved from 0.00810 to 0.00808, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0081 - mae: 0.0485 - val_loss: 0.0081 - val_mae: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 179/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0481\n",
      "Epoch 179: val_loss did not improve from 0.00808\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0080 - mae: 0.0482 - val_loss: 0.0081 - val_mae: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 180/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0077 - mae: 0.0472\n",
      "Epoch 180: val_loss improved from 0.00808 to 0.00797, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0077 - mae: 0.0472 - val_loss: 0.0080 - val_mae: 0.0391 - lr: 1.0000e-04\n",
      "Epoch 181/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0078 - mae: 0.0478\n",
      "Epoch 181: val_loss improved from 0.00797 to 0.00777, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0078 - mae: 0.0478 - val_loss: 0.0078 - val_mae: 0.0400 - lr: 1.0000e-04\n",
      "Epoch 182/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0477\n",
      "Epoch 182: val_loss did not improve from 0.00777\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0076 - mae: 0.0477 - val_loss: 0.0080 - val_mae: 0.0407 - lr: 1.0000e-04\n",
      "Epoch 183/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0074 - mae: 0.0466\n",
      "Epoch 183: val_loss improved from 0.00777 to 0.00767, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0074 - mae: 0.0466 - val_loss: 0.0077 - val_mae: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 184/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0477\n",
      "Epoch 184: val_loss improved from 0.00767 to 0.00761, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0076 - mae: 0.0477 - val_loss: 0.0076 - val_mae: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 185/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0077 - mae: 0.0485\n",
      "Epoch 185: val_loss improved from 0.00761 to 0.00743, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0077 - mae: 0.0485 - val_loss: 0.0074 - val_mae: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 186/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0074 - mae: 0.0472\n",
      "Epoch 186: val_loss did not improve from 0.00743\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0074 - mae: 0.0472 - val_loss: 0.0076 - val_mae: 0.0405 - lr: 1.0000e-04\n",
      "Epoch 187/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0452\n",
      "Epoch 187: val_loss did not improve from 0.00743\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0069 - mae: 0.0452 - val_loss: 0.0077 - val_mae: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 188/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0073 - mae: 0.0472\n",
      "Epoch 188: val_loss did not improve from 0.00743\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0073 - mae: 0.0472 - val_loss: 0.0076 - val_mae: 0.0408 - lr: 1.0000e-04\n",
      "Epoch 189/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0073 - mae: 0.0480\n",
      "Epoch 189: val_loss improved from 0.00743 to 0.00706, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0073 - mae: 0.0480 - val_loss: 0.0071 - val_mae: 0.0379 - lr: 1.0000e-04\n",
      "Epoch 190/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0473\n",
      "Epoch 190: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0072 - mae: 0.0473 - val_loss: 0.0072 - val_mae: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 191/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0480\n",
      "Epoch 191: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0072 - mae: 0.0480 - val_loss: 0.0072 - val_mae: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 192/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0471\n",
      "Epoch 192: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0071 - mae: 0.0471 - val_loss: 0.0071 - val_mae: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 193/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0494\n",
      "Epoch 193: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0074 - mae: 0.0493 - val_loss: 0.0071 - val_mae: 0.0398 - lr: 1.0000e-04\n",
      "Epoch 194/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0070 - mae: 0.0471\n",
      "Epoch 194: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0070 - mae: 0.0471 - val_loss: 0.0074 - val_mae: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 195/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0468\n",
      "Epoch 195: val_loss did not improve from 0.00706\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0069 - mae: 0.0468 - val_loss: 0.0072 - val_mae: 0.0418 - lr: 1.0000e-04\n",
      "Epoch 196/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0066 - mae: 0.0456\n",
      "Epoch 196: val_loss improved from 0.00706 to 0.00679, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0066 - mae: 0.0456 - val_loss: 0.0068 - val_mae: 0.0380 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0068 - mae: 0.0473\n",
      "Epoch 197: val_loss did not improve from 0.00679\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0068 - mae: 0.0473 - val_loss: 0.0070 - val_mae: 0.0404 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0066 - mae: 0.0460\n",
      "Epoch 198: val_loss improved from 0.00679 to 0.00675, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0066 - mae: 0.0460 - val_loss: 0.0067 - val_mae: 0.0378 - lr: 5.0000e-05\n",
      "Epoch 199/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0443\n",
      "Epoch 199: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0062 - mae: 0.0443 - val_loss: 0.0069 - val_mae: 0.0392 - lr: 5.0000e-05\n",
      "Epoch 200/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0458\n",
      "Epoch 200: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0065 - mae: 0.0458 - val_loss: 0.0068 - val_mae: 0.0388 - lr: 5.0000e-05\n",
      "Epoch 201/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0065 - mae: 0.0460\n",
      "Epoch 201: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0065 - mae: 0.0460 - val_loss: 0.0069 - val_mae: 0.0404 - lr: 5.0000e-05\n",
      "Epoch 202/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0457\n",
      "Epoch 202: val_loss improved from 0.00675 to 0.00672, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0064 - mae: 0.0457 - val_loss: 0.0067 - val_mae: 0.0376 - lr: 5.0000e-05\n",
      "Epoch 203/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0435\n",
      "Epoch 203: val_loss improved from 0.00672 to 0.00669, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0062 - mae: 0.0437 - val_loss: 0.0067 - val_mae: 0.0380 - lr: 2.5000e-05\n",
      "Epoch 204/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0445\n",
      "Epoch 204: val_loss improved from 0.00669 to 0.00664, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0061 - mae: 0.0445 - val_loss: 0.0066 - val_mae: 0.0376 - lr: 2.5000e-05\n",
      "Epoch 205/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0453\n",
      "Epoch 205: val_loss improved from 0.00664 to 0.00660, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0063 - mae: 0.0453 - val_loss: 0.0066 - val_mae: 0.0377 - lr: 2.5000e-05\n",
      "Epoch 206/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0458\n",
      "Epoch 206: val_loss improved from 0.00660 to 0.00649, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0064 - mae: 0.0458 - val_loss: 0.0065 - val_mae: 0.0382 - lr: 2.5000e-05\n",
      "Epoch 207/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0437\n",
      "Epoch 207: val_loss improved from 0.00649 to 0.00640, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0437 - val_loss: 0.0064 - val_mae: 0.0374 - lr: 2.5000e-05\n",
      "Epoch 208/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0426\n",
      "Epoch 208: val_loss did not improve from 0.00640\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0059 - mae: 0.0426 - val_loss: 0.0064 - val_mae: 0.0376 - lr: 2.5000e-05\n",
      "Epoch 209/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0437\n",
      "Epoch 209: val_loss did not improve from 0.00640\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0437 - val_loss: 0.0064 - val_mae: 0.0378 - lr: 2.5000e-05\n",
      "Epoch 210/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0437\n",
      "Epoch 210: val_loss did not improve from 0.00640\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0437 - val_loss: 0.0064 - val_mae: 0.0385 - lr: 2.5000e-05\n",
      "Epoch 211/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0425\n",
      "Epoch 211: val_loss improved from 0.00640 to 0.00638, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0058 - mae: 0.0425 - val_loss: 0.0064 - val_mae: 0.0381 - lr: 2.5000e-05\n",
      "Epoch 212/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0445\n",
      "Epoch 212: val_loss improved from 0.00638 to 0.00638, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0060 - mae: 0.0446 - val_loss: 0.0064 - val_mae: 0.0380 - lr: 2.5000e-05\n",
      "Epoch 213/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0436\n",
      "Epoch 213: val_loss improved from 0.00638 to 0.00628, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0059 - mae: 0.0436 - val_loss: 0.0063 - val_mae: 0.0374 - lr: 2.5000e-05\n",
      "Epoch 214/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0426\n",
      "Epoch 214: val_loss did not improve from 0.00628\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0057 - mae: 0.0426 - val_loss: 0.0063 - val_mae: 0.0381 - lr: 2.5000e-05\n",
      "Epoch 215/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0442\n",
      "Epoch 215: val_loss improved from 0.00628 to 0.00624, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0059 - mae: 0.0442 - val_loss: 0.0062 - val_mae: 0.0377 - lr: 2.5000e-05\n",
      "Epoch 216/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0442\n",
      "Epoch 216: val_loss improved from 0.00624 to 0.00619, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0059 - mae: 0.0442 - val_loss: 0.0062 - val_mae: 0.0374 - lr: 2.5000e-05\n",
      "Epoch 217/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0424\n",
      "Epoch 217: val_loss improved from 0.00619 to 0.00610, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0056 - mae: 0.0424 - val_loss: 0.0061 - val_mae: 0.0368 - lr: 2.5000e-05\n",
      "Epoch 218/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0437\n",
      "Epoch 218: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0437 - val_loss: 0.0061 - val_mae: 0.0372 - lr: 2.5000e-05\n",
      "Epoch 219/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0440\n",
      "Epoch 219: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0059 - mae: 0.0440 - val_loss: 0.0061 - val_mae: 0.0370 - lr: 2.5000e-05\n",
      "Epoch 220/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0058 - mae: 0.0443\n",
      "Epoch 220: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0058 - mae: 0.0443 - val_loss: 0.0061 - val_mae: 0.0370 - lr: 2.5000e-05\n",
      "Epoch 221/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0431\n",
      "Epoch 221: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0057 - mae: 0.0431 - val_loss: 0.0061 - val_mae: 0.0373 - lr: 2.5000e-05\n",
      "Epoch 222/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0426\n",
      "Epoch 222: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0056 - mae: 0.0426 - val_loss: 0.0062 - val_mae: 0.0375 - lr: 2.5000e-05\n",
      "Epoch 223/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0444\n",
      "Epoch 223: val_loss did not improve from 0.00610\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0058 - mae: 0.0444 - val_loss: 0.0061 - val_mae: 0.0374 - lr: 2.5000e-05\n",
      "Epoch 224/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0432\n",
      "Epoch 224: val_loss improved from 0.00610 to 0.00609, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0432 - val_loss: 0.0061 - val_mae: 0.0373 - lr: 1.2500e-05\n",
      "Epoch 225/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0058 - mae: 0.0440\n",
      "Epoch 225: val_loss did not improve from 0.00609\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0058 - mae: 0.0440 - val_loss: 0.0061 - val_mae: 0.0373 - lr: 1.2500e-05\n",
      "Epoch 226/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0445\n",
      "Epoch 226: val_loss improved from 0.00609 to 0.00605, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0058 - mae: 0.0445 - val_loss: 0.0060 - val_mae: 0.0370 - lr: 1.2500e-05\n",
      "Epoch 227/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0432\n",
      "Epoch 227: val_loss improved from 0.00605 to 0.00601, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0432 - val_loss: 0.0060 - val_mae: 0.0370 - lr: 1.2500e-05\n",
      "Epoch 228/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0431\n",
      "Epoch 228: val_loss improved from 0.00601 to 0.00596, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0432 - val_loss: 0.0060 - val_mae: 0.0364 - lr: 1.2500e-05\n",
      "Epoch 229/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0425\n",
      "Epoch 229: val_loss improved from 0.00596 to 0.00595, saving model to models\\date_predictor-v25_fold1.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0425 - val_loss: 0.0060 - val_mae: 0.0365 - lr: 1.2500e-05\n",
      "Epoch 230/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0442\n",
      "Epoch 230: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0442 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.2500e-05\n",
      "Epoch 231/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0438\n",
      "Epoch 231: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0057 - mae: 0.0438 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.2500e-05\n",
      "Epoch 232/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0423\n",
      "Epoch 232: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0054 - mae: 0.0424 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 1.2500e-05\n",
      "Epoch 233/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0430\n",
      "Epoch 233: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0054 - mae: 0.0430 - val_loss: 0.0060 - val_mae: 0.0372 - lr: 1.2500e-05\n",
      "Epoch 234/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0436\n",
      "Epoch 234: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0057 - mae: 0.0436 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 1.2500e-05\n",
      "Epoch 235/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0434\n",
      "Epoch 235: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0055 - mae: 0.0434 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 6.2500e-06\n",
      "Epoch 236/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0437\n",
      "Epoch 236: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0437 - val_loss: 0.0060 - val_mae: 0.0370 - lr: 6.2500e-06\n",
      "Epoch 237/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0429\n",
      "Epoch 237: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0054 - mae: 0.0429 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 6.2500e-06\n",
      "Epoch 238/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0429\n",
      "Epoch 238: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0429 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 6.2500e-06\n",
      "Epoch 239/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0419\n",
      "Epoch 239: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0419 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 6.2500e-06\n",
      "Epoch 240/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0445\n",
      "Epoch 240: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0057 - mae: 0.0444 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 6.2500e-06\n",
      "Epoch 241/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0423\n",
      "Epoch 241: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0054 - mae: 0.0423 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 3.1250e-06\n",
      "Epoch 242/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0424\n",
      "Epoch 242: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0425 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 3.1250e-06\n",
      "Epoch 243/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0425\n",
      "Epoch 243: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 3.1250e-06\n",
      "Epoch 244/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0436\n",
      "Epoch 244: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0055 - mae: 0.0436 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 3.1250e-06\n",
      "Epoch 245/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0431\n",
      "Epoch 245: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0055 - mae: 0.0431 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 3.1250e-06\n",
      "Epoch 246/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0435\n",
      "Epoch 246: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0435 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 3.1250e-06\n",
      "Epoch 247/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0442\n",
      "Epoch 247: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0056 - mae: 0.0443 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.5625e-06\n",
      "Epoch 248/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0425\n",
      "Epoch 248: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0425 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.5625e-06\n",
      "Epoch 249/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0428\n",
      "Epoch 249: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0428 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.5625e-06\n",
      "Epoch 250/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0441\n",
      "Epoch 250: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0441 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.5625e-06\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "Fold 1 MAE: 55.445733466756685\n",
      "\n",
      "--- Training Fold 2/5 ---\n",
      "Epoch 1/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 4.2237 - mae: 1.5753\n",
      "Epoch 1: val_loss improved from inf to 0.39426, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 6s 22ms/step - loss: 4.2130 - mae: 1.5733 - val_loss: 0.3943 - val_mae: 0.4267 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 3.5419 - mae: 1.4453\n",
      "Epoch 2: val_loss improved from 0.39426 to 0.30589, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 3.5523 - mae: 1.4466 - val_loss: 0.3059 - val_mae: 0.3539 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 2.8136 - mae: 1.2791\n",
      "Epoch 3: val_loss improved from 0.30589 to 0.26621, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.8093 - mae: 1.2782 - val_loss: 0.2662 - val_mae: 0.3167 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.3713 - mae: 1.1592\n",
      "Epoch 4: val_loss improved from 0.26621 to 0.23807, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 2.3734 - mae: 1.1599 - val_loss: 0.2381 - val_mae: 0.2885 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.0551 - mae: 1.0870\n",
      "Epoch 5: val_loss improved from 0.23807 to 0.22094, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.0541 - mae: 1.0868 - val_loss: 0.2209 - val_mae: 0.2713 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.7461 - mae: 0.9946\n",
      "Epoch 6: val_loss improved from 0.22094 to 0.20630, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.7456 - mae: 0.9945 - val_loss: 0.2063 - val_mae: 0.2556 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.4736 - mae: 0.9104\n",
      "Epoch 7: val_loss improved from 0.20630 to 0.19940, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 1.4711 - mae: 0.9099 - val_loss: 0.1994 - val_mae: 0.2482 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.3060 - mae: 0.8556\n",
      "Epoch 8: val_loss improved from 0.19940 to 0.18719, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 1.3061 - mae: 0.8557 - val_loss: 0.1872 - val_mae: 0.2340 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.1610 - mae: 0.7977\n",
      "Epoch 9: val_loss improved from 0.18719 to 0.17778, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 1.1610 - mae: 0.7977 - val_loss: 0.1778 - val_mae: 0.2222 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.0248 - mae: 0.7527\n",
      "Epoch 10: val_loss improved from 0.17778 to 0.17237, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 1.0255 - mae: 0.7529 - val_loss: 0.1724 - val_mae: 0.2160 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9470 - mae: 0.7143\n",
      "Epoch 11: val_loss improved from 0.17237 to 0.16434, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.9470 - mae: 0.7143 - val_loss: 0.1643 - val_mae: 0.2057 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8037 - mae: 0.6512\n",
      "Epoch 12: val_loss improved from 0.16434 to 0.15853, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.8037 - mae: 0.6512 - val_loss: 0.1585 - val_mae: 0.1981 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.7237 - mae: 0.6169\n",
      "Epoch 13: val_loss improved from 0.15853 to 0.15432, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.7247 - mae: 0.6173 - val_loss: 0.1543 - val_mae: 0.1927 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.6338 - mae: 0.5713\n",
      "Epoch 14: val_loss improved from 0.15432 to 0.15045, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.6343 - mae: 0.5716 - val_loss: 0.1504 - val_mae: 0.1865 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.5637 - mae: 0.5304\n",
      "Epoch 15: val_loss improved from 0.15045 to 0.14746, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.5632 - mae: 0.5303 - val_loss: 0.1475 - val_mae: 0.1824 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.5183 - mae: 0.5061\n",
      "Epoch 16: val_loss improved from 0.14746 to 0.14448, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.5190 - mae: 0.5064 - val_loss: 0.1445 - val_mae: 0.1772 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.4584 - mae: 0.4645\n",
      "Epoch 17: val_loss improved from 0.14448 to 0.14259, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.4578 - mae: 0.4640 - val_loss: 0.1426 - val_mae: 0.1742 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.4088 - mae: 0.4367\n",
      "Epoch 18: val_loss improved from 0.14259 to 0.14089, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.4084 - mae: 0.4365 - val_loss: 0.1409 - val_mae: 0.1714 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3698 - mae: 0.4072\n",
      "Epoch 19: val_loss improved from 0.14089 to 0.13929, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.3698 - mae: 0.4072 - val_loss: 0.1393 - val_mae: 0.1686 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.3333 - mae: 0.3786\n",
      "Epoch 20: val_loss improved from 0.13929 to 0.13826, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.3326 - mae: 0.3780 - val_loss: 0.1383 - val_mae: 0.1667 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.3094 - mae: 0.3608\n",
      "Epoch 21: val_loss improved from 0.13826 to 0.13693, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.3098 - mae: 0.3611 - val_loss: 0.1369 - val_mae: 0.1642 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2834 - mae: 0.3360\n",
      "Epoch 22: val_loss improved from 0.13693 to 0.13621, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2837 - mae: 0.3364 - val_loss: 0.1362 - val_mae: 0.1628 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2619 - mae: 0.3185\n",
      "Epoch 23: val_loss improved from 0.13621 to 0.13591, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2619 - mae: 0.3185 - val_loss: 0.1359 - val_mae: 0.1621 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2389 - mae: 0.2977\n",
      "Epoch 24: val_loss improved from 0.13591 to 0.13509, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.2390 - mae: 0.2978 - val_loss: 0.1351 - val_mae: 0.1604 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2253 - mae: 0.2828\n",
      "Epoch 25: val_loss improved from 0.13509 to 0.13488, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2253 - mae: 0.2828 - val_loss: 0.1349 - val_mae: 0.1600 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2141 - mae: 0.2704\n",
      "Epoch 26: val_loss did not improve from 0.13488\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2141 - mae: 0.2704 - val_loss: 0.1354 - val_mae: 0.1610 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1977 - mae: 0.2523\n",
      "Epoch 27: val_loss improved from 0.13488 to 0.13488, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1975 - mae: 0.2521 - val_loss: 0.1349 - val_mae: 0.1602 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1852 - mae: 0.2350\n",
      "Epoch 28: val_loss improved from 0.13488 to 0.13442, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1855 - mae: 0.2354 - val_loss: 0.1344 - val_mae: 0.1594 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1779 - mae: 0.2269\n",
      "Epoch 29: val_loss improved from 0.13442 to 0.13399, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1778 - mae: 0.2269 - val_loss: 0.1340 - val_mae: 0.1587 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1709 - mae: 0.2170\n",
      "Epoch 30: val_loss improved from 0.13399 to 0.13387, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1708 - mae: 0.2169 - val_loss: 0.1339 - val_mae: 0.1586 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1656 - mae: 0.2104\n",
      "Epoch 31: val_loss improved from 0.13387 to 0.13364, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1656 - mae: 0.2104 - val_loss: 0.1336 - val_mae: 0.1583 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1595 - mae: 0.2007\n",
      "Epoch 32: val_loss improved from 0.13364 to 0.13330, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1595 - mae: 0.2006 - val_loss: 0.1333 - val_mae: 0.1577 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1564 - mae: 0.1956\n",
      "Epoch 33: val_loss improved from 0.13330 to 0.13322, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1565 - mae: 0.1956 - val_loss: 0.1332 - val_mae: 0.1577 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1526 - mae: 0.1893\n",
      "Epoch 34: val_loss did not improve from 0.13322\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1526 - mae: 0.1892 - val_loss: 0.1335 - val_mae: 0.1586 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1485 - mae: 0.1837\n",
      "Epoch 35: val_loss did not improve from 0.13322\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1484 - mae: 0.1836 - val_loss: 0.1335 - val_mae: 0.1587 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1452 - mae: 0.1781\n",
      "Epoch 36: val_loss improved from 0.13322 to 0.13283, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1452 - mae: 0.1782 - val_loss: 0.1328 - val_mae: 0.1574 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1429 - mae: 0.1740\n",
      "Epoch 37: val_loss improved from 0.13283 to 0.13247, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1429 - mae: 0.1740 - val_loss: 0.1325 - val_mae: 0.1568 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1410 - mae: 0.1712\n",
      "Epoch 38: val_loss improved from 0.13247 to 0.13203, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1410 - mae: 0.1713 - val_loss: 0.1320 - val_mae: 0.1558 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1394 - mae: 0.1689\n",
      "Epoch 39: val_loss did not improve from 0.13203\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1394 - mae: 0.1689 - val_loss: 0.1321 - val_mae: 0.1563 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1388 - mae: 0.1684\n",
      "Epoch 40: val_loss improved from 0.13203 to 0.13185, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1388 - mae: 0.1684 - val_loss: 0.1318 - val_mae: 0.1558 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1361 - mae: 0.1622\n",
      "Epoch 41: val_loss improved from 0.13185 to 0.13165, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1361 - mae: 0.1622 - val_loss: 0.1317 - val_mae: 0.1556 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1356 - mae: 0.1622\n",
      "Epoch 42: val_loss improved from 0.13165 to 0.13148, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1356 - mae: 0.1622 - val_loss: 0.1315 - val_mae: 0.1553 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1347 - mae: 0.1604\n",
      "Epoch 43: val_loss improved from 0.13148 to 0.13134, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1346 - mae: 0.1602 - val_loss: 0.1313 - val_mae: 0.1552 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1341 - mae: 0.1598\n",
      "Epoch 44: val_loss improved from 0.13134 to 0.13118, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1341 - mae: 0.1598 - val_loss: 0.1312 - val_mae: 0.1552 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1332 - mae: 0.1577\n",
      "Epoch 45: val_loss improved from 0.13118 to 0.13113, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1331 - mae: 0.1576 - val_loss: 0.1311 - val_mae: 0.1553 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1324 - mae: 0.1575\n",
      "Epoch 46: val_loss improved from 0.13113 to 0.13105, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1325 - mae: 0.1575 - val_loss: 0.1310 - val_mae: 0.1554 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1313 - mae: 0.1552\n",
      "Epoch 47: val_loss improved from 0.13105 to 0.13089, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1313 - mae: 0.1552 - val_loss: 0.1309 - val_mae: 0.1554 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1310 - mae: 0.1541\n",
      "Epoch 48: val_loss improved from 0.13089 to 0.13074, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1310 - mae: 0.1542 - val_loss: 0.1307 - val_mae: 0.1554 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1308 - mae: 0.1546\n",
      "Epoch 49: val_loss improved from 0.13074 to 0.13059, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1308 - mae: 0.1547 - val_loss: 0.1306 - val_mae: 0.1553 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1302 - mae: 0.1540\n",
      "Epoch 50: val_loss improved from 0.13059 to 0.13043, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1302 - mae: 0.1540 - val_loss: 0.1304 - val_mae: 0.1556 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1295 - mae: 0.1525\n",
      "Epoch 51: val_loss improved from 0.13043 to 0.13020, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1295 - mae: 0.1525 - val_loss: 0.1302 - val_mae: 0.1556 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1292 - mae: 0.1522\n",
      "Epoch 52: val_loss improved from 0.13020 to 0.12990, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1292 - mae: 0.1521 - val_loss: 0.1299 - val_mae: 0.1554 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1287 - mae: 0.1517\n",
      "Epoch 53: val_loss improved from 0.12990 to 0.12979, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1287 - mae: 0.1517 - val_loss: 0.1298 - val_mae: 0.1560 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1285 - mae: 0.1518\n",
      "Epoch 54: val_loss improved from 0.12979 to 0.12959, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1285 - mae: 0.1518 - val_loss: 0.1296 - val_mae: 0.1563 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1281 - mae: 0.1513\n",
      "Epoch 55: val_loss improved from 0.12959 to 0.12928, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1281 - mae: 0.1514 - val_loss: 0.1293 - val_mae: 0.1562 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1272 - mae: 0.1498\n",
      "Epoch 56: val_loss improved from 0.12928 to 0.12875, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1272 - mae: 0.1498 - val_loss: 0.1287 - val_mae: 0.1557 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1273 - mae: 0.1510\n",
      "Epoch 57: val_loss improved from 0.12875 to 0.12840, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1272 - mae: 0.1508 - val_loss: 0.1284 - val_mae: 0.1557 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1269 - mae: 0.1507\n",
      "Epoch 58: val_loss improved from 0.12840 to 0.12784, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1269 - mae: 0.1508 - val_loss: 0.1278 - val_mae: 0.1551 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1259 - mae: 0.1496\n",
      "Epoch 59: val_loss improved from 0.12784 to 0.12772, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1259 - mae: 0.1496 - val_loss: 0.1277 - val_mae: 0.1564 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1255 - mae: 0.1496\n",
      "Epoch 60: val_loss improved from 0.12772 to 0.12738, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1255 - mae: 0.1497 - val_loss: 0.1274 - val_mae: 0.1570 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1249 - mae: 0.1490\n",
      "Epoch 61: val_loss improved from 0.12738 to 0.12650, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.1249 - mae: 0.1491 - val_loss: 0.1265 - val_mae: 0.1559 - lr: 1.0000e-04\n",
      "Epoch 62/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1243 - mae: 0.1490\n",
      "Epoch 62: val_loss improved from 0.12650 to 0.12589, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1244 - mae: 0.1490 - val_loss: 0.1259 - val_mae: 0.1559 - lr: 1.0000e-04\n",
      "Epoch 63/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1233 - mae: 0.1476\n",
      "Epoch 63: val_loss improved from 0.12589 to 0.12484, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1233 - mae: 0.1477 - val_loss: 0.1248 - val_mae: 0.1550 - lr: 1.0000e-04\n",
      "Epoch 64/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1222 - mae: 0.1464\n",
      "Epoch 64: val_loss improved from 0.12484 to 0.12379, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1222 - mae: 0.1463 - val_loss: 0.1238 - val_mae: 0.1543 - lr: 1.0000e-04\n",
      "Epoch 65/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1209 - mae: 0.1448\n",
      "Epoch 65: val_loss improved from 0.12379 to 0.12240, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1209 - mae: 0.1448 - val_loss: 0.1224 - val_mae: 0.1528 - lr: 1.0000e-04\n",
      "Epoch 66/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1192 - mae: 0.1418\n",
      "Epoch 66: val_loss improved from 0.12240 to 0.12041, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1192 - mae: 0.1419 - val_loss: 0.1204 - val_mae: 0.1500 - lr: 1.0000e-04\n",
      "Epoch 67/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1171 - mae: 0.1379\n",
      "Epoch 67: val_loss improved from 0.12041 to 0.11868, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1171 - mae: 0.1380 - val_loss: 0.1187 - val_mae: 0.1476 - lr: 1.0000e-04\n",
      "Epoch 68/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1154 - mae: 0.1353\n",
      "Epoch 68: val_loss improved from 0.11868 to 0.11544, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1154 - mae: 0.1352 - val_loss: 0.1154 - val_mae: 0.1409 - lr: 1.0000e-04\n",
      "Epoch 69/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1124 - mae: 0.1292\n",
      "Epoch 69: val_loss improved from 0.11544 to 0.11179, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1124 - mae: 0.1292 - val_loss: 0.1118 - val_mae: 0.1324 - lr: 1.0000e-04\n",
      "Epoch 70/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1106 - mae: 0.1258\n",
      "Epoch 70: val_loss improved from 0.11179 to 0.10884, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.1106 - mae: 0.1258 - val_loss: 0.1088 - val_mae: 0.1258 - lr: 1.0000e-04\n",
      "Epoch 71/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1077 - mae: 0.1203\n",
      "Epoch 71: val_loss improved from 0.10884 to 0.10617, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1077 - mae: 0.1203 - val_loss: 0.1062 - val_mae: 0.1202 - lr: 1.0000e-04\n",
      "Epoch 72/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1052 - mae: 0.1153\n",
      "Epoch 72: val_loss improved from 0.10617 to 0.10330, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1053 - mae: 0.1155 - val_loss: 0.1033 - val_mae: 0.1133 - lr: 1.0000e-04\n",
      "Epoch 73/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1030 - mae: 0.1107\n",
      "Epoch 73: val_loss improved from 0.10330 to 0.10041, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1030 - mae: 0.1107 - val_loss: 0.1004 - val_mae: 0.1061 - lr: 1.0000e-04\n",
      "Epoch 74/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1007 - mae: 0.1067\n",
      "Epoch 74: val_loss improved from 0.10041 to 0.09825, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1008 - mae: 0.1069 - val_loss: 0.0982 - val_mae: 0.1013 - lr: 1.0000e-04\n",
      "Epoch 75/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0988 - mae: 0.1031\n",
      "Epoch 75: val_loss improved from 0.09825 to 0.09631, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0988 - mae: 0.1030 - val_loss: 0.0963 - val_mae: 0.0971 - lr: 1.0000e-04\n",
      "Epoch 76/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0970 - mae: 0.1008\n",
      "Epoch 76: val_loss improved from 0.09631 to 0.09400, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0970 - mae: 0.1008 - val_loss: 0.0940 - val_mae: 0.0912 - lr: 1.0000e-04\n",
      "Epoch 77/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0948 - mae: 0.0968\n",
      "Epoch 77: val_loss improved from 0.09400 to 0.09224, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0948 - mae: 0.0967 - val_loss: 0.0922 - val_mae: 0.0883 - lr: 1.0000e-04\n",
      "Epoch 78/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0932 - mae: 0.0944\n",
      "Epoch 78: val_loss improved from 0.09224 to 0.09096, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0932 - mae: 0.0944 - val_loss: 0.0910 - val_mae: 0.0868 - lr: 1.0000e-04\n",
      "Epoch 79/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0913 - mae: 0.0908\n",
      "Epoch 79: val_loss improved from 0.09096 to 0.08975, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0913 - mae: 0.0908 - val_loss: 0.0898 - val_mae: 0.0863 - lr: 1.0000e-04\n",
      "Epoch 80/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0897 - mae: 0.0886\n",
      "Epoch 80: val_loss improved from 0.08975 to 0.08843, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0897 - mae: 0.0886 - val_loss: 0.0884 - val_mae: 0.0851 - lr: 1.0000e-04\n",
      "Epoch 81/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0880 - mae: 0.0867\n",
      "Epoch 81: val_loss improved from 0.08843 to 0.08637, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0881 - mae: 0.0869 - val_loss: 0.0864 - val_mae: 0.0808 - lr: 1.0000e-04\n",
      "Epoch 82/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0868 - mae: 0.0863\n",
      "Epoch 82: val_loss improved from 0.08637 to 0.08491, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0868 - mae: 0.0863 - val_loss: 0.0849 - val_mae: 0.0801 - lr: 1.0000e-04\n",
      "Epoch 83/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0851 - mae: 0.0844\n",
      "Epoch 83: val_loss improved from 0.08491 to 0.08392, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0851 - mae: 0.0844 - val_loss: 0.0839 - val_mae: 0.0808 - lr: 1.0000e-04\n",
      "Epoch 84/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0836 - mae: 0.0831\n",
      "Epoch 84: val_loss improved from 0.08392 to 0.08191, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0836 - mae: 0.0831 - val_loss: 0.0819 - val_mae: 0.0770 - lr: 1.0000e-04\n",
      "Epoch 85/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0821 - mae: 0.0827\n",
      "Epoch 85: val_loss improved from 0.08191 to 0.08036, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0821 - mae: 0.0827 - val_loss: 0.0804 - val_mae: 0.0755 - lr: 1.0000e-04\n",
      "Epoch 86/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0803 - mae: 0.0805\n",
      "Epoch 86: val_loss improved from 0.08036 to 0.07885, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0803 - mae: 0.0805 - val_loss: 0.0788 - val_mae: 0.0751 - lr: 1.0000e-04\n",
      "Epoch 87/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0786 - mae: 0.0792\n",
      "Epoch 87: val_loss improved from 0.07885 to 0.07739, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0787 - mae: 0.0793 - val_loss: 0.0774 - val_mae: 0.0738 - lr: 1.0000e-04\n",
      "Epoch 88/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0775 - mae: 0.0794\n",
      "Epoch 88: val_loss improved from 0.07739 to 0.07583, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0775 - mae: 0.0794 - val_loss: 0.0758 - val_mae: 0.0732 - lr: 1.0000e-04\n",
      "Epoch 89/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0753 - mae: 0.0766\n",
      "Epoch 89: val_loss improved from 0.07583 to 0.07416, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0753 - mae: 0.0766 - val_loss: 0.0742 - val_mae: 0.0721 - lr: 1.0000e-04\n",
      "Epoch 90/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0736 - mae: 0.0750\n",
      "Epoch 90: val_loss improved from 0.07416 to 0.07259, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0736 - mae: 0.0750 - val_loss: 0.0726 - val_mae: 0.0714 - lr: 1.0000e-04\n",
      "Epoch 91/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0722 - mae: 0.0753\n",
      "Epoch 91: val_loss improved from 0.07259 to 0.07133, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0722 - mae: 0.0752 - val_loss: 0.0713 - val_mae: 0.0725 - lr: 1.0000e-04\n",
      "Epoch 92/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0702 - mae: 0.0732\n",
      "Epoch 92: val_loss improved from 0.07133 to 0.06934, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0702 - mae: 0.0732 - val_loss: 0.0693 - val_mae: 0.0703 - lr: 1.0000e-04\n",
      "Epoch 93/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0686 - mae: 0.0734\n",
      "Epoch 93: val_loss improved from 0.06934 to 0.06769, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0686 - mae: 0.0734 - val_loss: 0.0677 - val_mae: 0.0697 - lr: 1.0000e-04\n",
      "Epoch 94/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0670 - mae: 0.0720\n",
      "Epoch 94: val_loss improved from 0.06769 to 0.06624, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0670 - mae: 0.0720 - val_loss: 0.0662 - val_mae: 0.0694 - lr: 1.0000e-04\n",
      "Epoch 95/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0650 - mae: 0.0700\n",
      "Epoch 95: val_loss improved from 0.06624 to 0.06406, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0650 - mae: 0.0700 - val_loss: 0.0641 - val_mae: 0.0663 - lr: 1.0000e-04\n",
      "Epoch 96/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0635 - mae: 0.0703\n",
      "Epoch 96: val_loss improved from 0.06406 to 0.06300, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0635 - mae: 0.0703 - val_loss: 0.0630 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 97/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0614 - mae: 0.0682\n",
      "Epoch 97: val_loss improved from 0.06300 to 0.06079, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0614 - mae: 0.0682 - val_loss: 0.0608 - val_mae: 0.0657 - lr: 1.0000e-04\n",
      "Epoch 98/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0600 - mae: 0.0688\n",
      "Epoch 98: val_loss improved from 0.06079 to 0.05938, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0600 - mae: 0.0688 - val_loss: 0.0594 - val_mae: 0.0669 - lr: 1.0000e-04\n",
      "Epoch 99/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0581 - mae: 0.0677\n",
      "Epoch 99: val_loss improved from 0.05938 to 0.05739, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0581 - mae: 0.0677 - val_loss: 0.0574 - val_mae: 0.0642 - lr: 1.0000e-04\n",
      "Epoch 100/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0564 - mae: 0.0674\n",
      "Epoch 100: val_loss improved from 0.05739 to 0.05551, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0563 - mae: 0.0673 - val_loss: 0.0555 - val_mae: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 101/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0546 - mae: 0.0657\n",
      "Epoch 101: val_loss improved from 0.05551 to 0.05416, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0546 - mae: 0.0657 - val_loss: 0.0542 - val_mae: 0.0642 - lr: 1.0000e-04\n",
      "Epoch 102/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0527 - mae: 0.0646\n",
      "Epoch 102: val_loss improved from 0.05416 to 0.05220, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0527 - mae: 0.0649 - val_loss: 0.0522 - val_mae: 0.0622 - lr: 1.0000e-04\n",
      "Epoch 103/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0513 - mae: 0.0650\n",
      "Epoch 103: val_loss improved from 0.05220 to 0.05085, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0513 - mae: 0.0650 - val_loss: 0.0508 - val_mae: 0.0631 - lr: 1.0000e-04\n",
      "Epoch 104/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0496 - mae: 0.0644\n",
      "Epoch 104: val_loss improved from 0.05085 to 0.04873, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0496 - mae: 0.0644 - val_loss: 0.0487 - val_mae: 0.0596 - lr: 1.0000e-04\n",
      "Epoch 105/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0476 - mae: 0.0622\n",
      "Epoch 105: val_loss improved from 0.04873 to 0.04764, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0476 - mae: 0.0621 - val_loss: 0.0476 - val_mae: 0.0622 - lr: 1.0000e-04\n",
      "Epoch 106/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0463 - mae: 0.0633\n",
      "Epoch 106: val_loss improved from 0.04764 to 0.04570, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0463 - mae: 0.0633 - val_loss: 0.0457 - val_mae: 0.0599 - lr: 1.0000e-04\n",
      "Epoch 107/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0446 - mae: 0.0623\n",
      "Epoch 107: val_loss improved from 0.04570 to 0.04401, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0446 - mae: 0.0624 - val_loss: 0.0440 - val_mae: 0.0581 - lr: 1.0000e-04\n",
      "Epoch 108/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0433 - mae: 0.0624\n",
      "Epoch 108: val_loss improved from 0.04401 to 0.04275, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0433 - mae: 0.0625 - val_loss: 0.0427 - val_mae: 0.0596 - lr: 1.0000e-04\n",
      "Epoch 109/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0419 - mae: 0.0628\n",
      "Epoch 109: val_loss improved from 0.04275 to 0.04131, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0419 - mae: 0.0628 - val_loss: 0.0413 - val_mae: 0.0596 - lr: 1.0000e-04\n",
      "Epoch 110/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0404 - mae: 0.0624\n",
      "Epoch 110: val_loss improved from 0.04131 to 0.03961, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0404 - mae: 0.0624 - val_loss: 0.0396 - val_mae: 0.0572 - lr: 1.0000e-04\n",
      "Epoch 111/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0386 - mae: 0.0597\n",
      "Epoch 111: val_loss improved from 0.03961 to 0.03838, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0386 - mae: 0.0597 - val_loss: 0.0384 - val_mae: 0.0586 - lr: 1.0000e-04\n",
      "Epoch 112/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0371 - mae: 0.0589\n",
      "Epoch 112: val_loss improved from 0.03838 to 0.03699, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0371 - mae: 0.0589 - val_loss: 0.0370 - val_mae: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 113/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0358 - mae: 0.0588\n",
      "Epoch 113: val_loss improved from 0.03699 to 0.03595, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0358 - mae: 0.0589 - val_loss: 0.0360 - val_mae: 0.0592 - lr: 1.0000e-04\n",
      "Epoch 114/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0346 - mae: 0.0596\n",
      "Epoch 114: val_loss improved from 0.03595 to 0.03513, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0346 - mae: 0.0596 - val_loss: 0.0351 - val_mae: 0.0615 - lr: 1.0000e-04\n",
      "Epoch 115/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0331 - mae: 0.0573\n",
      "Epoch 115: val_loss improved from 0.03513 to 0.03313, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0331 - mae: 0.0574 - val_loss: 0.0331 - val_mae: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 116/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0321 - mae: 0.0580\n",
      "Epoch 116: val_loss improved from 0.03313 to 0.03247, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0321 - mae: 0.0580 - val_loss: 0.0325 - val_mae: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 117/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0312 - mae: 0.0583\n",
      "Epoch 117: val_loss improved from 0.03247 to 0.03088, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0312 - mae: 0.0584 - val_loss: 0.0309 - val_mae: 0.0561 - lr: 1.0000e-04\n",
      "Epoch 118/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0296 - mae: 0.0560\n",
      "Epoch 118: val_loss improved from 0.03088 to 0.03000, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0296 - mae: 0.0561 - val_loss: 0.0300 - val_mae: 0.0568 - lr: 1.0000e-04\n",
      "Epoch 119/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0286 - mae: 0.0563\n",
      "Epoch 119: val_loss improved from 0.03000 to 0.02895, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0287 - mae: 0.0564 - val_loss: 0.0289 - val_mae: 0.0563 - lr: 1.0000e-04\n",
      "Epoch 120/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0576\n",
      "Epoch 120: val_loss improved from 0.02895 to 0.02811, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0279 - mae: 0.0577 - val_loss: 0.0281 - val_mae: 0.0570 - lr: 1.0000e-04\n",
      "Epoch 121/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0265 - mae: 0.0548\n",
      "Epoch 121: val_loss improved from 0.02811 to 0.02721, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0265 - mae: 0.0549 - val_loss: 0.0272 - val_mae: 0.0574 - lr: 1.0000e-04\n",
      "Epoch 122/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0259 - mae: 0.0566\n",
      "Epoch 122: val_loss improved from 0.02721 to 0.02627, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0259 - mae: 0.0567 - val_loss: 0.0263 - val_mae: 0.0570 - lr: 1.0000e-04\n",
      "Epoch 123/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0250 - mae: 0.0556\n",
      "Epoch 123: val_loss improved from 0.02627 to 0.02532, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0250 - mae: 0.0556 - val_loss: 0.0253 - val_mae: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 124/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.0544\n",
      "Epoch 124: val_loss improved from 0.02532 to 0.02440, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0240 - mae: 0.0545 - val_loss: 0.0244 - val_mae: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 125/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0233 - mae: 0.0545\n",
      "Epoch 125: val_loss improved from 0.02440 to 0.02356, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0233 - mae: 0.0544 - val_loss: 0.0236 - val_mae: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 126/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0223 - mae: 0.0535\n",
      "Epoch 126: val_loss improved from 0.02356 to 0.02277, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0223 - mae: 0.0536 - val_loss: 0.0228 - val_mae: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 127/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0216 - mae: 0.0538\n",
      "Epoch 127: val_loss improved from 0.02277 to 0.02204, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0216 - mae: 0.0538 - val_loss: 0.0220 - val_mae: 0.0541 - lr: 1.0000e-04\n",
      "Epoch 128/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0210 - mae: 0.0546\n",
      "Epoch 128: val_loss improved from 0.02204 to 0.02146, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0210 - mae: 0.0546 - val_loss: 0.0215 - val_mae: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 129/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0525\n",
      "Epoch 129: val_loss improved from 0.02146 to 0.02056, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0201 - mae: 0.0525 - val_loss: 0.0206 - val_mae: 0.0530 - lr: 1.0000e-04\n",
      "Epoch 130/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0196 - mae: 0.0534\n",
      "Epoch 130: val_loss improved from 0.02056 to 0.02020, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0196 - mae: 0.0534 - val_loss: 0.0202 - val_mae: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 131/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0189 - mae: 0.0531\n",
      "Epoch 131: val_loss improved from 0.02020 to 0.01904, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0189 - mae: 0.0532 - val_loss: 0.0190 - val_mae: 0.0510 - lr: 1.0000e-04\n",
      "Epoch 132/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0183 - mae: 0.0529\n",
      "Epoch 132: val_loss did not improve from 0.01904\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0183 - mae: 0.0530 - val_loss: 0.0192 - val_mae: 0.0556 - lr: 1.0000e-04\n",
      "Epoch 133/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0181 - mae: 0.0541\n",
      "Epoch 133: val_loss improved from 0.01904 to 0.01827, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0181 - mae: 0.0542 - val_loss: 0.0183 - val_mae: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 134/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0177 - mae: 0.0548\n",
      "Epoch 134: val_loss improved from 0.01827 to 0.01765, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0177 - mae: 0.0548 - val_loss: 0.0177 - val_mae: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 135/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0171 - mae: 0.0547\n",
      "Epoch 135: val_loss did not improve from 0.01765\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0171 - mae: 0.0547 - val_loss: 0.0177 - val_mae: 0.0563 - lr: 1.0000e-04\n",
      "Epoch 136/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0162 - mae: 0.0523\n",
      "Epoch 136: val_loss improved from 0.01765 to 0.01694, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0162 - mae: 0.0522 - val_loss: 0.0169 - val_mae: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 137/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0157 - mae: 0.0514\n",
      "Epoch 137: val_loss improved from 0.01694 to 0.01650, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0157 - mae: 0.0514 - val_loss: 0.0165 - val_mae: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 138/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0153 - mae: 0.0526\n",
      "Epoch 138: val_loss improved from 0.01650 to 0.01546, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0153 - mae: 0.0526 - val_loss: 0.0155 - val_mae: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 139/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0147 - mae: 0.0508\n",
      "Epoch 139: val_loss did not improve from 0.01546\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0147 - mae: 0.0508 - val_loss: 0.0156 - val_mae: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 140/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0143 - mae: 0.0507\n",
      "Epoch 140: val_loss improved from 0.01546 to 0.01506, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0143 - mae: 0.0507 - val_loss: 0.0151 - val_mae: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 141/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0142 - mae: 0.0523\n",
      "Epoch 141: val_loss improved from 0.01506 to 0.01454, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0142 - mae: 0.0522 - val_loss: 0.0145 - val_mae: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 142/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0135 - mae: 0.0509\n",
      "Epoch 142: val_loss improved from 0.01454 to 0.01389, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0135 - mae: 0.0508 - val_loss: 0.0139 - val_mae: 0.0477 - lr: 1.0000e-04\n",
      "Epoch 143/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0515\n",
      "Epoch 143: val_loss did not improve from 0.01389\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0133 - mae: 0.0515 - val_loss: 0.0142 - val_mae: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 144/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0133 - mae: 0.0529\n",
      "Epoch 144: val_loss did not improve from 0.01389\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0133 - mae: 0.0529 - val_loss: 0.0139 - val_mae: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 145/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0526\n",
      "Epoch 145: val_loss improved from 0.01389 to 0.01330, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0130 - mae: 0.0526 - val_loss: 0.0133 - val_mae: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 146/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0123 - mae: 0.0506\n",
      "Epoch 146: val_loss improved from 0.01330 to 0.01262, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0123 - mae: 0.0507 - val_loss: 0.0126 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 147/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0124 - mae: 0.0522\n",
      "Epoch 147: val_loss did not improve from 0.01262\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0124 - mae: 0.0522 - val_loss: 0.0127 - val_mae: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 148/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0117 - mae: 0.0509\n",
      "Epoch 148: val_loss improved from 0.01262 to 0.01247, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0117 - mae: 0.0509 - val_loss: 0.0125 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 149/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0502\n",
      "Epoch 149: val_loss improved from 0.01247 to 0.01203, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0115 - mae: 0.0503 - val_loss: 0.0120 - val_mae: 0.0483 - lr: 1.0000e-04\n",
      "Epoch 150/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0498\n",
      "Epoch 150: val_loss did not improve from 0.01203\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0113 - mae: 0.0498 - val_loss: 0.0124 - val_mae: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 151/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0501\n",
      "Epoch 151: val_loss improved from 0.01203 to 0.01201, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0111 - mae: 0.0501 - val_loss: 0.0120 - val_mae: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 152/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0110 - mae: 0.0500\n",
      "Epoch 152: val_loss improved from 0.01201 to 0.01120, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0110 - mae: 0.0501 - val_loss: 0.0112 - val_mae: 0.0458 - lr: 1.0000e-04\n",
      "Epoch 153/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0105 - mae: 0.0487\n",
      "Epoch 153: val_loss improved from 0.01120 to 0.01120, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0105 - mae: 0.0487 - val_loss: 0.0112 - val_mae: 0.0465 - lr: 1.0000e-04\n",
      "Epoch 154/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0103 - mae: 0.0494\n",
      "Epoch 154: val_loss did not improve from 0.01120\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0103 - mae: 0.0493 - val_loss: 0.0114 - val_mae: 0.0506 - lr: 1.0000e-04\n",
      "Epoch 155/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0490\n",
      "Epoch 155: val_loss improved from 0.01120 to 0.01086, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0101 - mae: 0.0490 - val_loss: 0.0109 - val_mae: 0.0477 - lr: 1.0000e-04\n",
      "Epoch 156/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0490\n",
      "Epoch 156: val_loss improved from 0.01086 to 0.01062, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0100 - mae: 0.0490 - val_loss: 0.0106 - val_mae: 0.0465 - lr: 1.0000e-04\n",
      "Epoch 157/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0503\n",
      "Epoch 157: val_loss improved from 0.01062 to 0.01051, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0100 - mae: 0.0503 - val_loss: 0.0105 - val_mae: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 158/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0491\n",
      "Epoch 158: val_loss did not improve from 0.01051\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0097 - mae: 0.0491 - val_loss: 0.0106 - val_mae: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 159/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0487\n",
      "Epoch 159: val_loss improved from 0.01051 to 0.00994, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0095 - mae: 0.0489 - val_loss: 0.0099 - val_mae: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 160/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0501\n",
      "Epoch 160: val_loss improved from 0.00994 to 0.00979, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0095 - mae: 0.0502 - val_loss: 0.0098 - val_mae: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 161/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0093 - mae: 0.0494\n",
      "Epoch 161: val_loss improved from 0.00979 to 0.00957, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0093 - mae: 0.0494 - val_loss: 0.0096 - val_mae: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 162/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0498\n",
      "Epoch 162: val_loss did not improve from 0.00957\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0092 - mae: 0.0500 - val_loss: 0.0097 - val_mae: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 163/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0494\n",
      "Epoch 163: val_loss improved from 0.00957 to 0.00939, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0090 - mae: 0.0494 - val_loss: 0.0094 - val_mae: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 164/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0487\n",
      "Epoch 164: val_loss did not improve from 0.00939\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0087 - mae: 0.0487 - val_loss: 0.0096 - val_mae: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 165/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0087 - mae: 0.0489\n",
      "Epoch 165: val_loss improved from 0.00939 to 0.00933, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0087 - mae: 0.0489 - val_loss: 0.0093 - val_mae: 0.0451 - lr: 1.0000e-04\n",
      "Epoch 166/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0493\n",
      "Epoch 166: val_loss improved from 0.00933 to 0.00925, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0087 - mae: 0.0493 - val_loss: 0.0092 - val_mae: 0.0463 - lr: 1.0000e-04\n",
      "Epoch 167/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0489\n",
      "Epoch 167: val_loss improved from 0.00925 to 0.00885, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0085 - mae: 0.0489 - val_loss: 0.0088 - val_mae: 0.0441 - lr: 1.0000e-04\n",
      "Epoch 168/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0506\n",
      "Epoch 168: val_loss improved from 0.00885 to 0.00881, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0086 - mae: 0.0505 - val_loss: 0.0088 - val_mae: 0.0444 - lr: 1.0000e-04\n",
      "Epoch 169/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0477\n",
      "Epoch 169: val_loss improved from 0.00881 to 0.00852, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0081 - mae: 0.0478 - val_loss: 0.0085 - val_mae: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 170/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0083 - mae: 0.0493\n",
      "Epoch 170: val_loss did not improve from 0.00852\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0083 - mae: 0.0493 - val_loss: 0.0087 - val_mae: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 171/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0493\n",
      "Epoch 171: val_loss did not improve from 0.00852\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0083 - mae: 0.0496 - val_loss: 0.0088 - val_mae: 0.0462 - lr: 1.0000e-04\n",
      "Epoch 172/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0481\n",
      "Epoch 172: val_loss improved from 0.00852 to 0.00844, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0079 - mae: 0.0481 - val_loss: 0.0084 - val_mae: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 173/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0075 - mae: 0.0462\n",
      "Epoch 173: val_loss improved from 0.00844 to 0.00834, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0075 - mae: 0.0462 - val_loss: 0.0083 - val_mae: 0.0440 - lr: 1.0000e-04\n",
      "Epoch 174/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0480\n",
      "Epoch 174: val_loss improved from 0.00834 to 0.00825, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0078 - mae: 0.0481 - val_loss: 0.0082 - val_mae: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 175/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0482\n",
      "Epoch 175: val_loss improved from 0.00825 to 0.00802, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0078 - mae: 0.0483 - val_loss: 0.0080 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 176/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0077 - mae: 0.0491\n",
      "Epoch 176: val_loss improved from 0.00802 to 0.00797, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0077 - mae: 0.0491 - val_loss: 0.0080 - val_mae: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 177/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0462\n",
      "Epoch 177: val_loss did not improve from 0.00797\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0072 - mae: 0.0462 - val_loss: 0.0083 - val_mae: 0.0453 - lr: 1.0000e-04\n",
      "Epoch 178/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0471\n",
      "Epoch 178: val_loss improved from 0.00797 to 0.00790, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0073 - mae: 0.0473 - val_loss: 0.0079 - val_mae: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 179/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0476\n",
      "Epoch 179: val_loss improved from 0.00790 to 0.00763, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0074 - mae: 0.0476 - val_loss: 0.0076 - val_mae: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 180/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0452\n",
      "Epoch 180: val_loss did not improve from 0.00763\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0069 - mae: 0.0452 - val_loss: 0.0077 - val_mae: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 181/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0474\n",
      "Epoch 181: val_loss did not improve from 0.00763\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0071 - mae: 0.0474 - val_loss: 0.0078 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 182/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0479\n",
      "Epoch 182: val_loss did not improve from 0.00763\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0072 - mae: 0.0479 - val_loss: 0.0077 - val_mae: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 183/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0479\n",
      "Epoch 183: val_loss did not improve from 0.00763\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0072 - mae: 0.0479 - val_loss: 0.0080 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 184/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0475\n",
      "Epoch 184: val_loss improved from 0.00763 to 0.00733, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0070 - mae: 0.0475 - val_loss: 0.0073 - val_mae: 0.0404 - lr: 1.0000e-04\n",
      "Epoch 185/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0485\n",
      "Epoch 185: val_loss did not improve from 0.00733\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0071 - mae: 0.0485 - val_loss: 0.0074 - val_mae: 0.0409 - lr: 1.0000e-04\n",
      "Epoch 186/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0473\n",
      "Epoch 186: val_loss improved from 0.00733 to 0.00727, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0070 - mae: 0.0473 - val_loss: 0.0073 - val_mae: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 187/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0474\n",
      "Epoch 187: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0069 - mae: 0.0475 - val_loss: 0.0075 - val_mae: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 188/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0068 - mae: 0.0469\n",
      "Epoch 188: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0068 - mae: 0.0469 - val_loss: 0.0073 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 189/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0469\n",
      "Epoch 189: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0068 - mae: 0.0469 - val_loss: 0.0076 - val_mae: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 190/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0474\n",
      "Epoch 190: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0067 - mae: 0.0473 - val_loss: 0.0073 - val_mae: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 191/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0450\n",
      "Epoch 191: val_loss improved from 0.00727 to 0.00720, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0064 - mae: 0.0450 - val_loss: 0.0072 - val_mae: 0.0400 - lr: 5.0000e-05\n",
      "Epoch 192/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0456\n",
      "Epoch 192: val_loss improved from 0.00720 to 0.00711, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0064 - mae: 0.0456 - val_loss: 0.0071 - val_mae: 0.0401 - lr: 5.0000e-05\n",
      "Epoch 193/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0480\n",
      "Epoch 193: val_loss improved from 0.00711 to 0.00698, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0068 - mae: 0.0482 - val_loss: 0.0070 - val_mae: 0.0390 - lr: 5.0000e-05\n",
      "Epoch 194/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0471\n",
      "Epoch 194: val_loss improved from 0.00698 to 0.00695, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0066 - mae: 0.0473 - val_loss: 0.0069 - val_mae: 0.0389 - lr: 5.0000e-05\n",
      "Epoch 195/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0456\n",
      "Epoch 195: val_loss did not improve from 0.00695\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0063 - mae: 0.0456 - val_loss: 0.0071 - val_mae: 0.0402 - lr: 5.0000e-05\n",
      "Epoch 196/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0450\n",
      "Epoch 196: val_loss did not improve from 0.00695\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0061 - mae: 0.0451 - val_loss: 0.0070 - val_mae: 0.0403 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0443\n",
      "Epoch 197: val_loss improved from 0.00695 to 0.00667, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0061 - mae: 0.0444 - val_loss: 0.0067 - val_mae: 0.0375 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0446\n",
      "Epoch 198: val_loss did not improve from 0.00667\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0061 - mae: 0.0447 - val_loss: 0.0068 - val_mae: 0.0394 - lr: 5.0000e-05\n",
      "Epoch 199/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0456\n",
      "Epoch 199: val_loss did not improve from 0.00667\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0062 - mae: 0.0456 - val_loss: 0.0068 - val_mae: 0.0382 - lr: 5.0000e-05\n",
      "Epoch 200/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0065 - mae: 0.0472\n",
      "Epoch 200: val_loss improved from 0.00667 to 0.00661, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.0065 - mae: 0.0472 - val_loss: 0.0066 - val_mae: 0.0382 - lr: 5.0000e-05\n",
      "Epoch 201/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0438\n",
      "Epoch 201: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0060 - mae: 0.0438 - val_loss: 0.0067 - val_mae: 0.0391 - lr: 5.0000e-05\n",
      "Epoch 202/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0440\n",
      "Epoch 202: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0059 - mae: 0.0441 - val_loss: 0.0067 - val_mae: 0.0384 - lr: 5.0000e-05\n",
      "Epoch 203/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0452\n",
      "Epoch 203: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0060 - mae: 0.0452 - val_loss: 0.0070 - val_mae: 0.0418 - lr: 5.0000e-05\n",
      "Epoch 204/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0449\n",
      "Epoch 204: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0449 - val_loss: 0.0067 - val_mae: 0.0384 - lr: 2.5000e-05\n",
      "Epoch 205/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0450\n",
      "Epoch 205: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0060 - mae: 0.0450 - val_loss: 0.0066 - val_mae: 0.0381 - lr: 2.5000e-05\n",
      "Epoch 206/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0434\n",
      "Epoch 206: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0057 - mae: 0.0434 - val_loss: 0.0067 - val_mae: 0.0396 - lr: 2.5000e-05\n",
      "Epoch 207/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0058 - mae: 0.0442\n",
      "Epoch 207: val_loss did not improve from 0.00661\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0058 - mae: 0.0442 - val_loss: 0.0066 - val_mae: 0.0395 - lr: 2.5000e-05\n",
      "Epoch 208/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0426\n",
      "Epoch 208: val_loss improved from 0.00661 to 0.00653, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0426 - val_loss: 0.0065 - val_mae: 0.0381 - lr: 2.5000e-05\n",
      "Epoch 209/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0451\n",
      "Epoch 209: val_loss did not improve from 0.00653\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0059 - mae: 0.0451 - val_loss: 0.0066 - val_mae: 0.0392 - lr: 2.5000e-05\n",
      "Epoch 210/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0059 - mae: 0.0449\n",
      "Epoch 210: val_loss improved from 0.00653 to 0.00639, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0059 - mae: 0.0449 - val_loss: 0.0064 - val_mae: 0.0371 - lr: 2.5000e-05\n",
      "Epoch 211/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0441\n",
      "Epoch 211: val_loss did not improve from 0.00639\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0441 - val_loss: 0.0065 - val_mae: 0.0388 - lr: 2.5000e-05\n",
      "Epoch 212/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0435\n",
      "Epoch 212: val_loss did not improve from 0.00639\n",
      "183/183 [==============================] - 3s 17ms/step - loss: 0.0056 - mae: 0.0435 - val_loss: 0.0065 - val_mae: 0.0385 - lr: 2.5000e-05\n",
      "Epoch 213/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0425\n",
      "Epoch 213: val_loss did not improve from 0.00639\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0425 - val_loss: 0.0064 - val_mae: 0.0389 - lr: 2.5000e-05\n",
      "Epoch 214/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0429\n",
      "Epoch 214: val_loss improved from 0.00639 to 0.00634, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0055 - mae: 0.0429 - val_loss: 0.0063 - val_mae: 0.0373 - lr: 2.5000e-05\n",
      "Epoch 215/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0441\n",
      "Epoch 215: val_loss improved from 0.00634 to 0.00631, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0056 - mae: 0.0441 - val_loss: 0.0063 - val_mae: 0.0375 - lr: 2.5000e-05\n",
      "Epoch 216/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0437\n",
      "Epoch 216: val_loss improved from 0.00631 to 0.00629, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0437 - val_loss: 0.0063 - val_mae: 0.0376 - lr: 2.5000e-05\n",
      "Epoch 217/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0426\n",
      "Epoch 217: val_loss improved from 0.00629 to 0.00622, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0054 - mae: 0.0426 - val_loss: 0.0062 - val_mae: 0.0373 - lr: 2.5000e-05\n",
      "Epoch 218/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0437\n",
      "Epoch 218: val_loss improved from 0.00622 to 0.00622, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0437 - val_loss: 0.0062 - val_mae: 0.0381 - lr: 2.5000e-05\n",
      "Epoch 219/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0428\n",
      "Epoch 219: val_loss did not improve from 0.00622\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0055 - mae: 0.0428 - val_loss: 0.0064 - val_mae: 0.0404 - lr: 2.5000e-05\n",
      "Epoch 220/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0429\n",
      "Epoch 220: val_loss did not improve from 0.00622\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0054 - mae: 0.0429 - val_loss: 0.0062 - val_mae: 0.0379 - lr: 2.5000e-05\n",
      "Epoch 221/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0444\n",
      "Epoch 221: val_loss did not improve from 0.00622\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0055 - mae: 0.0445 - val_loss: 0.0062 - val_mae: 0.0382 - lr: 2.5000e-05\n",
      "Epoch 222/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0438\n",
      "Epoch 222: val_loss did not improve from 0.00622\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0438 - val_loss: 0.0063 - val_mae: 0.0386 - lr: 2.5000e-05\n",
      "Epoch 223/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0449\n",
      "Epoch 223: val_loss did not improve from 0.00622\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0057 - mae: 0.0449 - val_loss: 0.0063 - val_mae: 0.0385 - lr: 1.2500e-05\n",
      "Epoch 224/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0436\n",
      "Epoch 224: val_loss improved from 0.00622 to 0.00616, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0435 - val_loss: 0.0062 - val_mae: 0.0374 - lr: 1.2500e-05\n",
      "Epoch 225/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0432\n",
      "Epoch 225: val_loss did not improve from 0.00616\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0054 - mae: 0.0432 - val_loss: 0.0062 - val_mae: 0.0377 - lr: 1.2500e-05\n",
      "Epoch 226/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0434\n",
      "Epoch 226: val_loss did not improve from 0.00616\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0434 - val_loss: 0.0062 - val_mae: 0.0379 - lr: 1.2500e-05\n",
      "Epoch 227/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0443\n",
      "Epoch 227: val_loss improved from 0.00616 to 0.00614, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0444 - val_loss: 0.0061 - val_mae: 0.0379 - lr: 1.2500e-05\n",
      "Epoch 228/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0440\n",
      "Epoch 228: val_loss improved from 0.00614 to 0.00613, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0055 - mae: 0.0441 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 1.2500e-05\n",
      "Epoch 229/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0437\n",
      "Epoch 229: val_loss improved from 0.00613 to 0.00607, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0054 - mae: 0.0437 - val_loss: 0.0061 - val_mae: 0.0377 - lr: 1.2500e-05\n",
      "Epoch 230/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0431\n",
      "Epoch 230: val_loss did not improve from 0.00607\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0431 - val_loss: 0.0061 - val_mae: 0.0379 - lr: 1.2500e-05\n",
      "Epoch 231/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0432\n",
      "Epoch 231: val_loss improved from 0.00607 to 0.00606, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0433 - val_loss: 0.0061 - val_mae: 0.0376 - lr: 6.2500e-06\n",
      "Epoch 232/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0443\n",
      "Epoch 232: val_loss improved from 0.00606 to 0.00604, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0055 - mae: 0.0443 - val_loss: 0.0060 - val_mae: 0.0375 - lr: 6.2500e-06\n",
      "Epoch 233/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0437\n",
      "Epoch 233: val_loss improved from 0.00604 to 0.00604, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0053 - mae: 0.0439 - val_loss: 0.0060 - val_mae: 0.0373 - lr: 6.2500e-06\n",
      "Epoch 234/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0451\n",
      "Epoch 234: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0057 - mae: 0.0451 - val_loss: 0.0061 - val_mae: 0.0376 - lr: 6.2500e-06\n",
      "Epoch 235/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0431\n",
      "Epoch 235: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0431 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 6.2500e-06\n",
      "Epoch 236/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0425\n",
      "Epoch 236: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0427 - val_loss: 0.0061 - val_mae: 0.0375 - lr: 6.2500e-06\n",
      "Epoch 237/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0426\n",
      "Epoch 237: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0377 - lr: 6.2500e-06\n",
      "Epoch 238/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0447\n",
      "Epoch 238: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0448 - val_loss: 0.0061 - val_mae: 0.0377 - lr: 6.2500e-06\n",
      "Epoch 239/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0427\n",
      "Epoch 239: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0427 - val_loss: 0.0060 - val_mae: 0.0375 - lr: 3.1250e-06\n",
      "Epoch 240/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0437\n",
      "Epoch 240: val_loss improved from 0.00604 to 0.00604, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0053 - mae: 0.0437 - val_loss: 0.0060 - val_mae: 0.0376 - lr: 3.1250e-06\n",
      "Epoch 241/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0423\n",
      "Epoch 241: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0424 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 3.1250e-06\n",
      "Epoch 242/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0419\n",
      "Epoch 242: val_loss did not improve from 0.00604\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0419 - val_loss: 0.0061 - val_mae: 0.0377 - lr: 3.1250e-06\n",
      "Epoch 243/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0451\n",
      "Epoch 243: val_loss improved from 0.00604 to 0.00603, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0056 - mae: 0.0451 - val_loss: 0.0060 - val_mae: 0.0375 - lr: 3.1250e-06\n",
      "Epoch 244/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0438\n",
      "Epoch 244: val_loss did not improve from 0.00603\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0053 - mae: 0.0437 - val_loss: 0.0061 - val_mae: 0.0377 - lr: 3.1250e-06\n",
      "Epoch 245/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0415\n",
      "Epoch 245: val_loss did not improve from 0.00603\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0050 - mae: 0.0415 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 1.5625e-06\n",
      "Epoch 246/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0418\n",
      "Epoch 246: val_loss did not improve from 0.00603\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0052 - mae: 0.0419 - val_loss: 0.0061 - val_mae: 0.0375 - lr: 1.5625e-06\n",
      "Epoch 247/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0437\n",
      "Epoch 247: val_loss improved from 0.00603 to 0.00602, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0054 - mae: 0.0438 - val_loss: 0.0060 - val_mae: 0.0373 - lr: 1.5625e-06\n",
      "Epoch 248/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0423\n",
      "Epoch 248: val_loss did not improve from 0.00602\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0052 - mae: 0.0423 - val_loss: 0.0060 - val_mae: 0.0372 - lr: 1.5625e-06\n",
      "Epoch 249/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0432\n",
      "Epoch 249: val_loss improved from 0.00602 to 0.00601, saving model to models\\date_predictor-v25_fold2.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0052 - mae: 0.0432 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.5625e-06\n",
      "Epoch 250/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0051 - mae: 0.0417\n",
      "Epoch 250: val_loss did not improve from 0.00601\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0051 - mae: 0.0417 - val_loss: 0.0061 - val_mae: 0.0373 - lr: 1.5625e-06\n",
      "46/46 [==============================] - 0s 2ms/step\n",
      "Fold 2 MAE: 55.396073508278995\n",
      "\n",
      "--- Training Fold 3/5 ---\n",
      "Epoch 1/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 3.8384 - mae: 1.5048\n",
      "Epoch 1: val_loss improved from inf to 0.49325, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 8s 22ms/step - loss: 3.8371 - mae: 1.5045 - val_loss: 0.4932 - val_mae: 0.5191 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 3.0478 - mae: 1.3373\n",
      "Epoch 2: val_loss improved from 0.49325 to 0.32809, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 3.0490 - mae: 1.3377 - val_loss: 0.3281 - val_mae: 0.3777 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 2.5456 - mae: 1.2071\n",
      "Epoch 3: val_loss improved from 0.32809 to 0.26201, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.5511 - mae: 1.2077 - val_loss: 0.2620 - val_mae: 0.3147 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 2.1103 - mae: 1.0979\n",
      "Epoch 4: val_loss improved from 0.26201 to 0.21570, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 2.1038 - mae: 1.0962 - val_loss: 0.2157 - val_mae: 0.2657 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.8166 - mae: 1.0108\n",
      "Epoch 5: val_loss improved from 0.21570 to 0.19403, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.8175 - mae: 1.0110 - val_loss: 0.1940 - val_mae: 0.2411 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5965 - mae: 0.9503\n",
      "Epoch 6: val_loss improved from 0.19403 to 0.18363, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 1.5965 - mae: 0.9503 - val_loss: 0.1836 - val_mae: 0.2289 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.4032 - mae: 0.8827\n",
      "Epoch 7: val_loss improved from 0.18363 to 0.17492, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 1.4030 - mae: 0.8825 - val_loss: 0.1749 - val_mae: 0.2176 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.2293 - mae: 0.8332\n",
      "Epoch 8: val_loss improved from 0.17492 to 0.16569, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 1.2266 - mae: 0.8323 - val_loss: 0.1657 - val_mae: 0.2063 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.0647 - mae: 0.7638\n",
      "Epoch 9: val_loss improved from 0.16569 to 0.15964, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 1.0647 - mae: 0.7639 - val_loss: 0.1596 - val_mae: 0.1994 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.9537 - mae: 0.7179\n",
      "Epoch 10: val_loss improved from 0.15964 to 0.15520, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.9536 - mae: 0.7178 - val_loss: 0.1552 - val_mae: 0.1928 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.8284 - mae: 0.6644\n",
      "Epoch 11: val_loss improved from 0.15520 to 0.15153, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.8284 - mae: 0.6643 - val_loss: 0.1515 - val_mae: 0.1879 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.7520 - mae: 0.6306\n",
      "Epoch 12: val_loss improved from 0.15153 to 0.14829, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.7526 - mae: 0.6308 - val_loss: 0.1483 - val_mae: 0.1820 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6505 - mae: 0.5774\n",
      "Epoch 13: val_loss improved from 0.14829 to 0.14633, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.6505 - mae: 0.5774 - val_loss: 0.1463 - val_mae: 0.1788 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.6017 - mae: 0.5537\n",
      "Epoch 14: val_loss improved from 0.14633 to 0.14472, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.6016 - mae: 0.5538 - val_loss: 0.1447 - val_mae: 0.1754 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5444 - mae: 0.5204\n",
      "Epoch 15: val_loss improved from 0.14472 to 0.14274, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.5443 - mae: 0.5204 - val_loss: 0.1427 - val_mae: 0.1721 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.4646 - mae: 0.4742\n",
      "Epoch 16: val_loss improved from 0.14274 to 0.14068, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.4649 - mae: 0.4743 - val_loss: 0.1407 - val_mae: 0.1692 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4212 - mae: 0.4444\n",
      "Epoch 17: val_loss improved from 0.14068 to 0.13926, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.4212 - mae: 0.4444 - val_loss: 0.1393 - val_mae: 0.1668 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.3883 - mae: 0.4194\n",
      "Epoch 18: val_loss improved from 0.13926 to 0.13807, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.3878 - mae: 0.4191 - val_loss: 0.1381 - val_mae: 0.1646 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.3469 - mae: 0.3917\n",
      "Epoch 19: val_loss improved from 0.13807 to 0.13781, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.3466 - mae: 0.3916 - val_loss: 0.1378 - val_mae: 0.1641 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3205 - mae: 0.3711\n",
      "Epoch 20: val_loss improved from 0.13781 to 0.13645, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.3207 - mae: 0.3713 - val_loss: 0.1364 - val_mae: 0.1619 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2881 - mae: 0.3416\n",
      "Epoch 21: val_loss improved from 0.13645 to 0.13593, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.2881 - mae: 0.3416 - val_loss: 0.1359 - val_mae: 0.1608 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2697 - mae: 0.3263\n",
      "Epoch 22: val_loss improved from 0.13593 to 0.13558, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.2696 - mae: 0.3262 - val_loss: 0.1356 - val_mae: 0.1601 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2486 - mae: 0.3065\n",
      "Epoch 23: val_loss improved from 0.13558 to 0.13516, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.2486 - mae: 0.3065 - val_loss: 0.1352 - val_mae: 0.1592 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2318 - mae: 0.2903\n",
      "Epoch 24: val_loss improved from 0.13516 to 0.13480, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.2318 - mae: 0.2904 - val_loss: 0.1348 - val_mae: 0.1585 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2106 - mae: 0.2673\n",
      "Epoch 25: val_loss improved from 0.13480 to 0.13423, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.2108 - mae: 0.2674 - val_loss: 0.1342 - val_mae: 0.1574 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2040 - mae: 0.2565\n",
      "Epoch 26: val_loss improved from 0.13423 to 0.13381, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.2040 - mae: 0.2565 - val_loss: 0.1338 - val_mae: 0.1566 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1930 - mae: 0.2454\n",
      "Epoch 27: val_loss improved from 0.13381 to 0.13349, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1930 - mae: 0.2455 - val_loss: 0.1335 - val_mae: 0.1560 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1810 - mae: 0.2321\n",
      "Epoch 28: val_loss improved from 0.13349 to 0.13327, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1811 - mae: 0.2322 - val_loss: 0.1333 - val_mae: 0.1557 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1731 - mae: 0.2211\n",
      "Epoch 29: val_loss improved from 0.13327 to 0.13303, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1729 - mae: 0.2207 - val_loss: 0.1330 - val_mae: 0.1553 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1689 - mae: 0.2150\n",
      "Epoch 30: val_loss did not improve from 0.13303\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1690 - mae: 0.2151 - val_loss: 0.1334 - val_mae: 0.1564 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1618 - mae: 0.2050\n",
      "Epoch 31: val_loss improved from 0.13303 to 0.13250, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.1618 - mae: 0.2050 - val_loss: 0.1325 - val_mae: 0.1542 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1573 - mae: 0.1979\n",
      "Epoch 32: val_loss improved from 0.13250 to 0.13215, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.1572 - mae: 0.1978 - val_loss: 0.1322 - val_mae: 0.1533 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1523 - mae: 0.1897\n",
      "Epoch 33: val_loss improved from 0.13215 to 0.13191, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.1523 - mae: 0.1896 - val_loss: 0.1319 - val_mae: 0.1529 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1510 - mae: 0.1881\n",
      "Epoch 34: val_loss improved from 0.13191 to 0.13178, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1511 - mae: 0.1881 - val_loss: 0.1318 - val_mae: 0.1527 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1469 - mae: 0.1805\n",
      "Epoch 35: val_loss improved from 0.13178 to 0.13147, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1469 - mae: 0.1805 - val_loss: 0.1315 - val_mae: 0.1519 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1452 - mae: 0.1785\n",
      "Epoch 36: val_loss improved from 0.13147 to 0.13140, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.1451 - mae: 0.1785 - val_loss: 0.1314 - val_mae: 0.1520 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1426 - mae: 0.1744\n",
      "Epoch 37: val_loss improved from 0.13140 to 0.13109, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1427 - mae: 0.1744 - val_loss: 0.1311 - val_mae: 0.1511 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1409 - mae: 0.1704\n",
      "Epoch 38: val_loss improved from 0.13109 to 0.13109, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1409 - mae: 0.1704 - val_loss: 0.1311 - val_mae: 0.1512 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1390 - mae: 0.1671\n",
      "Epoch 39: val_loss improved from 0.13109 to 0.13085, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1390 - mae: 0.1671 - val_loss: 0.1309 - val_mae: 0.1507 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1368 - mae: 0.1634\n",
      "Epoch 40: val_loss improved from 0.13085 to 0.13071, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1368 - mae: 0.1635 - val_loss: 0.1307 - val_mae: 0.1503 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1368 - mae: 0.1643\n",
      "Epoch 41: val_loss improved from 0.13071 to 0.13063, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1368 - mae: 0.1643 - val_loss: 0.1306 - val_mae: 0.1505 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1354 - mae: 0.1613\n",
      "Epoch 42: val_loss improved from 0.13063 to 0.13053, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1354 - mae: 0.1613 - val_loss: 0.1305 - val_mae: 0.1503 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1344 - mae: 0.1604\n",
      "Epoch 43: val_loss did not improve from 0.13053\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1344 - mae: 0.1604 - val_loss: 0.1306 - val_mae: 0.1505 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1339 - mae: 0.1600\n",
      "Epoch 44: val_loss improved from 0.13053 to 0.13044, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1339 - mae: 0.1597 - val_loss: 0.1304 - val_mae: 0.1503 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1333 - mae: 0.1583\n",
      "Epoch 45: val_loss improved from 0.13044 to 0.13019, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1333 - mae: 0.1584 - val_loss: 0.1302 - val_mae: 0.1502 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1327 - mae: 0.1583\n",
      "Epoch 46: val_loss improved from 0.13019 to 0.13013, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1327 - mae: 0.1582 - val_loss: 0.1301 - val_mae: 0.1503 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1323 - mae: 0.1571\n",
      "Epoch 47: val_loss improved from 0.13013 to 0.13001, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1323 - mae: 0.1570 - val_loss: 0.1300 - val_mae: 0.1504 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1314 - mae: 0.1551\n",
      "Epoch 48: val_loss improved from 0.13001 to 0.12986, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1314 - mae: 0.1551 - val_loss: 0.1299 - val_mae: 0.1504 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1308 - mae: 0.1552\n",
      "Epoch 49: val_loss improved from 0.12986 to 0.12955, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1308 - mae: 0.1552 - val_loss: 0.1296 - val_mae: 0.1501 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1305 - mae: 0.1545\n",
      "Epoch 50: val_loss did not improve from 0.12955\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1306 - mae: 0.1548 - val_loss: 0.1296 - val_mae: 0.1510 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1305 - mae: 0.1552\n",
      "Epoch 51: val_loss improved from 0.12955 to 0.12919, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1305 - mae: 0.1551 - val_loss: 0.1292 - val_mae: 0.1502 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1296 - mae: 0.1534\n",
      "Epoch 52: val_loss improved from 0.12919 to 0.12909, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1296 - mae: 0.1534 - val_loss: 0.1291 - val_mae: 0.1507 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1293 - mae: 0.1534\n",
      "Epoch 53: val_loss improved from 0.12909 to 0.12882, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1294 - mae: 0.1534 - val_loss: 0.1288 - val_mae: 0.1509 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1287 - mae: 0.1524\n",
      "Epoch 54: val_loss improved from 0.12882 to 0.12851, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1287 - mae: 0.1524 - val_loss: 0.1285 - val_mae: 0.1509 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1285 - mae: 0.1523\n",
      "Epoch 55: val_loss improved from 0.12851 to 0.12823, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1285 - mae: 0.1523 - val_loss: 0.1282 - val_mae: 0.1509 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1277 - mae: 0.1517\n",
      "Epoch 56: val_loss improved from 0.12823 to 0.12773, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1277 - mae: 0.1516 - val_loss: 0.1277 - val_mae: 0.1507 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1271 - mae: 0.1512\n",
      "Epoch 57: val_loss improved from 0.12773 to 0.12766, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1271 - mae: 0.1512 - val_loss: 0.1277 - val_mae: 0.1520 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1265 - mae: 0.1505\n",
      "Epoch 58: val_loss improved from 0.12766 to 0.12653, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1265 - mae: 0.1505 - val_loss: 0.1265 - val_mae: 0.1499 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1262 - mae: 0.1505\n",
      "Epoch 59: val_loss improved from 0.12653 to 0.12608, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1262 - mae: 0.1507 - val_loss: 0.1261 - val_mae: 0.1502 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1253 - mae: 0.1495\n",
      "Epoch 60: val_loss improved from 0.12608 to 0.12533, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1253 - mae: 0.1495 - val_loss: 0.1253 - val_mae: 0.1497 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1246 - mae: 0.1489\n",
      "Epoch 61: val_loss improved from 0.12533 to 0.12425, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1246 - mae: 0.1489 - val_loss: 0.1243 - val_mae: 0.1485 - lr: 1.0000e-04\n",
      "Epoch 62/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1235 - mae: 0.1480\n",
      "Epoch 62: val_loss improved from 0.12425 to 0.12309, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1235 - mae: 0.1480 - val_loss: 0.1231 - val_mae: 0.1477 - lr: 1.0000e-04\n",
      "Epoch 63/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1221 - mae: 0.1455\n",
      "Epoch 63: val_loss improved from 0.12309 to 0.12102, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1221 - mae: 0.1454 - val_loss: 0.1210 - val_mae: 0.1442 - lr: 1.0000e-04\n",
      "Epoch 64/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1207 - mae: 0.1435\n",
      "Epoch 64: val_loss improved from 0.12102 to 0.11917, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1207 - mae: 0.1435 - val_loss: 0.1192 - val_mae: 0.1417 - lr: 1.0000e-04\n",
      "Epoch 65/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1187 - mae: 0.1397\n",
      "Epoch 65: val_loss improved from 0.11917 to 0.11600, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1187 - mae: 0.1396 - val_loss: 0.1160 - val_mae: 0.1350 - lr: 1.0000e-04\n",
      "Epoch 66/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1165 - mae: 0.1352\n",
      "Epoch 66: val_loss improved from 0.11600 to 0.11339, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1165 - mae: 0.1352 - val_loss: 0.1134 - val_mae: 0.1297 - lr: 1.0000e-04\n",
      "Epoch 67/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1134 - mae: 0.1288\n",
      "Epoch 67: val_loss improved from 0.11339 to 0.11115, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1134 - mae: 0.1287 - val_loss: 0.1111 - val_mae: 0.1255 - lr: 1.0000e-04\n",
      "Epoch 68/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1118 - mae: 0.1251\n",
      "Epoch 68: val_loss improved from 0.11115 to 0.10857, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1118 - mae: 0.1251 - val_loss: 0.1086 - val_mae: 0.1192 - lr: 1.0000e-04\n",
      "Epoch 69/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1094 - mae: 0.1204\n",
      "Epoch 69: val_loss improved from 0.10857 to 0.10591, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1094 - mae: 0.1204 - val_loss: 0.1059 - val_mae: 0.1129 - lr: 1.0000e-04\n",
      "Epoch 70/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1065 - mae: 0.1148\n",
      "Epoch 70: val_loss improved from 0.10591 to 0.10296, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1065 - mae: 0.1147 - val_loss: 0.1030 - val_mae: 0.1051 - lr: 1.0000e-04\n",
      "Epoch 71/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1051 - mae: 0.1124\n",
      "Epoch 71: val_loss improved from 0.10296 to 0.10088, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1051 - mae: 0.1124 - val_loss: 0.1009 - val_mae: 0.1004 - lr: 1.0000e-04\n",
      "Epoch 72/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1029 - mae: 0.1089\n",
      "Epoch 72: val_loss improved from 0.10088 to 0.09932, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1029 - mae: 0.1089 - val_loss: 0.0993 - val_mae: 0.0974 - lr: 1.0000e-04\n",
      "Epoch 73/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1002 - mae: 0.1024\n",
      "Epoch 73: val_loss improved from 0.09932 to 0.09749, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1003 - mae: 0.1024 - val_loss: 0.0975 - val_mae: 0.0945 - lr: 1.0000e-04\n",
      "Epoch 74/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0983 - mae: 0.0999\n",
      "Epoch 74: val_loss improved from 0.09749 to 0.09583, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0983 - mae: 0.1000 - val_loss: 0.0958 - val_mae: 0.0913 - lr: 1.0000e-04\n",
      "Epoch 75/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0973 - mae: 0.0990\n",
      "Epoch 75: val_loss improved from 0.09583 to 0.09398, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0973 - mae: 0.0990 - val_loss: 0.0940 - val_mae: 0.0878 - lr: 1.0000e-04\n",
      "Epoch 76/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0953 - mae: 0.0954\n",
      "Epoch 76: val_loss improved from 0.09398 to 0.09275, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0953 - mae: 0.0954 - val_loss: 0.0927 - val_mae: 0.0866 - lr: 1.0000e-04\n",
      "Epoch 77/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0933 - mae: 0.0924\n",
      "Epoch 77: val_loss improved from 0.09275 to 0.09074, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0933 - mae: 0.0925 - val_loss: 0.0907 - val_mae: 0.0823 - lr: 1.0000e-04\n",
      "Epoch 78/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0917 - mae: 0.0907\n",
      "Epoch 78: val_loss improved from 0.09074 to 0.08946, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0917 - mae: 0.0906 - val_loss: 0.0895 - val_mae: 0.0817 - lr: 1.0000e-04\n",
      "Epoch 79/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0905 - mae: 0.0896\n",
      "Epoch 79: val_loss improved from 0.08946 to 0.08781, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0905 - mae: 0.0896 - val_loss: 0.0878 - val_mae: 0.0793 - lr: 1.0000e-04\n",
      "Epoch 80/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0890 - mae: 0.0889\n",
      "Epoch 80: val_loss improved from 0.08781 to 0.08636, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0890 - mae: 0.0889 - val_loss: 0.0864 - val_mae: 0.0782 - lr: 1.0000e-04\n",
      "Epoch 81/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0866 - mae: 0.0842\n",
      "Epoch 81: val_loss improved from 0.08636 to 0.08472, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0867 - mae: 0.0843 - val_loss: 0.0847 - val_mae: 0.0758 - lr: 1.0000e-04\n",
      "Epoch 82/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0856 - mae: 0.0843\n",
      "Epoch 82: val_loss improved from 0.08472 to 0.08367, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0856 - mae: 0.0843 - val_loss: 0.0837 - val_mae: 0.0761 - lr: 1.0000e-04\n",
      "Epoch 83/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0838 - mae: 0.0825\n",
      "Epoch 83: val_loss improved from 0.08367 to 0.08190, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0838 - mae: 0.0825 - val_loss: 0.0819 - val_mae: 0.0737 - lr: 1.0000e-04\n",
      "Epoch 84/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0821 - mae: 0.0813\n",
      "Epoch 84: val_loss improved from 0.08190 to 0.08037, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0821 - mae: 0.0813 - val_loss: 0.0804 - val_mae: 0.0727 - lr: 1.0000e-04\n",
      "Epoch 85/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0803 - mae: 0.0788\n",
      "Epoch 85: val_loss improved from 0.08037 to 0.07859, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0803 - mae: 0.0788 - val_loss: 0.0786 - val_mae: 0.0707 - lr: 1.0000e-04\n",
      "Epoch 86/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0788 - mae: 0.0779\n",
      "Epoch 86: val_loss improved from 0.07859 to 0.07726, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0788 - mae: 0.0780 - val_loss: 0.0773 - val_mae: 0.0708 - lr: 1.0000e-04\n",
      "Epoch 87/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0774 - mae: 0.0779\n",
      "Epoch 87: val_loss improved from 0.07726 to 0.07585, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0774 - mae: 0.0779 - val_loss: 0.0758 - val_mae: 0.0708 - lr: 1.0000e-04\n",
      "Epoch 88/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0753 - mae: 0.0752\n",
      "Epoch 88: val_loss improved from 0.07585 to 0.07399, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0753 - mae: 0.0752 - val_loss: 0.0740 - val_mae: 0.0678 - lr: 1.0000e-04\n",
      "Epoch 89/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0737 - mae: 0.0742\n",
      "Epoch 89: val_loss improved from 0.07399 to 0.07297, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0737 - mae: 0.0742 - val_loss: 0.0730 - val_mae: 0.0713 - lr: 1.0000e-04\n",
      "Epoch 90/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0720 - mae: 0.0736\n",
      "Epoch 90: val_loss improved from 0.07297 to 0.07106, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0720 - mae: 0.0738 - val_loss: 0.0711 - val_mae: 0.0693 - lr: 1.0000e-04\n",
      "Epoch 91/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0701 - mae: 0.0717\n",
      "Epoch 91: val_loss improved from 0.07106 to 0.06908, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0701 - mae: 0.0718 - val_loss: 0.0691 - val_mae: 0.0665 - lr: 1.0000e-04\n",
      "Epoch 92/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0685 - mae: 0.0713\n",
      "Epoch 92: val_loss improved from 0.06908 to 0.06757, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0685 - mae: 0.0712 - val_loss: 0.0676 - val_mae: 0.0666 - lr: 1.0000e-04\n",
      "Epoch 93/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0669 - mae: 0.0709\n",
      "Epoch 93: val_loss improved from 0.06757 to 0.06599, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0669 - mae: 0.0709 - val_loss: 0.0660 - val_mae: 0.0663 - lr: 1.0000e-04\n",
      "Epoch 94/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0650 - mae: 0.0701\n",
      "Epoch 94: val_loss improved from 0.06599 to 0.06402, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0650 - mae: 0.0701 - val_loss: 0.0640 - val_mae: 0.0641 - lr: 1.0000e-04\n",
      "Epoch 95/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0637 - mae: 0.0706\n",
      "Epoch 95: val_loss improved from 0.06402 to 0.06241, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0637 - mae: 0.0706 - val_loss: 0.0624 - val_mae: 0.0637 - lr: 1.0000e-04\n",
      "Epoch 96/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0617 - mae: 0.0685\n",
      "Epoch 96: val_loss improved from 0.06241 to 0.06098, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0617 - mae: 0.0685 - val_loss: 0.0610 - val_mae: 0.0647 - lr: 1.0000e-04\n",
      "Epoch 97/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0600 - mae: 0.0682\n",
      "Epoch 97: val_loss improved from 0.06098 to 0.05923, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0600 - mae: 0.0682 - val_loss: 0.0592 - val_mae: 0.0635 - lr: 1.0000e-04\n",
      "Epoch 98/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0582 - mae: 0.0678\n",
      "Epoch 98: val_loss improved from 0.05923 to 0.05739, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0582 - mae: 0.0678 - val_loss: 0.0574 - val_mae: 0.0622 - lr: 1.0000e-04\n",
      "Epoch 99/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0566 - mae: 0.0675\n",
      "Epoch 99: val_loss improved from 0.05739 to 0.05555, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0566 - mae: 0.0676 - val_loss: 0.0556 - val_mae: 0.0605 - lr: 1.0000e-04\n",
      "Epoch 100/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0545 - mae: 0.0651\n",
      "Epoch 100: val_loss improved from 0.05555 to 0.05422, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0545 - mae: 0.0651 - val_loss: 0.0542 - val_mae: 0.0621 - lr: 1.0000e-04\n",
      "Epoch 101/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0530 - mae: 0.0651\n",
      "Epoch 101: val_loss improved from 0.05422 to 0.05298, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0530 - mae: 0.0651 - val_loss: 0.0530 - val_mae: 0.0635 - lr: 1.0000e-04\n",
      "Epoch 102/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0512 - mae: 0.0642\n",
      "Epoch 102: val_loss improved from 0.05298 to 0.05134, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0512 - mae: 0.0643 - val_loss: 0.0513 - val_mae: 0.0633 - lr: 1.0000e-04\n",
      "Epoch 103/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0498 - mae: 0.0646\n",
      "Epoch 103: val_loss improved from 0.05134 to 0.04948, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0498 - mae: 0.0646 - val_loss: 0.0495 - val_mae: 0.0610 - lr: 1.0000e-04\n",
      "Epoch 104/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0481 - mae: 0.0635\n",
      "Epoch 104: val_loss improved from 0.04948 to 0.04803, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0481 - mae: 0.0635 - val_loss: 0.0480 - val_mae: 0.0612 - lr: 1.0000e-04\n",
      "Epoch 105/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0463 - mae: 0.0615\n",
      "Epoch 105: val_loss improved from 0.04803 to 0.04621, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0463 - mae: 0.0616 - val_loss: 0.0462 - val_mae: 0.0589 - lr: 1.0000e-04\n",
      "Epoch 106/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0447 - mae: 0.0614\n",
      "Epoch 106: val_loss improved from 0.04621 to 0.04492, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0447 - mae: 0.0614 - val_loss: 0.0449 - val_mae: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 107/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0432 - mae: 0.0608\n",
      "Epoch 107: val_loss improved from 0.04492 to 0.04361, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0432 - mae: 0.0608 - val_loss: 0.0436 - val_mae: 0.0610 - lr: 1.0000e-04\n",
      "Epoch 108/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0421 - mae: 0.0629\n",
      "Epoch 108: val_loss improved from 0.04361 to 0.04196, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0421 - mae: 0.0629 - val_loss: 0.0420 - val_mae: 0.0591 - lr: 1.0000e-04\n",
      "Epoch 109/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0405 - mae: 0.0620\n",
      "Epoch 109: val_loss improved from 0.04196 to 0.04049, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0405 - mae: 0.0620 - val_loss: 0.0405 - val_mae: 0.0592 - lr: 1.0000e-04\n",
      "Epoch 110/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0389 - mae: 0.0605\n",
      "Epoch 110: val_loss improved from 0.04049 to 0.03898, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0389 - mae: 0.0604 - val_loss: 0.0390 - val_mae: 0.0580 - lr: 1.0000e-04\n",
      "Epoch 111/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0376 - mae: 0.0607\n",
      "Epoch 111: val_loss improved from 0.03898 to 0.03795, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0376 - mae: 0.0608 - val_loss: 0.0380 - val_mae: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 112/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0361 - mae: 0.0598\n",
      "Epoch 112: val_loss improved from 0.03795 to 0.03604, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0361 - mae: 0.0598 - val_loss: 0.0360 - val_mae: 0.0550 - lr: 1.0000e-04\n",
      "Epoch 113/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0348 - mae: 0.0590\n",
      "Epoch 113: val_loss improved from 0.03604 to 0.03494, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0348 - mae: 0.0590 - val_loss: 0.0349 - val_mae: 0.0561 - lr: 1.0000e-04\n",
      "Epoch 114/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0335 - mae: 0.0582\n",
      "Epoch 114: val_loss improved from 0.03494 to 0.03365, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0335 - mae: 0.0583 - val_loss: 0.0337 - val_mae: 0.0554 - lr: 1.0000e-04\n",
      "Epoch 115/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0323 - mae: 0.0578\n",
      "Epoch 115: val_loss improved from 0.03365 to 0.03246, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0323 - mae: 0.0578 - val_loss: 0.0325 - val_mae: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 116/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0313 - mae: 0.0583\n",
      "Epoch 116: val_loss improved from 0.03246 to 0.03155, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0313 - mae: 0.0584 - val_loss: 0.0316 - val_mae: 0.0557 - lr: 1.0000e-04\n",
      "Epoch 117/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0301 - mae: 0.0579\n",
      "Epoch 117: val_loss improved from 0.03155 to 0.03047, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0301 - mae: 0.0580 - val_loss: 0.0305 - val_mae: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 118/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0290 - mae: 0.0570\n",
      "Epoch 118: val_loss improved from 0.03047 to 0.02923, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0290 - mae: 0.0570 - val_loss: 0.0292 - val_mae: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 119/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0560\n",
      "Epoch 119: val_loss improved from 0.02923 to 0.02869, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0279 - mae: 0.0560 - val_loss: 0.0287 - val_mae: 0.0567 - lr: 1.0000e-04\n",
      "Epoch 120/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0270 - mae: 0.0566\n",
      "Epoch 120: val_loss improved from 0.02869 to 0.02753, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0270 - mae: 0.0566 - val_loss: 0.0275 - val_mae: 0.0553 - lr: 1.0000e-04\n",
      "Epoch 121/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0260 - mae: 0.0556\n",
      "Epoch 121: val_loss improved from 0.02753 to 0.02644, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0260 - mae: 0.0557 - val_loss: 0.0264 - val_mae: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 122/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0250 - mae: 0.0549\n",
      "Epoch 122: val_loss improved from 0.02644 to 0.02628, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0250 - mae: 0.0550 - val_loss: 0.0263 - val_mae: 0.0577 - lr: 1.0000e-04\n",
      "Epoch 123/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0244 - mae: 0.0557\n",
      "Epoch 123: val_loss improved from 0.02628 to 0.02502, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0244 - mae: 0.0557 - val_loss: 0.0250 - val_mae: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 124/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0233 - mae: 0.0543\n",
      "Epoch 124: val_loss improved from 0.02502 to 0.02382, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0233 - mae: 0.0543 - val_loss: 0.0238 - val_mae: 0.0515 - lr: 1.0000e-04\n",
      "Epoch 125/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0536\n",
      "Epoch 125: val_loss improved from 0.02382 to 0.02314, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0225 - mae: 0.0536 - val_loss: 0.0231 - val_mae: 0.0515 - lr: 1.0000e-04\n",
      "Epoch 126/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0217 - mae: 0.0533\n",
      "Epoch 126: val_loss improved from 0.02314 to 0.02265, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0216 - mae: 0.0533 - val_loss: 0.0226 - val_mae: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 127/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0209 - mae: 0.0533\n",
      "Epoch 127: val_loss improved from 0.02265 to 0.02169, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0209 - mae: 0.0532 - val_loss: 0.0217 - val_mae: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 128/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0201 - mae: 0.0528\n",
      "Epoch 128: val_loss improved from 0.02169 to 0.02114, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0201 - mae: 0.0527 - val_loss: 0.0211 - val_mae: 0.0518 - lr: 1.0000e-04\n",
      "Epoch 129/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0197 - mae: 0.0538\n",
      "Epoch 129: val_loss improved from 0.02114 to 0.02057, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0197 - mae: 0.0538 - val_loss: 0.0206 - val_mae: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 130/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0192 - mae: 0.0541\n",
      "Epoch 130: val_loss improved from 0.02057 to 0.01989, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0192 - mae: 0.0541 - val_loss: 0.0199 - val_mae: 0.0523 - lr: 1.0000e-04\n",
      "Epoch 131/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0182 - mae: 0.0520\n",
      "Epoch 131: val_loss improved from 0.01989 to 0.01933, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0182 - mae: 0.0520 - val_loss: 0.0193 - val_mae: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 132/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0178 - mae: 0.0529\n",
      "Epoch 132: val_loss improved from 0.01933 to 0.01867, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0178 - mae: 0.0528 - val_loss: 0.0187 - val_mae: 0.0510 - lr: 1.0000e-04\n",
      "Epoch 133/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0174 - mae: 0.0535\n",
      "Epoch 133: val_loss improved from 0.01867 to 0.01799, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0174 - mae: 0.0535 - val_loss: 0.0180 - val_mae: 0.0509 - lr: 1.0000e-04\n",
      "Epoch 134/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0164 - mae: 0.0507\n",
      "Epoch 134: val_loss improved from 0.01799 to 0.01735, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0164 - mae: 0.0507 - val_loss: 0.0173 - val_mae: 0.0486 - lr: 1.0000e-04\n",
      "Epoch 135/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0164 - mae: 0.0527\n",
      "Epoch 135: val_loss did not improve from 0.01735\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0164 - mae: 0.0527 - val_loss: 0.0176 - val_mae: 0.0530 - lr: 1.0000e-04\n",
      "Epoch 136/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0158 - mae: 0.0526\n",
      "Epoch 136: val_loss improved from 0.01735 to 0.01665, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0158 - mae: 0.0528 - val_loss: 0.0166 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 137/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0154 - mae: 0.0525\n",
      "Epoch 137: val_loss improved from 0.01665 to 0.01601, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0154 - mae: 0.0526 - val_loss: 0.0160 - val_mae: 0.0483 - lr: 1.0000e-04\n",
      "Epoch 138/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0147 - mae: 0.0512\n",
      "Epoch 138: val_loss improved from 0.01601 to 0.01596, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0147 - mae: 0.0513 - val_loss: 0.0160 - val_mae: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 139/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0144 - mae: 0.0516\n",
      "Epoch 139: val_loss improved from 0.01596 to 0.01553, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0144 - mae: 0.0516 - val_loss: 0.0155 - val_mae: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 140/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0140 - mae: 0.0520\n",
      "Epoch 140: val_loss improved from 0.01553 to 0.01486, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0140 - mae: 0.0521 - val_loss: 0.0149 - val_mae: 0.0484 - lr: 1.0000e-04\n",
      "Epoch 141/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0136 - mae: 0.0506\n",
      "Epoch 141: val_loss improved from 0.01486 to 0.01454, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0136 - mae: 0.0506 - val_loss: 0.0145 - val_mae: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 142/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0133 - mae: 0.0508\n",
      "Epoch 142: val_loss improved from 0.01454 to 0.01451, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0133 - mae: 0.0507 - val_loss: 0.0145 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 143/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0511\n",
      "Epoch 143: val_loss improved from 0.01451 to 0.01396, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0130 - mae: 0.0511 - val_loss: 0.0140 - val_mae: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 144/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0126 - mae: 0.0505\n",
      "Epoch 144: val_loss improved from 0.01396 to 0.01368, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0126 - mae: 0.0506 - val_loss: 0.0137 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 145/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0502\n",
      "Epoch 145: val_loss improved from 0.01368 to 0.01320, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0123 - mae: 0.0502 - val_loss: 0.0132 - val_mae: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 146/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0119 - mae: 0.0495\n",
      "Epoch 146: val_loss improved from 0.01320 to 0.01281, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0119 - mae: 0.0495 - val_loss: 0.0128 - val_mae: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 147/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0118 - mae: 0.0502\n",
      "Epoch 147: val_loss did not improve from 0.01281\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0118 - mae: 0.0502 - val_loss: 0.0128 - val_mae: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 148/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0115 - mae: 0.0507\n",
      "Epoch 148: val_loss improved from 0.01281 to 0.01227, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0115 - mae: 0.0507 - val_loss: 0.0123 - val_mae: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 149/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0111 - mae: 0.0499\n",
      "Epoch 149: val_loss improved from 0.01227 to 0.01220, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0111 - mae: 0.0499 - val_loss: 0.0122 - val_mae: 0.0459 - lr: 1.0000e-04\n",
      "Epoch 150/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0108 - mae: 0.0488\n",
      "Epoch 150: val_loss improved from 0.01220 to 0.01211, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0108 - mae: 0.0489 - val_loss: 0.0121 - val_mae: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 151/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0506\n",
      "Epoch 151: val_loss did not improve from 0.01211\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0107 - mae: 0.0506 - val_loss: 0.0122 - val_mae: 0.0496 - lr: 1.0000e-04\n",
      "Epoch 152/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0490\n",
      "Epoch 152: val_loss improved from 0.01211 to 0.01164, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0104 - mae: 0.0490 - val_loss: 0.0116 - val_mae: 0.0468 - lr: 1.0000e-04\n",
      "Epoch 153/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0498\n",
      "Epoch 153: val_loss improved from 0.01164 to 0.01143, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0104 - mae: 0.0498 - val_loss: 0.0114 - val_mae: 0.0456 - lr: 1.0000e-04\n",
      "Epoch 154/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0101 - mae: 0.0500\n",
      "Epoch 154: val_loss improved from 0.01143 to 0.01124, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0101 - mae: 0.0499 - val_loss: 0.0112 - val_mae: 0.0465 - lr: 1.0000e-04\n",
      "Epoch 155/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0498\n",
      "Epoch 155: val_loss improved from 0.01124 to 0.01109, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0100 - mae: 0.0498 - val_loss: 0.0111 - val_mae: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 156/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0095 - mae: 0.0480\n",
      "Epoch 156: val_loss improved from 0.01109 to 0.01089, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0095 - mae: 0.0480 - val_loss: 0.0109 - val_mae: 0.0463 - lr: 1.0000e-04\n",
      "Epoch 157/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0479\n",
      "Epoch 157: val_loss improved from 0.01089 to 0.01076, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0094 - mae: 0.0479 - val_loss: 0.0108 - val_mae: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 158/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0094 - mae: 0.0492\n",
      "Epoch 158: val_loss improved from 0.01076 to 0.01044, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0094 - mae: 0.0493 - val_loss: 0.0104 - val_mae: 0.0453 - lr: 1.0000e-04\n",
      "Epoch 159/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0092 - mae: 0.0488\n",
      "Epoch 159: val_loss did not improve from 0.01044\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0092 - mae: 0.0488 - val_loss: 0.0108 - val_mae: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 160/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0090 - mae: 0.0489\n",
      "Epoch 160: val_loss improved from 0.01044 to 0.01014, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0090 - mae: 0.0489 - val_loss: 0.0101 - val_mae: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 161/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0089 - mae: 0.0494\n",
      "Epoch 161: val_loss did not improve from 0.01014\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0089 - mae: 0.0494 - val_loss: 0.0104 - val_mae: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 162/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0481\n",
      "Epoch 162: val_loss improved from 0.01014 to 0.01011, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0086 - mae: 0.0482 - val_loss: 0.0101 - val_mae: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 163/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0486\n",
      "Epoch 163: val_loss improved from 0.01011 to 0.01005, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0086 - mae: 0.0486 - val_loss: 0.0101 - val_mae: 0.0460 - lr: 1.0000e-04\n",
      "Epoch 164/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0475\n",
      "Epoch 164: val_loss improved from 0.01005 to 0.00950, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0083 - mae: 0.0476 - val_loss: 0.0095 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 165/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0080 - mae: 0.0465\n",
      "Epoch 165: val_loss improved from 0.00950 to 0.00949, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0080 - mae: 0.0465 - val_loss: 0.0095 - val_mae: 0.0444 - lr: 1.0000e-04\n",
      "Epoch 166/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0481\n",
      "Epoch 166: val_loss did not improve from 0.00949\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0082 - mae: 0.0481 - val_loss: 0.0100 - val_mae: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 167/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0079 - mae: 0.0471\n",
      "Epoch 167: val_loss improved from 0.00949 to 0.00931, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0079 - mae: 0.0471 - val_loss: 0.0093 - val_mae: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 168/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0498\n",
      "Epoch 168: val_loss improved from 0.00931 to 0.00917, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0083 - mae: 0.0498 - val_loss: 0.0092 - val_mae: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 169/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0507\n",
      "Epoch 169: val_loss did not improve from 0.00917\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0083 - mae: 0.0511 - val_loss: 0.0096 - val_mae: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 170/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0488\n",
      "Epoch 170: val_loss improved from 0.00917 to 0.00902, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0079 - mae: 0.0488 - val_loss: 0.0090 - val_mae: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 171/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0473\n",
      "Epoch 171: val_loss improved from 0.00902 to 0.00889, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0076 - mae: 0.0473 - val_loss: 0.0089 - val_mae: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 172/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0078 - mae: 0.0485\n",
      "Epoch 172: val_loss improved from 0.00889 to 0.00889, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0078 - mae: 0.0485 - val_loss: 0.0089 - val_mae: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 173/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0476\n",
      "Epoch 173: val_loss improved from 0.00889 to 0.00888, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0075 - mae: 0.0477 - val_loss: 0.0089 - val_mae: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 174/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0481\n",
      "Epoch 174: val_loss improved from 0.00888 to 0.00878, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0075 - mae: 0.0481 - val_loss: 0.0088 - val_mae: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 175/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0457\n",
      "Epoch 175: val_loss improved from 0.00878 to 0.00877, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0071 - mae: 0.0457 - val_loss: 0.0088 - val_mae: 0.0444 - lr: 1.0000e-04\n",
      "Epoch 176/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0473\n",
      "Epoch 176: val_loss improved from 0.00877 to 0.00872, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0072 - mae: 0.0473 - val_loss: 0.0087 - val_mae: 0.0432 - lr: 1.0000e-04\n",
      "Epoch 177/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0459\n",
      "Epoch 177: val_loss improved from 0.00872 to 0.00865, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0069 - mae: 0.0461 - val_loss: 0.0087 - val_mae: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 178/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0474\n",
      "Epoch 178: val_loss improved from 0.00865 to 0.00823, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0071 - mae: 0.0474 - val_loss: 0.0082 - val_mae: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 179/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0497\n",
      "Epoch 179: val_loss did not improve from 0.00823\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0075 - mae: 0.0499 - val_loss: 0.0084 - val_mae: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 180/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0479\n",
      "Epoch 180: val_loss did not improve from 0.00823\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0071 - mae: 0.0479 - val_loss: 0.0084 - val_mae: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 181/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0482\n",
      "Epoch 181: val_loss did not improve from 0.00823\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0072 - mae: 0.0483 - val_loss: 0.0083 - val_mae: 0.0421 - lr: 1.0000e-04\n",
      "Epoch 182/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0459\n",
      "Epoch 182: val_loss improved from 0.00823 to 0.00810, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0068 - mae: 0.0460 - val_loss: 0.0081 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 183/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0499\n",
      "Epoch 183: val_loss improved from 0.00810 to 0.00810, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0073 - mae: 0.0500 - val_loss: 0.0081 - val_mae: 0.0416 - lr: 1.0000e-04\n",
      "Epoch 184/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0462\n",
      "Epoch 184: val_loss improved from 0.00810 to 0.00809, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0067 - mae: 0.0462 - val_loss: 0.0081 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 185/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0067 - mae: 0.0473\n",
      "Epoch 185: val_loss did not improve from 0.00809\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0067 - mae: 0.0473 - val_loss: 0.0084 - val_mae: 0.0445 - lr: 1.0000e-04\n",
      "Epoch 186/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0484\n",
      "Epoch 186: val_loss did not improve from 0.00809\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0069 - mae: 0.0484 - val_loss: 0.0081 - val_mae: 0.0409 - lr: 1.0000e-04\n",
      "Epoch 187/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0461\n",
      "Epoch 187: val_loss improved from 0.00809 to 0.00801, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0065 - mae: 0.0461 - val_loss: 0.0080 - val_mae: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 188/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0481\n",
      "Epoch 188: val_loss did not improve from 0.00801\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0069 - mae: 0.0481 - val_loss: 0.0081 - val_mae: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 189/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0482\n",
      "Epoch 189: val_loss improved from 0.00801 to 0.00788, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0067 - mae: 0.0483 - val_loss: 0.0079 - val_mae: 0.0399 - lr: 5.0000e-05\n",
      "Epoch 190/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0066 - mae: 0.0467\n",
      "Epoch 190: val_loss improved from 0.00788 to 0.00784, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0066 - mae: 0.0467 - val_loss: 0.0078 - val_mae: 0.0406 - lr: 5.0000e-05\n",
      "Epoch 191/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0453\n",
      "Epoch 191: val_loss improved from 0.00784 to 0.00769, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0063 - mae: 0.0453 - val_loss: 0.0077 - val_mae: 0.0397 - lr: 5.0000e-05\n",
      "Epoch 192/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0453\n",
      "Epoch 192: val_loss did not improve from 0.00769\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0062 - mae: 0.0453 - val_loss: 0.0077 - val_mae: 0.0404 - lr: 5.0000e-05\n",
      "Epoch 193/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0455\n",
      "Epoch 193: val_loss did not improve from 0.00769\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0062 - mae: 0.0455 - val_loss: 0.0077 - val_mae: 0.0407 - lr: 5.0000e-05\n",
      "Epoch 194/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0460\n",
      "Epoch 194: val_loss improved from 0.00769 to 0.00767, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0062 - mae: 0.0460 - val_loss: 0.0077 - val_mae: 0.0395 - lr: 5.0000e-05\n",
      "Epoch 195/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0472\n",
      "Epoch 195: val_loss did not improve from 0.00767\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0063 - mae: 0.0472 - val_loss: 0.0078 - val_mae: 0.0422 - lr: 5.0000e-05\n",
      "Epoch 196/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0447\n",
      "Epoch 196: val_loss improved from 0.00767 to 0.00750, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0060 - mae: 0.0447 - val_loss: 0.0075 - val_mae: 0.0400 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0451\n",
      "Epoch 197: val_loss did not improve from 0.00750\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0452 - val_loss: 0.0075 - val_mae: 0.0399 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0442\n",
      "Epoch 198: val_loss improved from 0.00750 to 0.00750, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0058 - mae: 0.0442 - val_loss: 0.0075 - val_mae: 0.0402 - lr: 5.0000e-05\n",
      "Epoch 199/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0466\n",
      "Epoch 199: val_loss improved from 0.00750 to 0.00748, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0061 - mae: 0.0465 - val_loss: 0.0075 - val_mae: 0.0404 - lr: 5.0000e-05\n",
      "Epoch 200/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0455\n",
      "Epoch 200: val_loss improved from 0.00748 to 0.00740, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0060 - mae: 0.0455 - val_loss: 0.0074 - val_mae: 0.0395 - lr: 5.0000e-05\n",
      "Epoch 201/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0466\n",
      "Epoch 201: val_loss did not improve from 0.00740\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0062 - mae: 0.0466 - val_loss: 0.0075 - val_mae: 0.0400 - lr: 5.0000e-05\n",
      "Epoch 202/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0437\n",
      "Epoch 202: val_loss did not improve from 0.00740\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0437 - val_loss: 0.0075 - val_mae: 0.0419 - lr: 5.0000e-05\n",
      "Epoch 203/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0439\n",
      "Epoch 203: val_loss did not improve from 0.00740\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0439 - val_loss: 0.0074 - val_mae: 0.0406 - lr: 5.0000e-05\n",
      "Epoch 204/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0445\n",
      "Epoch 204: val_loss did not improve from 0.00740\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0058 - mae: 0.0447 - val_loss: 0.0075 - val_mae: 0.0405 - lr: 5.0000e-05\n",
      "Epoch 205/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0444\n",
      "Epoch 205: val_loss improved from 0.00740 to 0.00732, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0057 - mae: 0.0445 - val_loss: 0.0073 - val_mae: 0.0392 - lr: 5.0000e-05\n",
      "Epoch 206/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0450\n",
      "Epoch 206: val_loss improved from 0.00732 to 0.00718, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0057 - mae: 0.0450 - val_loss: 0.0072 - val_mae: 0.0385 - lr: 5.0000e-05\n",
      "Epoch 207/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0441\n",
      "Epoch 207: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0055 - mae: 0.0441 - val_loss: 0.0072 - val_mae: 0.0402 - lr: 5.0000e-05\n",
      "Epoch 208/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0443\n",
      "Epoch 208: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0443 - val_loss: 0.0073 - val_mae: 0.0400 - lr: 5.0000e-05\n",
      "Epoch 209/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0450\n",
      "Epoch 209: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0057 - mae: 0.0452 - val_loss: 0.0073 - val_mae: 0.0405 - lr: 5.0000e-05\n",
      "Epoch 210/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0439\n",
      "Epoch 210: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 6s 30ms/step - loss: 0.0055 - mae: 0.0439 - val_loss: 0.0073 - val_mae: 0.0420 - lr: 5.0000e-05\n",
      "Epoch 211/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0450\n",
      "Epoch 211: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.0056 - mae: 0.0450 - val_loss: 0.0073 - val_mae: 0.0398 - lr: 5.0000e-05\n",
      "Epoch 212/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0447\n",
      "Epoch 212: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.0056 - mae: 0.0447 - val_loss: 0.0073 - val_mae: 0.0394 - lr: 5.0000e-05\n",
      "Epoch 213/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0439\n",
      "Epoch 213: val_loss did not improve from 0.00718\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.0054 - mae: 0.0440 - val_loss: 0.0072 - val_mae: 0.0400 - lr: 2.5000e-05\n",
      "Epoch 214/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0451\n",
      "Epoch 214: val_loss improved from 0.00718 to 0.00711, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 30ms/step - loss: 0.0055 - mae: 0.0451 - val_loss: 0.0071 - val_mae: 0.0390 - lr: 2.5000e-05\n",
      "Epoch 215/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0413\n",
      "Epoch 215: val_loss did not improve from 0.00711\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0050 - mae: 0.0413 - val_loss: 0.0071 - val_mae: 0.0398 - lr: 2.5000e-05\n",
      "Epoch 216/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0430\n",
      "Epoch 216: val_loss did not improve from 0.00711\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0071 - val_mae: 0.0397 - lr: 2.5000e-05\n",
      "Epoch 217/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0430\n",
      "Epoch 217: val_loss improved from 0.00711 to 0.00706, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0071 - val_mae: 0.0390 - lr: 2.5000e-05\n",
      "Epoch 218/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0430\n",
      "Epoch 218: val_loss improved from 0.00706 to 0.00705, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0052 - mae: 0.0431 - val_loss: 0.0070 - val_mae: 0.0390 - lr: 2.5000e-05\n",
      "Epoch 219/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0441\n",
      "Epoch 219: val_loss improved from 0.00705 to 0.00694, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0054 - mae: 0.0440 - val_loss: 0.0069 - val_mae: 0.0383 - lr: 2.5000e-05\n",
      "Epoch 220/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0427\n",
      "Epoch 220: val_loss improved from 0.00694 to 0.00693, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0052 - mae: 0.0427 - val_loss: 0.0069 - val_mae: 0.0388 - lr: 2.5000e-05\n",
      "Epoch 221/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0442\n",
      "Epoch 221: val_loss did not improve from 0.00693\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0053 - mae: 0.0442 - val_loss: 0.0070 - val_mae: 0.0392 - lr: 2.5000e-05\n",
      "Epoch 222/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0439\n",
      "Epoch 222: val_loss improved from 0.00693 to 0.00689, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0440 - val_loss: 0.0069 - val_mae: 0.0383 - lr: 2.5000e-05\n",
      "Epoch 223/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0427\n",
      "Epoch 223: val_loss did not improve from 0.00689\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0051 - mae: 0.0429 - val_loss: 0.0070 - val_mae: 0.0399 - lr: 2.5000e-05\n",
      "Epoch 224/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0449\n",
      "Epoch 224: val_loss did not improve from 0.00689\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0449 - val_loss: 0.0070 - val_mae: 0.0396 - lr: 2.5000e-05\n",
      "Epoch 225/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0435\n",
      "Epoch 225: val_loss did not improve from 0.00689\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0435 - val_loss: 0.0069 - val_mae: 0.0393 - lr: 2.5000e-05\n",
      "Epoch 226/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0435\n",
      "Epoch 226: val_loss improved from 0.00689 to 0.00688, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0435 - val_loss: 0.0069 - val_mae: 0.0394 - lr: 1.2500e-05\n",
      "Epoch 227/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0432\n",
      "Epoch 227: val_loss improved from 0.00688 to 0.00684, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0050 - mae: 0.0432 - val_loss: 0.0068 - val_mae: 0.0387 - lr: 1.2500e-05\n",
      "Epoch 228/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0051 - mae: 0.0429\n",
      "Epoch 228: val_loss did not improve from 0.00684\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0051 - mae: 0.0429 - val_loss: 0.0069 - val_mae: 0.0394 - lr: 1.2500e-05\n",
      "Epoch 229/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0049 - mae: 0.0416\n",
      "Epoch 229: val_loss did not improve from 0.00684\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0049 - mae: 0.0416 - val_loss: 0.0069 - val_mae: 0.0385 - lr: 1.2500e-05\n",
      "Epoch 230/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0421\n",
      "Epoch 230: val_loss did not improve from 0.00684\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0421 - val_loss: 0.0069 - val_mae: 0.0389 - lr: 1.2500e-05\n",
      "Epoch 231/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0443\n",
      "Epoch 231: val_loss did not improve from 0.00684\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0446 - val_loss: 0.0069 - val_mae: 0.0394 - lr: 1.2500e-05\n",
      "Epoch 232/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0433\n",
      "Epoch 232: val_loss did not improve from 0.00684\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0433 - val_loss: 0.0069 - val_mae: 0.0390 - lr: 6.2500e-06\n",
      "Epoch 233/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0440\n",
      "Epoch 233: val_loss improved from 0.00684 to 0.00684, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0053 - mae: 0.0440 - val_loss: 0.0068 - val_mae: 0.0388 - lr: 6.2500e-06\n",
      "Epoch 234/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0434\n",
      "Epoch 234: val_loss improved from 0.00684 to 0.00683, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0050 - mae: 0.0434 - val_loss: 0.0068 - val_mae: 0.0385 - lr: 6.2500e-06\n",
      "Epoch 235/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0426\n",
      "Epoch 235: val_loss did not improve from 0.00683\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0050 - mae: 0.0425 - val_loss: 0.0068 - val_mae: 0.0387 - lr: 6.2500e-06\n",
      "Epoch 236/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0049 - mae: 0.0416\n",
      "Epoch 236: val_loss improved from 0.00683 to 0.00681, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0049 - mae: 0.0416 - val_loss: 0.0068 - val_mae: 0.0385 - lr: 6.2500e-06\n",
      "Epoch 237/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0434\n",
      "Epoch 237: val_loss did not improve from 0.00681\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0050 - mae: 0.0435 - val_loss: 0.0068 - val_mae: 0.0388 - lr: 6.2500e-06\n",
      "Epoch 238/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0449\n",
      "Epoch 238: val_loss did not improve from 0.00681\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0052 - mae: 0.0449 - val_loss: 0.0068 - val_mae: 0.0388 - lr: 6.2500e-06\n",
      "Epoch 239/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0436\n",
      "Epoch 239: val_loss improved from 0.00681 to 0.00679, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0051 - mae: 0.0436 - val_loss: 0.0068 - val_mae: 0.0386 - lr: 6.2500e-06\n",
      "Epoch 240/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0051 - mae: 0.0435\n",
      "Epoch 240: val_loss improved from 0.00679 to 0.00678, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0051 - mae: 0.0435 - val_loss: 0.0068 - val_mae: 0.0382 - lr: 6.2500e-06\n",
      "Epoch 241/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0049 - mae: 0.0422\n",
      "Epoch 241: val_loss did not improve from 0.00678\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0049 - mae: 0.0422 - val_loss: 0.0068 - val_mae: 0.0384 - lr: 3.1250e-06\n",
      "Epoch 242/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0047 - mae: 0.0414\n",
      "Epoch 242: val_loss did not improve from 0.00678\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0048 - mae: 0.0415 - val_loss: 0.0068 - val_mae: 0.0387 - lr: 3.1250e-06\n",
      "Epoch 243/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0418\n",
      "Epoch 243: val_loss did not improve from 0.00678\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0048 - mae: 0.0419 - val_loss: 0.0068 - val_mae: 0.0387 - lr: 3.1250e-06\n",
      "Epoch 244/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0419\n",
      "Epoch 244: val_loss improved from 0.00678 to 0.00676, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0048 - mae: 0.0419 - val_loss: 0.0068 - val_mae: 0.0384 - lr: 3.1250e-06\n",
      "Epoch 245/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0437\n",
      "Epoch 245: val_loss improved from 0.00676 to 0.00675, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0438 - val_loss: 0.0068 - val_mae: 0.0382 - lr: 3.1250e-06\n",
      "Epoch 246/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0417\n",
      "Epoch 246: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0048 - mae: 0.0418 - val_loss: 0.0068 - val_mae: 0.0385 - lr: 3.1250e-06\n",
      "Epoch 247/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0449\n",
      "Epoch 247: val_loss improved from 0.00675 to 0.00674, saving model to models\\date_predictor-v25_fold3.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0449 - val_loss: 0.0067 - val_mae: 0.0382 - lr: 1.5625e-06\n",
      "Epoch 248/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0433\n",
      "Epoch 248: val_loss did not improve from 0.00674\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0434 - val_loss: 0.0068 - val_mae: 0.0384 - lr: 1.5625e-06\n",
      "Epoch 249/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0047 - mae: 0.0411\n",
      "Epoch 249: val_loss did not improve from 0.00674\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0047 - mae: 0.0411 - val_loss: 0.0068 - val_mae: 0.0388 - lr: 1.5625e-06\n",
      "Epoch 250/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0051 - mae: 0.0442\n",
      "Epoch 250: val_loss did not improve from 0.00674\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0051 - mae: 0.0442 - val_loss: 0.0067 - val_mae: 0.0383 - lr: 1.5625e-06\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "Fold 3 MAE: 56.618182159402885\n",
      "\n",
      "--- Training Fold 4/5 ---\n",
      "Epoch 1/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 3.6383 - mae: 1.4738\n",
      "Epoch 1: val_loss improved from inf to 0.66478, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 6s 23ms/step - loss: 3.6383 - mae: 1.4738 - val_loss: 0.6648 - val_mae: 0.6484 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.9468 - mae: 1.3246\n",
      "Epoch 2: val_loss improved from 0.66478 to 0.57608, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 2.9522 - mae: 1.3255 - val_loss: 0.5761 - val_mae: 0.6034 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.3545 - mae: 1.1801\n",
      "Epoch 3: val_loss improved from 0.57608 to 0.43131, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 2.3528 - mae: 1.1794 - val_loss: 0.4313 - val_mae: 0.4973 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9381 - mae: 1.0562\n",
      "Epoch 4: val_loss improved from 0.43131 to 0.35937, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.9381 - mae: 1.0562 - val_loss: 0.3594 - val_mae: 0.4370 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.6726 - mae: 0.9814\n",
      "Epoch 5: val_loss improved from 0.35937 to 0.30666, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 1.6735 - mae: 0.9818 - val_loss: 0.3067 - val_mae: 0.3868 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.4763 - mae: 0.9141\n",
      "Epoch 6: val_loss improved from 0.30666 to 0.25562, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.4773 - mae: 0.9144 - val_loss: 0.2556 - val_mae: 0.3291 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.2538 - mae: 0.8379\n",
      "Epoch 7: val_loss improved from 0.25562 to 0.23553, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.2534 - mae: 0.8378 - val_loss: 0.2355 - val_mae: 0.3068 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.0638 - mae: 0.7642\n",
      "Epoch 8: val_loss improved from 0.23553 to 0.21756, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.0638 - mae: 0.7642 - val_loss: 0.2176 - val_mae: 0.2844 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.9373 - mae: 0.7129\n",
      "Epoch 9: val_loss improved from 0.21756 to 0.20206, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.9378 - mae: 0.7130 - val_loss: 0.2021 - val_mae: 0.2632 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.8654 - mae: 0.6832\n",
      "Epoch 10: val_loss improved from 0.20206 to 0.18332, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.8656 - mae: 0.6834 - val_loss: 0.1833 - val_mae: 0.2358 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7627 - mae: 0.6361\n",
      "Epoch 11: val_loss improved from 0.18332 to 0.17434, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.7627 - mae: 0.6361 - val_loss: 0.1743 - val_mae: 0.2228 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.6505 - mae: 0.5835\n",
      "Epoch 12: val_loss improved from 0.17434 to 0.17135, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.6502 - mae: 0.5834 - val_loss: 0.1714 - val_mae: 0.2189 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6002 - mae: 0.5551\n",
      "Epoch 13: val_loss improved from 0.17135 to 0.16679, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.6002 - mae: 0.5551 - val_loss: 0.1668 - val_mae: 0.2118 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.5155 - mae: 0.5076\n",
      "Epoch 14: val_loss improved from 0.16679 to 0.16088, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.5160 - mae: 0.5079 - val_loss: 0.1609 - val_mae: 0.2026 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.4633 - mae: 0.4722\n",
      "Epoch 15: val_loss improved from 0.16088 to 0.15796, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.4635 - mae: 0.4723 - val_loss: 0.1580 - val_mae: 0.1980 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.4275 - mae: 0.4526\n",
      "Epoch 16: val_loss did not improve from 0.15796\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.4276 - mae: 0.4527 - val_loss: 0.1588 - val_mae: 0.1997 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3877 - mae: 0.4221\n",
      "Epoch 17: val_loss improved from 0.15796 to 0.15421, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.3876 - mae: 0.4221 - val_loss: 0.1542 - val_mae: 0.1925 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.3433 - mae: 0.3898\n",
      "Epoch 18: val_loss improved from 0.15421 to 0.15121, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.3425 - mae: 0.3891 - val_loss: 0.1512 - val_mae: 0.1876 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3110 - mae: 0.3610\n",
      "Epoch 19: val_loss improved from 0.15121 to 0.14883, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.3110 - mae: 0.3610 - val_loss: 0.1488 - val_mae: 0.1844 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2863 - mae: 0.3447\n",
      "Epoch 20: val_loss improved from 0.14883 to 0.14763, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.2863 - mae: 0.3447 - val_loss: 0.1476 - val_mae: 0.1827 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2697 - mae: 0.3279\n",
      "Epoch 21: val_loss improved from 0.14763 to 0.14562, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.2697 - mae: 0.3279 - val_loss: 0.1456 - val_mae: 0.1795 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2399 - mae: 0.2977\n",
      "Epoch 22: val_loss improved from 0.14562 to 0.14268, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.2398 - mae: 0.2977 - val_loss: 0.1427 - val_mae: 0.1750 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2232 - mae: 0.2792\n",
      "Epoch 23: val_loss improved from 0.14268 to 0.14092, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.2233 - mae: 0.2793 - val_loss: 0.1409 - val_mae: 0.1720 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.2098 - mae: 0.2652\n",
      "Epoch 24: val_loss improved from 0.14092 to 0.13970, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.2097 - mae: 0.2651 - val_loss: 0.1397 - val_mae: 0.1702 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2003 - mae: 0.2549\n",
      "Epoch 25: val_loss improved from 0.13970 to 0.13873, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.2000 - mae: 0.2546 - val_loss: 0.1387 - val_mae: 0.1687 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1870 - mae: 0.2385\n",
      "Epoch 26: val_loss improved from 0.13873 to 0.13814, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1870 - mae: 0.2384 - val_loss: 0.1381 - val_mae: 0.1678 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1781 - mae: 0.2267\n",
      "Epoch 27: val_loss improved from 0.13814 to 0.13702, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1780 - mae: 0.2266 - val_loss: 0.1370 - val_mae: 0.1658 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1737 - mae: 0.2213\n",
      "Epoch 28: val_loss improved from 0.13702 to 0.13624, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1737 - mae: 0.2213 - val_loss: 0.1362 - val_mae: 0.1645 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1659 - mae: 0.2105\n",
      "Epoch 29: val_loss improved from 0.13624 to 0.13514, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1659 - mae: 0.2105 - val_loss: 0.1351 - val_mae: 0.1625 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1612 - mae: 0.2036\n",
      "Epoch 30: val_loss improved from 0.13514 to 0.13432, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1612 - mae: 0.2036 - val_loss: 0.1343 - val_mae: 0.1610 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1553 - mae: 0.1943\n",
      "Epoch 31: val_loss improved from 0.13432 to 0.13353, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1554 - mae: 0.1944 - val_loss: 0.1335 - val_mae: 0.1596 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1514 - mae: 0.1878\n",
      "Epoch 32: val_loss improved from 0.13353 to 0.13248, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1514 - mae: 0.1878 - val_loss: 0.1325 - val_mae: 0.1575 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1479 - mae: 0.1826\n",
      "Epoch 33: val_loss improved from 0.13248 to 0.13196, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1479 - mae: 0.1824 - val_loss: 0.1320 - val_mae: 0.1567 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1451 - mae: 0.1773\n",
      "Epoch 34: val_loss improved from 0.13196 to 0.13148, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1450 - mae: 0.1771 - val_loss: 0.1315 - val_mae: 0.1557 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1436 - mae: 0.1756\n",
      "Epoch 35: val_loss improved from 0.13148 to 0.13076, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1436 - mae: 0.1756 - val_loss: 0.1308 - val_mae: 0.1544 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1409 - mae: 0.1718\n",
      "Epoch 36: val_loss improved from 0.13076 to 0.13054, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1409 - mae: 0.1718 - val_loss: 0.1305 - val_mae: 0.1541 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1392 - mae: 0.1681\n",
      "Epoch 37: val_loss improved from 0.13054 to 0.12994, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1392 - mae: 0.1681 - val_loss: 0.1299 - val_mae: 0.1527 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1384 - mae: 0.1676\n",
      "Epoch 38: val_loss improved from 0.12994 to 0.12947, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1385 - mae: 0.1678 - val_loss: 0.1295 - val_mae: 0.1516 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1361 - mae: 0.1623\n",
      "Epoch 39: val_loss improved from 0.12947 to 0.12903, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1361 - mae: 0.1623 - val_loss: 0.1290 - val_mae: 0.1505 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1350 - mae: 0.1620\n",
      "Epoch 40: val_loss improved from 0.12903 to 0.12875, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1350 - mae: 0.1620 - val_loss: 0.1288 - val_mae: 0.1498 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1342 - mae: 0.1606\n",
      "Epoch 41: val_loss improved from 0.12875 to 0.12853, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1342 - mae: 0.1606 - val_loss: 0.1285 - val_mae: 0.1495 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1329 - mae: 0.1579\n",
      "Epoch 42: val_loss improved from 0.12853 to 0.12833, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1329 - mae: 0.1580 - val_loss: 0.1283 - val_mae: 0.1490 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1327 - mae: 0.1569\n",
      "Epoch 43: val_loss improved from 0.12833 to 0.12821, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1327 - mae: 0.1568 - val_loss: 0.1282 - val_mae: 0.1491 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1314 - mae: 0.1549\n",
      "Epoch 44: val_loss improved from 0.12821 to 0.12806, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1314 - mae: 0.1549 - val_loss: 0.1281 - val_mae: 0.1489 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1314 - mae: 0.1554\n",
      "Epoch 45: val_loss improved from 0.12806 to 0.12786, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1314 - mae: 0.1554 - val_loss: 0.1279 - val_mae: 0.1485 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1310 - mae: 0.1546\n",
      "Epoch 46: val_loss improved from 0.12786 to 0.12765, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1310 - mae: 0.1546 - val_loss: 0.1276 - val_mae: 0.1484 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1302 - mae: 0.1536\n",
      "Epoch 47: val_loss improved from 0.12765 to 0.12749, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1302 - mae: 0.1536 - val_loss: 0.1275 - val_mae: 0.1485 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1295 - mae: 0.1524\n",
      "Epoch 48: val_loss improved from 0.12749 to 0.12729, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1295 - mae: 0.1523 - val_loss: 0.1273 - val_mae: 0.1481 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1289 - mae: 0.1516\n",
      "Epoch 49: val_loss improved from 0.12729 to 0.12700, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1289 - mae: 0.1516 - val_loss: 0.1270 - val_mae: 0.1478 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1285 - mae: 0.1516\n",
      "Epoch 50: val_loss improved from 0.12700 to 0.12679, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1285 - mae: 0.1516 - val_loss: 0.1268 - val_mae: 0.1477 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1283 - mae: 0.1516\n",
      "Epoch 51: val_loss improved from 0.12679 to 0.12647, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1283 - mae: 0.1516 - val_loss: 0.1265 - val_mae: 0.1476 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1276 - mae: 0.1503\n",
      "Epoch 52: val_loss improved from 0.12647 to 0.12618, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1276 - mae: 0.1505 - val_loss: 0.1262 - val_mae: 0.1474 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1273 - mae: 0.1507\n",
      "Epoch 53: val_loss improved from 0.12618 to 0.12592, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1273 - mae: 0.1505 - val_loss: 0.1259 - val_mae: 0.1475 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1271 - mae: 0.1505\n",
      "Epoch 54: val_loss improved from 0.12592 to 0.12549, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1271 - mae: 0.1505 - val_loss: 0.1255 - val_mae: 0.1472 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1264 - mae: 0.1498\n",
      "Epoch 55: val_loss improved from 0.12549 to 0.12515, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1264 - mae: 0.1498 - val_loss: 0.1252 - val_mae: 0.1473 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1259 - mae: 0.1488\n",
      "Epoch 56: val_loss improved from 0.12515 to 0.12488, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1259 - mae: 0.1488 - val_loss: 0.1249 - val_mae: 0.1476 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1254 - mae: 0.1495\n",
      "Epoch 57: val_loss improved from 0.12488 to 0.12404, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1254 - mae: 0.1495 - val_loss: 0.1240 - val_mae: 0.1466 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1247 - mae: 0.1486\n",
      "Epoch 58: val_loss improved from 0.12404 to 0.12321, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1247 - mae: 0.1486 - val_loss: 0.1232 - val_mae: 0.1457 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1241 - mae: 0.1478\n",
      "Epoch 59: val_loss improved from 0.12321 to 0.12253, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1240 - mae: 0.1475 - val_loss: 0.1225 - val_mae: 0.1452 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1228 - mae: 0.1459\n",
      "Epoch 60: val_loss improved from 0.12253 to 0.12160, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1228 - mae: 0.1459 - val_loss: 0.1216 - val_mae: 0.1444 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1216 - mae: 0.1442\n",
      "Epoch 61: val_loss improved from 0.12160 to 0.12017, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1216 - mae: 0.1442 - val_loss: 0.1202 - val_mae: 0.1424 - lr: 1.0000e-04\n",
      "Epoch 62/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1203 - mae: 0.1425\n",
      "Epoch 62: val_loss improved from 0.12017 to 0.11805, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1203 - mae: 0.1425 - val_loss: 0.1181 - val_mae: 0.1386 - lr: 1.0000e-04\n",
      "Epoch 63/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1182 - mae: 0.1385\n",
      "Epoch 63: val_loss improved from 0.11805 to 0.11608, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1182 - mae: 0.1385 - val_loss: 0.1161 - val_mae: 0.1351 - lr: 1.0000e-04\n",
      "Epoch 64/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1157 - mae: 0.1334\n",
      "Epoch 64: val_loss improved from 0.11608 to 0.11348, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1157 - mae: 0.1334 - val_loss: 0.1135 - val_mae: 0.1297 - lr: 1.0000e-04\n",
      "Epoch 65/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1143 - mae: 0.1306\n",
      "Epoch 65: val_loss improved from 0.11348 to 0.11086, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1143 - mae: 0.1306 - val_loss: 0.1109 - val_mae: 0.1237 - lr: 1.0000e-04\n",
      "Epoch 66/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1120 - mae: 0.1263\n",
      "Epoch 66: val_loss improved from 0.11086 to 0.10843, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1120 - mae: 0.1263 - val_loss: 0.1084 - val_mae: 0.1181 - lr: 1.0000e-04\n",
      "Epoch 67/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1093 - mae: 0.1208\n",
      "Epoch 67: val_loss improved from 0.10843 to 0.10564, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1093 - mae: 0.1208 - val_loss: 0.1056 - val_mae: 0.1110 - lr: 1.0000e-04\n",
      "Epoch 68/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1073 - mae: 0.1159\n",
      "Epoch 68: val_loss improved from 0.10564 to 0.10323, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1073 - mae: 0.1159 - val_loss: 0.1032 - val_mae: 0.1052 - lr: 1.0000e-04\n",
      "Epoch 69/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1047 - mae: 0.1114\n",
      "Epoch 69: val_loss improved from 0.10323 to 0.10079, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1048 - mae: 0.1114 - val_loss: 0.1008 - val_mae: 0.0987 - lr: 1.0000e-04\n",
      "Epoch 70/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1031 - mae: 0.1088\n",
      "Epoch 70: val_loss improved from 0.10079 to 0.09932, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1031 - mae: 0.1088 - val_loss: 0.0993 - val_mae: 0.0962 - lr: 1.0000e-04\n",
      "Epoch 71/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1008 - mae: 0.1037\n",
      "Epoch 71: val_loss improved from 0.09932 to 0.09785, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1008 - mae: 0.1037 - val_loss: 0.0978 - val_mae: 0.0937 - lr: 1.0000e-04\n",
      "Epoch 72/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0994 - mae: 0.1018\n",
      "Epoch 72: val_loss improved from 0.09785 to 0.09594, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0994 - mae: 0.1018 - val_loss: 0.0959 - val_mae: 0.0897 - lr: 1.0000e-04\n",
      "Epoch 73/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0977 - mae: 0.0997\n",
      "Epoch 73: val_loss improved from 0.09594 to 0.09430, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0977 - mae: 0.0997 - val_loss: 0.0943 - val_mae: 0.0865 - lr: 1.0000e-04\n",
      "Epoch 74/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0959 - mae: 0.0972\n",
      "Epoch 74: val_loss improved from 0.09430 to 0.09288, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.0959 - mae: 0.0972 - val_loss: 0.0929 - val_mae: 0.0847 - lr: 1.0000e-04\n",
      "Epoch 75/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0943 - mae: 0.0937\n",
      "Epoch 75: val_loss improved from 0.09288 to 0.09149, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0943 - mae: 0.0937 - val_loss: 0.0915 - val_mae: 0.0829 - lr: 1.0000e-04\n",
      "Epoch 76/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0925 - mae: 0.0915\n",
      "Epoch 76: val_loss improved from 0.09149 to 0.08994, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0925 - mae: 0.0915 - val_loss: 0.0899 - val_mae: 0.0807 - lr: 1.0000e-04\n",
      "Epoch 77/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0907 - mae: 0.0897\n",
      "Epoch 77: val_loss improved from 0.08994 to 0.08829, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0907 - mae: 0.0897 - val_loss: 0.0883 - val_mae: 0.0782 - lr: 1.0000e-04\n",
      "Epoch 78/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0894 - mae: 0.0882\n",
      "Epoch 78: val_loss improved from 0.08829 to 0.08685, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0894 - mae: 0.0882 - val_loss: 0.0868 - val_mae: 0.0769 - lr: 1.0000e-04\n",
      "Epoch 79/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0876 - mae: 0.0855\n",
      "Epoch 79: val_loss improved from 0.08685 to 0.08524, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0876 - mae: 0.0855 - val_loss: 0.0852 - val_mae: 0.0748 - lr: 1.0000e-04\n",
      "Epoch 80/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0858 - mae: 0.0834\n",
      "Epoch 80: val_loss improved from 0.08524 to 0.08362, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0858 - mae: 0.0834 - val_loss: 0.0836 - val_mae: 0.0725 - lr: 1.0000e-04\n",
      "Epoch 81/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0846 - mae: 0.0829\n",
      "Epoch 81: val_loss improved from 0.08362 to 0.08245, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0846 - mae: 0.0830 - val_loss: 0.0824 - val_mae: 0.0731 - lr: 1.0000e-04\n",
      "Epoch 82/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0830 - mae: 0.0813\n",
      "Epoch 82: val_loss improved from 0.08245 to 0.08059, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0830 - mae: 0.0813 - val_loss: 0.0806 - val_mae: 0.0703 - lr: 1.0000e-04\n",
      "Epoch 83/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0814 - mae: 0.0801\n",
      "Epoch 83: val_loss improved from 0.08059 to 0.07914, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0814 - mae: 0.0801 - val_loss: 0.0791 - val_mae: 0.0696 - lr: 1.0000e-04\n",
      "Epoch 84/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0801 - mae: 0.0793\n",
      "Epoch 84: val_loss improved from 0.07914 to 0.07839, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0801 - mae: 0.0793 - val_loss: 0.0784 - val_mae: 0.0721 - lr: 1.0000e-04\n",
      "Epoch 85/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0781 - mae: 0.0773\n",
      "Epoch 85: val_loss improved from 0.07839 to 0.07630, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0781 - mae: 0.0773 - val_loss: 0.0763 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 86/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0768 - mae: 0.0773\n",
      "Epoch 86: val_loss improved from 0.07630 to 0.07479, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0768 - mae: 0.0773 - val_loss: 0.0748 - val_mae: 0.0680 - lr: 1.0000e-04\n",
      "Epoch 87/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0748 - mae: 0.0751\n",
      "Epoch 87: val_loss improved from 0.07479 to 0.07339, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0748 - mae: 0.0752 - val_loss: 0.0734 - val_mae: 0.0684 - lr: 1.0000e-04\n",
      "Epoch 88/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.0742\n",
      "Epoch 88: val_loss improved from 0.07339 to 0.07190, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.0732 - mae: 0.0742 - val_loss: 0.0719 - val_mae: 0.0678 - lr: 1.0000e-04\n",
      "Epoch 89/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0713 - mae: 0.0727\n",
      "Epoch 89: val_loss improved from 0.07190 to 0.07038, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0713 - mae: 0.0726 - val_loss: 0.0704 - val_mae: 0.0672 - lr: 1.0000e-04\n",
      "Epoch 90/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0700 - mae: 0.0731\n",
      "Epoch 90: val_loss improved from 0.07038 to 0.06837, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0700 - mae: 0.0731 - val_loss: 0.0684 - val_mae: 0.0648 - lr: 1.0000e-04\n",
      "Epoch 91/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0679 - mae: 0.0702\n",
      "Epoch 91: val_loss improved from 0.06837 to 0.06694, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0678 - mae: 0.0701 - val_loss: 0.0669 - val_mae: 0.0653 - lr: 1.0000e-04\n",
      "Epoch 92/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0659 - mae: 0.0684\n",
      "Epoch 92: val_loss improved from 0.06694 to 0.06533, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0659 - mae: 0.0684 - val_loss: 0.0653 - val_mae: 0.0649 - lr: 1.0000e-04\n",
      "Epoch 93/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0644 - mae: 0.0689\n",
      "Epoch 93: val_loss improved from 0.06533 to 0.06396, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0644 - mae: 0.0689 - val_loss: 0.0640 - val_mae: 0.0662 - lr: 1.0000e-04\n",
      "Epoch 94/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0630 - mae: 0.0691\n",
      "Epoch 94: val_loss improved from 0.06396 to 0.06179, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0630 - mae: 0.0691 - val_loss: 0.0618 - val_mae: 0.0623 - lr: 1.0000e-04\n",
      "Epoch 95/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0611 - mae: 0.0682\n",
      "Epoch 95: val_loss improved from 0.06179 to 0.06001, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0611 - mae: 0.0682 - val_loss: 0.0600 - val_mae: 0.0616 - lr: 1.0000e-04\n",
      "Epoch 96/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0592 - mae: 0.0666\n",
      "Epoch 96: val_loss improved from 0.06001 to 0.05860, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0592 - mae: 0.0666 - val_loss: 0.0586 - val_mae: 0.0625 - lr: 1.0000e-04\n",
      "Epoch 97/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0575 - mae: 0.0665\n",
      "Epoch 97: val_loss improved from 0.05860 to 0.05690, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0575 - mae: 0.0664 - val_loss: 0.0569 - val_mae: 0.0622 - lr: 1.0000e-04\n",
      "Epoch 98/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0556 - mae: 0.0655\n",
      "Epoch 98: val_loss improved from 0.05690 to 0.05549, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0556 - mae: 0.0655 - val_loss: 0.0555 - val_mae: 0.0633 - lr: 1.0000e-04\n",
      "Epoch 99/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0539 - mae: 0.0644\n",
      "Epoch 99: val_loss improved from 0.05549 to 0.05350, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0539 - mae: 0.0643 - val_loss: 0.0535 - val_mae: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 100/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0523 - mae: 0.0640\n",
      "Epoch 100: val_loss improved from 0.05350 to 0.05173, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0523 - mae: 0.0640 - val_loss: 0.0517 - val_mae: 0.0600 - lr: 1.0000e-04\n",
      "Epoch 101/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0506 - mae: 0.0643\n",
      "Epoch 101: val_loss improved from 0.05173 to 0.05017, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0507 - mae: 0.0643 - val_loss: 0.0502 - val_mae: 0.0604 - lr: 1.0000e-04\n",
      "Epoch 102/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0488 - mae: 0.0629\n",
      "Epoch 102: val_loss improved from 0.05017 to 0.04856, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0488 - mae: 0.0630 - val_loss: 0.0486 - val_mae: 0.0598 - lr: 1.0000e-04\n",
      "Epoch 103/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0475 - mae: 0.0639\n",
      "Epoch 103: val_loss improved from 0.04856 to 0.04687, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0475 - mae: 0.0639 - val_loss: 0.0469 - val_mae: 0.0590 - lr: 1.0000e-04\n",
      "Epoch 104/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0456 - mae: 0.0620\n",
      "Epoch 104: val_loss improved from 0.04687 to 0.04566, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0456 - mae: 0.0620 - val_loss: 0.0457 - val_mae: 0.0609 - lr: 1.0000e-04\n",
      "Epoch 105/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0444 - mae: 0.0635\n",
      "Epoch 105: val_loss improved from 0.04566 to 0.04407, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0444 - mae: 0.0635 - val_loss: 0.0441 - val_mae: 0.0600 - lr: 1.0000e-04\n",
      "Epoch 106/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0422 - mae: 0.0593\n",
      "Epoch 106: val_loss improved from 0.04407 to 0.04250, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0422 - mae: 0.0593 - val_loss: 0.0425 - val_mae: 0.0592 - lr: 1.0000e-04\n",
      "Epoch 107/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0410 - mae: 0.0606\n",
      "Epoch 107: val_loss improved from 0.04250 to 0.04062, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0410 - mae: 0.0607 - val_loss: 0.0406 - val_mae: 0.0564 - lr: 1.0000e-04\n",
      "Epoch 108/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0394 - mae: 0.0593\n",
      "Epoch 108: val_loss improved from 0.04062 to 0.03954, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0394 - mae: 0.0593 - val_loss: 0.0395 - val_mae: 0.0580 - lr: 1.0000e-04\n",
      "Epoch 109/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0383 - mae: 0.0601\n",
      "Epoch 109: val_loss improved from 0.03954 to 0.03827, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0383 - mae: 0.0601 - val_loss: 0.0383 - val_mae: 0.0586 - lr: 1.0000e-04\n",
      "Epoch 110/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0363 - mae: 0.0569\n",
      "Epoch 110: val_loss improved from 0.03827 to 0.03669, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0363 - mae: 0.0569 - val_loss: 0.0367 - val_mae: 0.0570 - lr: 1.0000e-04\n",
      "Epoch 111/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0353 - mae: 0.0579\n",
      "Epoch 111: val_loss improved from 0.03669 to 0.03532, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0353 - mae: 0.0579 - val_loss: 0.0353 - val_mae: 0.0558 - lr: 1.0000e-04\n",
      "Epoch 112/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0342 - mae: 0.0587\n",
      "Epoch 112: val_loss improved from 0.03532 to 0.03422, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0342 - mae: 0.0587 - val_loss: 0.0342 - val_mae: 0.0563 - lr: 1.0000e-04\n",
      "Epoch 113/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0325 - mae: 0.0561\n",
      "Epoch 113: val_loss improved from 0.03422 to 0.03286, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0325 - mae: 0.0561 - val_loss: 0.0329 - val_mae: 0.0552 - lr: 1.0000e-04\n",
      "Epoch 114/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0317 - mae: 0.0577\n",
      "Epoch 114: val_loss improved from 0.03286 to 0.03153, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0317 - mae: 0.0577 - val_loss: 0.0315 - val_mae: 0.0541 - lr: 1.0000e-04\n",
      "Epoch 115/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0303 - mae: 0.0561\n",
      "Epoch 115: val_loss improved from 0.03153 to 0.03068, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0303 - mae: 0.0561 - val_loss: 0.0307 - val_mae: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 116/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0293 - mae: 0.0568\n",
      "Epoch 116: val_loss improved from 0.03068 to 0.02945, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0293 - mae: 0.0568 - val_loss: 0.0294 - val_mae: 0.0545 - lr: 1.0000e-04\n",
      "Epoch 117/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0282 - mae: 0.0564\n",
      "Epoch 117: val_loss improved from 0.02945 to 0.02822, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0282 - mae: 0.0564 - val_loss: 0.0282 - val_mae: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 118/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0273 - mae: 0.0570\n",
      "Epoch 118: val_loss improved from 0.02822 to 0.02731, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0273 - mae: 0.0570 - val_loss: 0.0273 - val_mae: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 119/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0263 - mae: 0.0561\n",
      "Epoch 119: val_loss improved from 0.02731 to 0.02645, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0263 - mae: 0.0561 - val_loss: 0.0265 - val_mae: 0.0537 - lr: 1.0000e-04\n",
      "Epoch 120/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0253 - mae: 0.0558\n",
      "Epoch 120: val_loss improved from 0.02645 to 0.02614, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0253 - mae: 0.0558 - val_loss: 0.0261 - val_mae: 0.0576 - lr: 1.0000e-04\n",
      "Epoch 121/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0240 - mae: 0.0534\n",
      "Epoch 121: val_loss improved from 0.02614 to 0.02460, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 25ms/step - loss: 0.0240 - mae: 0.0533 - val_loss: 0.0246 - val_mae: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 122/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0235 - mae: 0.0547\n",
      "Epoch 122: val_loss improved from 0.02460 to 0.02356, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0235 - mae: 0.0547 - val_loss: 0.0236 - val_mae: 0.0506 - lr: 1.0000e-04\n",
      "Epoch 123/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0226 - mae: 0.0538\n",
      "Epoch 123: val_loss improved from 0.02356 to 0.02294, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0226 - mae: 0.0537 - val_loss: 0.0229 - val_mae: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 124/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0219 - mae: 0.0544\n",
      "Epoch 124: val_loss improved from 0.02294 to 0.02254, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0219 - mae: 0.0544 - val_loss: 0.0225 - val_mae: 0.0539 - lr: 1.0000e-04\n",
      "Epoch 125/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0211 - mae: 0.0538\n",
      "Epoch 125: val_loss improved from 0.02254 to 0.02166, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0211 - mae: 0.0538 - val_loss: 0.0217 - val_mae: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 126/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0203 - mae: 0.0530\n",
      "Epoch 126: val_loss improved from 0.02166 to 0.02102, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0203 - mae: 0.0530 - val_loss: 0.0210 - val_mae: 0.0527 - lr: 1.0000e-04\n",
      "Epoch 127/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0196 - mae: 0.0529\n",
      "Epoch 127: val_loss improved from 0.02102 to 0.02040, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0196 - mae: 0.0528 - val_loss: 0.0204 - val_mae: 0.0529 - lr: 1.0000e-04\n",
      "Epoch 128/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0191 - mae: 0.0533\n",
      "Epoch 128: val_loss improved from 0.02040 to 0.01977, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0191 - mae: 0.0533 - val_loss: 0.0198 - val_mae: 0.0530 - lr: 1.0000e-04\n",
      "Epoch 129/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0183 - mae: 0.0519\n",
      "Epoch 129: val_loss improved from 0.01977 to 0.01926, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0183 - mae: 0.0519 - val_loss: 0.0193 - val_mae: 0.0525 - lr: 1.0000e-04\n",
      "Epoch 130/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0175 - mae: 0.0510\n",
      "Epoch 130: val_loss improved from 0.01926 to 0.01851, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0175 - mae: 0.0510 - val_loss: 0.0185 - val_mae: 0.0517 - lr: 1.0000e-04\n",
      "Epoch 131/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0172 - mae: 0.0519\n",
      "Epoch 131: val_loss improved from 0.01851 to 0.01789, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0172 - mae: 0.0521 - val_loss: 0.0179 - val_mae: 0.0507 - lr: 1.0000e-04\n",
      "Epoch 132/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0166 - mae: 0.0519\n",
      "Epoch 132: val_loss improved from 0.01789 to 0.01772, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0166 - mae: 0.0519 - val_loss: 0.0177 - val_mae: 0.0524 - lr: 1.0000e-04\n",
      "Epoch 133/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0162 - mae: 0.0524\n",
      "Epoch 133: val_loss did not improve from 0.01772\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0162 - mae: 0.0524 - val_loss: 0.0179 - val_mae: 0.0585 - lr: 1.0000e-04\n",
      "Epoch 134/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0160 - mae: 0.0536\n",
      "Epoch 134: val_loss improved from 0.01772 to 0.01661, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0160 - mae: 0.0536 - val_loss: 0.0166 - val_mae: 0.0510 - lr: 1.0000e-04\n",
      "Epoch 135/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0151 - mae: 0.0511\n",
      "Epoch 135: val_loss improved from 0.01661 to 0.01598, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0151 - mae: 0.0511 - val_loss: 0.0160 - val_mae: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 136/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0147 - mae: 0.0510\n",
      "Epoch 136: val_loss improved from 0.01598 to 0.01586, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0147 - mae: 0.0510 - val_loss: 0.0159 - val_mae: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 137/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0145 - mae: 0.0522\n",
      "Epoch 137: val_loss improved from 0.01586 to 0.01537, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0145 - mae: 0.0522 - val_loss: 0.0154 - val_mae: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 138/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0144 - mae: 0.0528\n",
      "Epoch 138: val_loss improved from 0.01537 to 0.01494, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0144 - mae: 0.0528 - val_loss: 0.0149 - val_mae: 0.0499 - lr: 1.0000e-04\n",
      "Epoch 139/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0137 - mae: 0.0513\n",
      "Epoch 139: val_loss improved from 0.01494 to 0.01480, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0137 - mae: 0.0513 - val_loss: 0.0148 - val_mae: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 140/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0487\n",
      "Epoch 140: val_loss improved from 0.01480 to 0.01446, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0129 - mae: 0.0488 - val_loss: 0.0145 - val_mae: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 141/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0129 - mae: 0.0508\n",
      "Epoch 141: val_loss improved from 0.01446 to 0.01372, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0129 - mae: 0.0509 - val_loss: 0.0137 - val_mae: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 142/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0128 - mae: 0.0503\n",
      "Epoch 142: val_loss improved from 0.01372 to 0.01367, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0128 - mae: 0.0504 - val_loss: 0.0137 - val_mae: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 143/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0124 - mae: 0.0506\n",
      "Epoch 143: val_loss improved from 0.01367 to 0.01348, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0124 - mae: 0.0505 - val_loss: 0.0135 - val_mae: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 144/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0122 - mae: 0.0501\n",
      "Epoch 144: val_loss improved from 0.01348 to 0.01307, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0122 - mae: 0.0502 - val_loss: 0.0131 - val_mae: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 145/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0116 - mae: 0.0491\n",
      "Epoch 145: val_loss improved from 0.01307 to 0.01265, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0117 - mae: 0.0492 - val_loss: 0.0126 - val_mae: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 146/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0118 - mae: 0.0514\n",
      "Epoch 146: val_loss did not improve from 0.01265\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0118 - mae: 0.0514 - val_loss: 0.0128 - val_mae: 0.0496 - lr: 1.0000e-04\n",
      "Epoch 147/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0114 - mae: 0.0506\n",
      "Epoch 147: val_loss improved from 0.01265 to 0.01229, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0114 - mae: 0.0506 - val_loss: 0.0123 - val_mae: 0.0478 - lr: 1.0000e-04\n",
      "Epoch 148/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0112 - mae: 0.0507\n",
      "Epoch 148: val_loss improved from 0.01229 to 0.01197, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0112 - mae: 0.0507 - val_loss: 0.0120 - val_mae: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 149/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0109 - mae: 0.0498\n",
      "Epoch 149: val_loss did not improve from 0.01197\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0109 - mae: 0.0498 - val_loss: 0.0122 - val_mae: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 150/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0491\n",
      "Epoch 150: val_loss improved from 0.01197 to 0.01188, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0106 - mae: 0.0492 - val_loss: 0.0119 - val_mae: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 151/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0487\n",
      "Epoch 151: val_loss improved from 0.01188 to 0.01164, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0104 - mae: 0.0487 - val_loss: 0.0116 - val_mae: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 152/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0100 - mae: 0.0481\n",
      "Epoch 152: val_loss improved from 0.01164 to 0.01150, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0100 - mae: 0.0481 - val_loss: 0.0115 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 153/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0100 - mae: 0.0492\n",
      "Epoch 153: val_loss improved from 0.01150 to 0.01112, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0100 - mae: 0.0492 - val_loss: 0.0111 - val_mae: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 154/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0097 - mae: 0.0487\n",
      "Epoch 154: val_loss improved from 0.01112 to 0.01079, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0098 - mae: 0.0487 - val_loss: 0.0108 - val_mae: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 155/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0096 - mae: 0.0487\n",
      "Epoch 155: val_loss improved from 0.01079 to 0.01061, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0097 - mae: 0.0487 - val_loss: 0.0106 - val_mae: 0.0466 - lr: 1.0000e-04\n",
      "Epoch 156/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0099 - mae: 0.0506\n",
      "Epoch 156: val_loss improved from 0.01061 to 0.01042, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0099 - mae: 0.0506 - val_loss: 0.0104 - val_mae: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 157/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0093 - mae: 0.0485\n",
      "Epoch 157: val_loss improved from 0.01042 to 0.01029, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0093 - mae: 0.0486 - val_loss: 0.0103 - val_mae: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 158/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0476\n",
      "Epoch 158: val_loss did not improve from 0.01029\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0090 - mae: 0.0476 - val_loss: 0.0104 - val_mae: 0.0477 - lr: 1.0000e-04\n",
      "Epoch 159/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0090 - mae: 0.0483\n",
      "Epoch 159: val_loss improved from 0.01029 to 0.00991, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0090 - mae: 0.0483 - val_loss: 0.0099 - val_mae: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 160/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0089 - mae: 0.0490\n",
      "Epoch 160: val_loss improved from 0.00991 to 0.00977, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0089 - mae: 0.0491 - val_loss: 0.0098 - val_mae: 0.0440 - lr: 1.0000e-04\n",
      "Epoch 161/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0089 - mae: 0.0496\n",
      "Epoch 161: val_loss did not improve from 0.00977\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0089 - mae: 0.0496 - val_loss: 0.0099 - val_mae: 0.0475 - lr: 1.0000e-04\n",
      "Epoch 162/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0492\n",
      "Epoch 162: val_loss improved from 0.00977 to 0.00953, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0088 - mae: 0.0492 - val_loss: 0.0095 - val_mae: 0.0449 - lr: 1.0000e-04\n",
      "Epoch 163/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0087 - mae: 0.0494\n",
      "Epoch 163: val_loss improved from 0.00953 to 0.00952, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0087 - mae: 0.0493 - val_loss: 0.0095 - val_mae: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 164/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0476\n",
      "Epoch 164: val_loss improved from 0.00952 to 0.00948, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0083 - mae: 0.0476 - val_loss: 0.0095 - val_mae: 0.0457 - lr: 1.0000e-04\n",
      "Epoch 165/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0486\n",
      "Epoch 165: val_loss improved from 0.00948 to 0.00930, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0083 - mae: 0.0486 - val_loss: 0.0093 - val_mae: 0.0455 - lr: 1.0000e-04\n",
      "Epoch 166/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0080 - mae: 0.0478\n",
      "Epoch 166: val_loss improved from 0.00930 to 0.00904, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0080 - mae: 0.0478 - val_loss: 0.0090 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 167/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0081 - mae: 0.0484\n",
      "Epoch 167: val_loss did not improve from 0.00904\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0081 - mae: 0.0484 - val_loss: 0.0094 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 168/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0470\n",
      "Epoch 168: val_loss improved from 0.00904 to 0.00892, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0078 - mae: 0.0470 - val_loss: 0.0089 - val_mae: 0.0443 - lr: 1.0000e-04\n",
      "Epoch 169/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0481\n",
      "Epoch 169: val_loss did not improve from 0.00892\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0078 - mae: 0.0481 - val_loss: 0.0091 - val_mae: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 170/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0476\n",
      "Epoch 170: val_loss improved from 0.00892 to 0.00868, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0077 - mae: 0.0477 - val_loss: 0.0087 - val_mae: 0.0436 - lr: 1.0000e-04\n",
      "Epoch 171/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0489\n",
      "Epoch 171: val_loss improved from 0.00868 to 0.00866, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0078 - mae: 0.0489 - val_loss: 0.0087 - val_mae: 0.0442 - lr: 1.0000e-04\n",
      "Epoch 172/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0482\n",
      "Epoch 172: val_loss did not improve from 0.00866\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0076 - mae: 0.0482 - val_loss: 0.0089 - val_mae: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 173/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0483\n",
      "Epoch 173: val_loss improved from 0.00866 to 0.00845, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0075 - mae: 0.0484 - val_loss: 0.0084 - val_mae: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 174/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0075 - mae: 0.0480\n",
      "Epoch 174: val_loss improved from 0.00845 to 0.00825, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0075 - mae: 0.0480 - val_loss: 0.0082 - val_mae: 0.0434 - lr: 1.0000e-04\n",
      "Epoch 175/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0076 - mae: 0.0481\n",
      "Epoch 175: val_loss did not improve from 0.00825\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0076 - mae: 0.0483 - val_loss: 0.0083 - val_mae: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 176/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0477\n",
      "Epoch 176: val_loss did not improve from 0.00825\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0073 - mae: 0.0477 - val_loss: 0.0083 - val_mae: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 177/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0469\n",
      "Epoch 177: val_loss improved from 0.00825 to 0.00821, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0071 - mae: 0.0469 - val_loss: 0.0082 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 178/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0075 - mae: 0.0488\n",
      "Epoch 178: val_loss did not improve from 0.00821\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0074 - mae: 0.0488 - val_loss: 0.0083 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 179/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0477\n",
      "Epoch 179: val_loss improved from 0.00821 to 0.00775, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0073 - mae: 0.0478 - val_loss: 0.0078 - val_mae: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 180/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0467\n",
      "Epoch 180: val_loss did not improve from 0.00775\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0070 - mae: 0.0468 - val_loss: 0.0078 - val_mae: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 181/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0437\n",
      "Epoch 181: val_loss did not improve from 0.00775\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0065 - mae: 0.0438 - val_loss: 0.0079 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 182/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0474\n",
      "Epoch 182: val_loss improved from 0.00775 to 0.00775, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0070 - mae: 0.0475 - val_loss: 0.0077 - val_mae: 0.0411 - lr: 1.0000e-04\n",
      "Epoch 183/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0071 - mae: 0.0479\n",
      "Epoch 183: val_loss did not improve from 0.00775\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0071 - mae: 0.0479 - val_loss: 0.0079 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 184/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0469\n",
      "Epoch 184: val_loss did not improve from 0.00775\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0069 - mae: 0.0469 - val_loss: 0.0081 - val_mae: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 185/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0470\n",
      "Epoch 185: val_loss did not improve from 0.00775\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0069 - mae: 0.0471 - val_loss: 0.0078 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 186/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0468\n",
      "Epoch 186: val_loss improved from 0.00775 to 0.00765, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0067 - mae: 0.0468 - val_loss: 0.0076 - val_mae: 0.0415 - lr: 5.0000e-05\n",
      "Epoch 187/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0065 - mae: 0.0452\n",
      "Epoch 187: val_loss did not improve from 0.00765\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0065 - mae: 0.0452 - val_loss: 0.0077 - val_mae: 0.0412 - lr: 5.0000e-05\n",
      "Epoch 188/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0065 - mae: 0.0455\n",
      "Epoch 188: val_loss did not improve from 0.00765\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0065 - mae: 0.0455 - val_loss: 0.0077 - val_mae: 0.0428 - lr: 5.0000e-05\n",
      "Epoch 189/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0455\n",
      "Epoch 189: val_loss did not improve from 0.00765\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0065 - mae: 0.0455 - val_loss: 0.0077 - val_mae: 0.0446 - lr: 5.0000e-05\n",
      "Epoch 190/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0442\n",
      "Epoch 190: val_loss improved from 0.00765 to 0.00744, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0062 - mae: 0.0442 - val_loss: 0.0074 - val_mae: 0.0420 - lr: 5.0000e-05\n",
      "Epoch 191/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0448\n",
      "Epoch 191: val_loss improved from 0.00744 to 0.00734, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0063 - mae: 0.0448 - val_loss: 0.0073 - val_mae: 0.0406 - lr: 5.0000e-05\n",
      "Epoch 192/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0437\n",
      "Epoch 192: val_loss improved from 0.00734 to 0.00732, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0061 - mae: 0.0436 - val_loss: 0.0073 - val_mae: 0.0409 - lr: 5.0000e-05\n",
      "Epoch 193/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0443\n",
      "Epoch 193: val_loss improved from 0.00732 to 0.00726, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0061 - mae: 0.0443 - val_loss: 0.0073 - val_mae: 0.0401 - lr: 5.0000e-05\n",
      "Epoch 194/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0441\n",
      "Epoch 194: val_loss did not improve from 0.00726\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0060 - mae: 0.0442 - val_loss: 0.0074 - val_mae: 0.0419 - lr: 5.0000e-05\n",
      "Epoch 195/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0444\n",
      "Epoch 195: val_loss did not improve from 0.00726\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0060 - mae: 0.0445 - val_loss: 0.0074 - val_mae: 0.0418 - lr: 5.0000e-05\n",
      "Epoch 196/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0449\n",
      "Epoch 196: val_loss did not improve from 0.00726\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0060 - mae: 0.0450 - val_loss: 0.0074 - val_mae: 0.0419 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0451\n",
      "Epoch 197: val_loss did not improve from 0.00726\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0060 - mae: 0.0451 - val_loss: 0.0073 - val_mae: 0.0419 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0455\n",
      "Epoch 198: val_loss improved from 0.00726 to 0.00722, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0061 - mae: 0.0456 - val_loss: 0.0072 - val_mae: 0.0406 - lr: 2.5000e-05\n",
      "Epoch 199/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0453\n",
      "Epoch 199: val_loss improved from 0.00722 to 0.00712, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0060 - mae: 0.0453 - val_loss: 0.0071 - val_mae: 0.0399 - lr: 2.5000e-05\n",
      "Epoch 200/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0431\n",
      "Epoch 200: val_loss did not improve from 0.00712\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0057 - mae: 0.0431 - val_loss: 0.0071 - val_mae: 0.0405 - lr: 2.5000e-05\n",
      "Epoch 201/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0059 - mae: 0.0445\n",
      "Epoch 201: val_loss did not improve from 0.00712\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0059 - mae: 0.0445 - val_loss: 0.0071 - val_mae: 0.0402 - lr: 2.5000e-05\n",
      "Epoch 202/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0441\n",
      "Epoch 202: val_loss improved from 0.00712 to 0.00705, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0058 - mae: 0.0441 - val_loss: 0.0071 - val_mae: 0.0405 - lr: 2.5000e-05\n",
      "Epoch 203/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0434\n",
      "Epoch 203: val_loss improved from 0.00705 to 0.00700, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0057 - mae: 0.0434 - val_loss: 0.0070 - val_mae: 0.0402 - lr: 2.5000e-05\n",
      "Epoch 204/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0457\n",
      "Epoch 204: val_loss did not improve from 0.00700\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0061 - mae: 0.0458 - val_loss: 0.0070 - val_mae: 0.0407 - lr: 2.5000e-05\n",
      "Epoch 205/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0445\n",
      "Epoch 205: val_loss improved from 0.00700 to 0.00694, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0057 - mae: 0.0446 - val_loss: 0.0069 - val_mae: 0.0397 - lr: 2.5000e-05\n",
      "Epoch 206/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0429\n",
      "Epoch 206: val_loss improved from 0.00694 to 0.00689, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0430 - val_loss: 0.0069 - val_mae: 0.0390 - lr: 2.5000e-05\n",
      "Epoch 207/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0435\n",
      "Epoch 207: val_loss did not improve from 0.00689\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0435 - val_loss: 0.0069 - val_mae: 0.0408 - lr: 2.5000e-05\n",
      "Epoch 208/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0436\n",
      "Epoch 208: val_loss improved from 0.00689 to 0.00688, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0436 - val_loss: 0.0069 - val_mae: 0.0400 - lr: 2.5000e-05\n",
      "Epoch 209/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0435\n",
      "Epoch 209: val_loss improved from 0.00688 to 0.00677, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0056 - mae: 0.0435 - val_loss: 0.0068 - val_mae: 0.0386 - lr: 2.5000e-05\n",
      "Epoch 210/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0055 - mae: 0.0433\n",
      "Epoch 210: val_loss did not improve from 0.00677\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.0055 - mae: 0.0433 - val_loss: 0.0069 - val_mae: 0.0397 - lr: 2.5000e-05\n",
      "Epoch 211/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0435\n",
      "Epoch 211: val_loss did not improve from 0.00677\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0055 - mae: 0.0435 - val_loss: 0.0069 - val_mae: 0.0394 - lr: 2.5000e-05\n",
      "Epoch 212/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0431\n",
      "Epoch 212: val_loss did not improve from 0.00677\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0055 - mae: 0.0432 - val_loss: 0.0069 - val_mae: 0.0402 - lr: 2.5000e-05\n",
      "Epoch 213/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0429\n",
      "Epoch 213: val_loss did not improve from 0.00677\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0055 - mae: 0.0429 - val_loss: 0.0068 - val_mae: 0.0393 - lr: 2.5000e-05\n",
      "Epoch 214/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0444\n",
      "Epoch 214: val_loss improved from 0.00677 to 0.00676, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0057 - mae: 0.0445 - val_loss: 0.0068 - val_mae: 0.0391 - lr: 2.5000e-05\n",
      "Epoch 215/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0433\n",
      "Epoch 215: val_loss did not improve from 0.00676\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0434 - val_loss: 0.0068 - val_mae: 0.0393 - lr: 2.5000e-05\n",
      "Epoch 216/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0433\n",
      "Epoch 216: val_loss improved from 0.00676 to 0.00675, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0433 - val_loss: 0.0068 - val_mae: 0.0394 - lr: 1.2500e-05\n",
      "Epoch 217/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0451\n",
      "Epoch 217: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0057 - mae: 0.0454 - val_loss: 0.0068 - val_mae: 0.0396 - lr: 1.2500e-05\n",
      "Epoch 218/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0440\n",
      "Epoch 218: val_loss did not improve from 0.00675\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0055 - mae: 0.0440 - val_loss: 0.0068 - val_mae: 0.0400 - lr: 1.2500e-05\n",
      "Epoch 219/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0431\n",
      "Epoch 219: val_loss improved from 0.00675 to 0.00672, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0431 - val_loss: 0.0067 - val_mae: 0.0391 - lr: 1.2500e-05\n",
      "Epoch 220/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0432\n",
      "Epoch 220: val_loss improved from 0.00672 to 0.00665, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0432 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 1.2500e-05\n",
      "Epoch 221/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0055 - mae: 0.0436\n",
      "Epoch 221: val_loss did not improve from 0.00665\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0055 - mae: 0.0437 - val_loss: 0.0067 - val_mae: 0.0394 - lr: 1.2500e-05\n",
      "Epoch 222/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0449\n",
      "Epoch 222: val_loss did not improve from 0.00665\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0449 - val_loss: 0.0067 - val_mae: 0.0393 - lr: 1.2500e-05\n",
      "Epoch 223/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0425\n",
      "Epoch 223: val_loss did not improve from 0.00665\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0053 - mae: 0.0424 - val_loss: 0.0067 - val_mae: 0.0390 - lr: 1.2500e-05\n",
      "Epoch 224/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0430\n",
      "Epoch 224: val_loss did not improve from 0.00665\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0053 - mae: 0.0430 - val_loss: 0.0067 - val_mae: 0.0396 - lr: 1.2500e-05\n",
      "Epoch 225/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0437\n",
      "Epoch 225: val_loss improved from 0.00665 to 0.00664, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0437 - val_loss: 0.0066 - val_mae: 0.0388 - lr: 1.2500e-05\n",
      "Epoch 226/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0414\n",
      "Epoch 226: val_loss improved from 0.00664 to 0.00662, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0415 - val_loss: 0.0066 - val_mae: 0.0388 - lr: 1.2500e-05\n",
      "Epoch 227/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0427\n",
      "Epoch 227: val_loss improved from 0.00662 to 0.00662, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0052 - mae: 0.0428 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 6.2500e-06\n",
      "Epoch 228/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0422\n",
      "Epoch 228: val_loss did not improve from 0.00662\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0052 - mae: 0.0421 - val_loss: 0.0066 - val_mae: 0.0388 - lr: 6.2500e-06\n",
      "Epoch 229/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0429\n",
      "Epoch 229: val_loss did not improve from 0.00662\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0066 - val_mae: 0.0391 - lr: 6.2500e-06\n",
      "Epoch 230/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0429\n",
      "Epoch 230: val_loss improved from 0.00662 to 0.00662, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0053 - mae: 0.0429 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 6.2500e-06\n",
      "Epoch 231/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0428\n",
      "Epoch 231: val_loss did not improve from 0.00662\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0052 - mae: 0.0428 - val_loss: 0.0066 - val_mae: 0.0392 - lr: 6.2500e-06\n",
      "Epoch 232/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0438\n",
      "Epoch 232: val_loss did not improve from 0.00662\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0053 - mae: 0.0438 - val_loss: 0.0066 - val_mae: 0.0394 - lr: 6.2500e-06\n",
      "Epoch 233/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0422\n",
      "Epoch 233: val_loss improved from 0.00662 to 0.00661, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0051 - mae: 0.0422 - val_loss: 0.0066 - val_mae: 0.0390 - lr: 3.1250e-06\n",
      "Epoch 234/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0414\n",
      "Epoch 234: val_loss improved from 0.00661 to 0.00660, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0413 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 3.1250e-06\n",
      "Epoch 235/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0415\n",
      "Epoch 235: val_loss did not improve from 0.00660\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0415 - val_loss: 0.0066 - val_mae: 0.0392 - lr: 3.1250e-06\n",
      "Epoch 236/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0422\n",
      "Epoch 236: val_loss improved from 0.00660 to 0.00660, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0421 - val_loss: 0.0066 - val_mae: 0.0388 - lr: 3.1250e-06\n",
      "Epoch 237/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0429\n",
      "Epoch 237: val_loss did not improve from 0.00660\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0052 - mae: 0.0430 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 3.1250e-06\n",
      "Epoch 238/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0426\n",
      "Epoch 238: val_loss improved from 0.00660 to 0.00659, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0053 - mae: 0.0426 - val_loss: 0.0066 - val_mae: 0.0389 - lr: 3.1250e-06\n",
      "Epoch 239/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0416\n",
      "Epoch 239: val_loss did not improve from 0.00659\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0417 - val_loss: 0.0066 - val_mae: 0.0393 - lr: 1.5625e-06\n",
      "Epoch 240/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0432\n",
      "Epoch 240: val_loss did not improve from 0.00659\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0052 - mae: 0.0432 - val_loss: 0.0066 - val_mae: 0.0390 - lr: 1.5625e-06\n",
      "Epoch 241/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0440\n",
      "Epoch 241: val_loss did not improve from 0.00659\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0441 - val_loss: 0.0066 - val_mae: 0.0391 - lr: 1.5625e-06\n",
      "Epoch 242/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0432\n",
      "Epoch 242: val_loss did not improve from 0.00659\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0433 - val_loss: 0.0066 - val_mae: 0.0391 - lr: 1.5625e-06\n",
      "Epoch 243/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0439\n",
      "Epoch 243: val_loss improved from 0.00659 to 0.00658, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0440 - val_loss: 0.0066 - val_mae: 0.0390 - lr: 1.5625e-06\n",
      "Epoch 244/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0441\n",
      "Epoch 244: val_loss improved from 0.00658 to 0.00658, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0442 - val_loss: 0.0066 - val_mae: 0.0387 - lr: 1.5625e-06\n",
      "Epoch 245/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0051 - mae: 0.0418\n",
      "Epoch 245: val_loss did not improve from 0.00658\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0051 - mae: 0.0418 - val_loss: 0.0066 - val_mae: 0.0392 - lr: 7.8125e-07\n",
      "Epoch 246/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0441\n",
      "Epoch 246: val_loss did not improve from 0.00658\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0054 - mae: 0.0441 - val_loss: 0.0066 - val_mae: 0.0390 - lr: 7.8125e-07\n",
      "Epoch 247/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0429\n",
      "Epoch 247: val_loss improved from 0.00658 to 0.00657, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0428 - val_loss: 0.0066 - val_mae: 0.0388 - lr: 7.8125e-07\n",
      "Epoch 248/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0435\n",
      "Epoch 248: val_loss improved from 0.00657 to 0.00657, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0053 - mae: 0.0436 - val_loss: 0.0066 - val_mae: 0.0387 - lr: 7.8125e-07\n",
      "Epoch 249/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0418\n",
      "Epoch 249: val_loss improved from 0.00657 to 0.00656, saving model to models\\date_predictor-v25_fold4.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0051 - mae: 0.0417 - val_loss: 0.0066 - val_mae: 0.0386 - lr: 7.8125e-07\n",
      "Epoch 250/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0414\n",
      "Epoch 250: val_loss did not improve from 0.00656\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0049 - mae: 0.0416 - val_loss: 0.0066 - val_mae: 0.0390 - lr: 7.8125e-07\n",
      "46/46 [==============================] - 0s 2ms/step\n",
      "Fold 4 MAE: 58.738891517780544\n",
      "\n",
      "--- Training Fold 5/5 ---\n",
      "Epoch 1/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 3.7420 - mae: 1.4865\n",
      "Epoch 1: val_loss improved from inf to 0.51260, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 6s 22ms/step - loss: 3.7405 - mae: 1.4862 - val_loss: 0.5126 - val_mae: 0.5171 - lr: 1.0000e-04\n",
      "Epoch 2/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 2.9887 - mae: 1.3269\n",
      "Epoch 2: val_loss improved from 0.51260 to 0.42754, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.9818 - mae: 1.3255 - val_loss: 0.4275 - val_mae: 0.4721 - lr: 1.0000e-04\n",
      "Epoch 3/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 2.4400 - mae: 1.1993\n",
      "Epoch 3: val_loss improved from 0.42754 to 0.32008, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 2.4358 - mae: 1.1984 - val_loss: 0.3201 - val_mae: 0.3797 - lr: 1.0000e-04\n",
      "Epoch 4/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 2.0320 - mae: 1.0832\n",
      "Epoch 4: val_loss improved from 0.32008 to 0.26646, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 2.0315 - mae: 1.0831 - val_loss: 0.2665 - val_mae: 0.3277 - lr: 1.0000e-04\n",
      "Epoch 5/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.8116 - mae: 1.0193\n",
      "Epoch 5: val_loss improved from 0.26646 to 0.24554, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 1.8106 - mae: 1.0190 - val_loss: 0.2455 - val_mae: 0.3094 - lr: 1.0000e-04\n",
      "Epoch 6/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5255 - mae: 0.9353\n",
      "Epoch 6: val_loss improved from 0.24554 to 0.21351, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.5255 - mae: 0.9353 - val_loss: 0.2135 - val_mae: 0.2713 - lr: 1.0000e-04\n",
      "Epoch 7/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.3554 - mae: 0.8734\n",
      "Epoch 7: val_loss improved from 0.21351 to 0.21005, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 1.3555 - mae: 0.8733 - val_loss: 0.2101 - val_mae: 0.2691 - lr: 1.0000e-04\n",
      "Epoch 8/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 1.2337 - mae: 0.8241\n",
      "Epoch 8: val_loss improved from 0.21005 to 0.19164, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 1.2316 - mae: 0.8234 - val_loss: 0.1916 - val_mae: 0.2451 - lr: 1.0000e-04\n",
      "Epoch 9/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 1.0513 - mae: 0.7581\n",
      "Epoch 9: val_loss improved from 0.19164 to 0.18404, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 1.0511 - mae: 0.7581 - val_loss: 0.1840 - val_mae: 0.2343 - lr: 1.0000e-04\n",
      "Epoch 10/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.9765 - mae: 0.7322\n",
      "Epoch 10: val_loss improved from 0.18404 to 0.17927, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.9767 - mae: 0.7323 - val_loss: 0.1793 - val_mae: 0.2285 - lr: 1.0000e-04\n",
      "Epoch 11/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8263 - mae: 0.6661\n",
      "Epoch 11: val_loss improved from 0.17927 to 0.17268, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.8263 - mae: 0.6661 - val_loss: 0.1727 - val_mae: 0.2193 - lr: 1.0000e-04\n",
      "Epoch 12/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7371 - mae: 0.6238\n",
      "Epoch 12: val_loss improved from 0.17268 to 0.16716, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.7371 - mae: 0.6238 - val_loss: 0.1672 - val_mae: 0.2112 - lr: 1.0000e-04\n",
      "Epoch 13/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.6400 - mae: 0.5788\n",
      "Epoch 13: val_loss improved from 0.16716 to 0.16490, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.6395 - mae: 0.5785 - val_loss: 0.1649 - val_mae: 0.2084 - lr: 1.0000e-04\n",
      "Epoch 14/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5879 - mae: 0.5488\n",
      "Epoch 14: val_loss improved from 0.16490 to 0.15891, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.5879 - mae: 0.5487 - val_loss: 0.1589 - val_mae: 0.1989 - lr: 1.0000e-04\n",
      "Epoch 15/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5131 - mae: 0.5060\n",
      "Epoch 15: val_loss improved from 0.15891 to 0.15397, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.5129 - mae: 0.5059 - val_loss: 0.1540 - val_mae: 0.1910 - lr: 1.0000e-04\n",
      "Epoch 16/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.4689 - mae: 0.4764\n",
      "Epoch 16: val_loss improved from 0.15397 to 0.14850, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.4689 - mae: 0.4766 - val_loss: 0.1485 - val_mae: 0.1816 - lr: 1.0000e-04\n",
      "Epoch 17/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.4212 - mae: 0.4447\n",
      "Epoch 17: val_loss did not improve from 0.14850\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.4208 - mae: 0.4447 - val_loss: 0.1491 - val_mae: 0.1827 - lr: 1.0000e-04\n",
      "Epoch 18/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3889 - mae: 0.4244\n",
      "Epoch 18: val_loss improved from 0.14850 to 0.14702, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.3888 - mae: 0.4242 - val_loss: 0.1470 - val_mae: 0.1795 - lr: 1.0000e-04\n",
      "Epoch 19/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3382 - mae: 0.3871\n",
      "Epoch 19: val_loss improved from 0.14702 to 0.14407, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.3381 - mae: 0.3869 - val_loss: 0.1441 - val_mae: 0.1747 - lr: 1.0000e-04\n",
      "Epoch 20/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.3126 - mae: 0.3643\n",
      "Epoch 20: val_loss improved from 0.14407 to 0.14260, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.3128 - mae: 0.3645 - val_loss: 0.1426 - val_mae: 0.1725 - lr: 1.0000e-04\n",
      "Epoch 21/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2824 - mae: 0.3402\n",
      "Epoch 21: val_loss improved from 0.14260 to 0.14160, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2824 - mae: 0.3402 - val_loss: 0.1416 - val_mae: 0.1710 - lr: 1.0000e-04\n",
      "Epoch 22/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2651 - mae: 0.3229\n",
      "Epoch 22: val_loss improved from 0.14160 to 0.14126, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.2651 - mae: 0.3227 - val_loss: 0.1413 - val_mae: 0.1708 - lr: 1.0000e-04\n",
      "Epoch 23/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2446 - mae: 0.3038\n",
      "Epoch 23: val_loss improved from 0.14126 to 0.14054, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.2446 - mae: 0.3037 - val_loss: 0.1405 - val_mae: 0.1698 - lr: 1.0000e-04\n",
      "Epoch 24/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2262 - mae: 0.2838\n",
      "Epoch 24: val_loss did not improve from 0.14054\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2262 - mae: 0.2837 - val_loss: 0.1410 - val_mae: 0.1709 - lr: 1.0000e-04\n",
      "Epoch 25/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.2099 - mae: 0.2672\n",
      "Epoch 25: val_loss improved from 0.14054 to 0.13951, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.2097 - mae: 0.2670 - val_loss: 0.1395 - val_mae: 0.1685 - lr: 1.0000e-04\n",
      "Epoch 26/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2023 - mae: 0.2582\n",
      "Epoch 26: val_loss improved from 0.13951 to 0.13847, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.2023 - mae: 0.2582 - val_loss: 0.1385 - val_mae: 0.1667 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1902 - mae: 0.2433\n",
      "Epoch 27: val_loss improved from 0.13847 to 0.13719, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1902 - mae: 0.2433 - val_loss: 0.1372 - val_mae: 0.1649 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1794 - mae: 0.2292\n",
      "Epoch 28: val_loss improved from 0.13719 to 0.13702, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.1794 - mae: 0.2292 - val_loss: 0.1370 - val_mae: 0.1648 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1714 - mae: 0.2168\n",
      "Epoch 29: val_loss improved from 0.13702 to 0.13649, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1714 - mae: 0.2168 - val_loss: 0.1365 - val_mae: 0.1642 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1652 - mae: 0.2091\n",
      "Epoch 30: val_loss improved from 0.13649 to 0.13534, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1652 - mae: 0.2091 - val_loss: 0.1353 - val_mae: 0.1623 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1604 - mae: 0.2006\n",
      "Epoch 31: val_loss improved from 0.13534 to 0.13513, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1603 - mae: 0.2005 - val_loss: 0.1351 - val_mae: 0.1624 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1549 - mae: 0.1933\n",
      "Epoch 32: val_loss improved from 0.13513 to 0.13473, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1549 - mae: 0.1933 - val_loss: 0.1347 - val_mae: 0.1618 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1513 - mae: 0.1882\n",
      "Epoch 33: val_loss improved from 0.13473 to 0.13422, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1512 - mae: 0.1881 - val_loss: 0.1342 - val_mae: 0.1610 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1472 - mae: 0.1820\n",
      "Epoch 34: val_loss improved from 0.13422 to 0.13298, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1471 - mae: 0.1819 - val_loss: 0.1330 - val_mae: 0.1585 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1456 - mae: 0.1787\n",
      "Epoch 35: val_loss improved from 0.13298 to 0.13203, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.1456 - mae: 0.1788 - val_loss: 0.1320 - val_mae: 0.1565 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1431 - mae: 0.1743\n",
      "Epoch 36: val_loss did not improve from 0.13203\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1431 - mae: 0.1743 - val_loss: 0.1322 - val_mae: 0.1570 - lr: 1.0000e-04\n",
      "Epoch 37/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1407 - mae: 0.1700\n",
      "Epoch 37: val_loss improved from 0.13203 to 0.13159, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1407 - mae: 0.1700 - val_loss: 0.1316 - val_mae: 0.1559 - lr: 1.0000e-04\n",
      "Epoch 38/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1393 - mae: 0.1677\n",
      "Epoch 38: val_loss improved from 0.13159 to 0.13076, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1392 - mae: 0.1675 - val_loss: 0.1308 - val_mae: 0.1540 - lr: 1.0000e-04\n",
      "Epoch 39/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1372 - mae: 0.1648\n",
      "Epoch 39: val_loss improved from 0.13076 to 0.13042, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1372 - mae: 0.1648 - val_loss: 0.1304 - val_mae: 0.1533 - lr: 1.0000e-04\n",
      "Epoch 40/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1361 - mae: 0.1630\n",
      "Epoch 40: val_loss improved from 0.13042 to 0.12998, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1361 - mae: 0.1630 - val_loss: 0.1300 - val_mae: 0.1524 - lr: 1.0000e-04\n",
      "Epoch 41/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1354 - mae: 0.1614\n",
      "Epoch 41: val_loss improved from 0.12998 to 0.12954, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1354 - mae: 0.1614 - val_loss: 0.1295 - val_mae: 0.1513 - lr: 1.0000e-04\n",
      "Epoch 42/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1338 - mae: 0.1594\n",
      "Epoch 42: val_loss improved from 0.12954 to 0.12919, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1338 - mae: 0.1594 - val_loss: 0.1292 - val_mae: 0.1505 - lr: 1.0000e-04\n",
      "Epoch 43/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1331 - mae: 0.1577\n",
      "Epoch 43: val_loss improved from 0.12919 to 0.12906, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1331 - mae: 0.1577 - val_loss: 0.1291 - val_mae: 0.1504 - lr: 1.0000e-04\n",
      "Epoch 44/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1325 - mae: 0.1568\n",
      "Epoch 44: val_loss improved from 0.12906 to 0.12887, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1326 - mae: 0.1568 - val_loss: 0.1289 - val_mae: 0.1503 - lr: 1.0000e-04\n",
      "Epoch 45/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1317 - mae: 0.1556\n",
      "Epoch 45: val_loss improved from 0.12887 to 0.12864, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1317 - mae: 0.1556 - val_loss: 0.1286 - val_mae: 0.1499 - lr: 1.0000e-04\n",
      "Epoch 46/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1310 - mae: 0.1543\n",
      "Epoch 46: val_loss improved from 0.12864 to 0.12829, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1310 - mae: 0.1543 - val_loss: 0.1283 - val_mae: 0.1493 - lr: 1.0000e-04\n",
      "Epoch 47/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1301 - mae: 0.1526\n",
      "Epoch 47: val_loss improved from 0.12829 to 0.12797, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1301 - mae: 0.1526 - val_loss: 0.1280 - val_mae: 0.1488 - lr: 1.0000e-04\n",
      "Epoch 48/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1300 - mae: 0.1531\n",
      "Epoch 48: val_loss improved from 0.12797 to 0.12774, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1300 - mae: 0.1531 - val_loss: 0.1277 - val_mae: 0.1485 - lr: 1.0000e-04\n",
      "Epoch 49/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1295 - mae: 0.1524\n",
      "Epoch 49: val_loss improved from 0.12774 to 0.12749, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1296 - mae: 0.1527 - val_loss: 0.1275 - val_mae: 0.1482 - lr: 1.0000e-04\n",
      "Epoch 50/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1291 - mae: 0.1518\n",
      "Epoch 50: val_loss improved from 0.12749 to 0.12713, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1291 - mae: 0.1517 - val_loss: 0.1271 - val_mae: 0.1477 - lr: 1.0000e-04\n",
      "Epoch 51/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1282 - mae: 0.1501\n",
      "Epoch 51: val_loss improved from 0.12713 to 0.12676, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1282 - mae: 0.1501 - val_loss: 0.1268 - val_mae: 0.1472 - lr: 1.0000e-04\n",
      "Epoch 52/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1277 - mae: 0.1497\n",
      "Epoch 52: val_loss improved from 0.12676 to 0.12627, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1278 - mae: 0.1498 - val_loss: 0.1263 - val_mae: 0.1465 - lr: 1.0000e-04\n",
      "Epoch 53/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1275 - mae: 0.1496\n",
      "Epoch 53: val_loss improved from 0.12627 to 0.12594, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1275 - mae: 0.1497 - val_loss: 0.1259 - val_mae: 0.1463 - lr: 1.0000e-04\n",
      "Epoch 54/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1271 - mae: 0.1498\n",
      "Epoch 54: val_loss improved from 0.12594 to 0.12557, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1271 - mae: 0.1497 - val_loss: 0.1256 - val_mae: 0.1463 - lr: 1.0000e-04\n",
      "Epoch 55/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1263 - mae: 0.1487\n",
      "Epoch 55: val_loss improved from 0.12557 to 0.12487, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1264 - mae: 0.1487 - val_loss: 0.1249 - val_mae: 0.1453 - lr: 1.0000e-04\n",
      "Epoch 56/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1258 - mae: 0.1477\n",
      "Epoch 56: val_loss improved from 0.12487 to 0.12418, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1258 - mae: 0.1477 - val_loss: 0.1242 - val_mae: 0.1446 - lr: 1.0000e-04\n",
      "Epoch 57/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1255 - mae: 0.1484\n",
      "Epoch 57: val_loss improved from 0.12418 to 0.12346, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1254 - mae: 0.1484 - val_loss: 0.1235 - val_mae: 0.1439 - lr: 1.0000e-04\n",
      "Epoch 58/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1247 - mae: 0.1474\n",
      "Epoch 58: val_loss improved from 0.12346 to 0.12258, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1247 - mae: 0.1473 - val_loss: 0.1226 - val_mae: 0.1428 - lr: 1.0000e-04\n",
      "Epoch 59/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1241 - mae: 0.1465\n",
      "Epoch 59: val_loss improved from 0.12258 to 0.12142, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1241 - mae: 0.1464 - val_loss: 0.1214 - val_mae: 0.1412 - lr: 1.0000e-04\n",
      "Epoch 60/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1223 - mae: 0.1437\n",
      "Epoch 60: val_loss improved from 0.12142 to 0.11970, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1223 - mae: 0.1437 - val_loss: 0.1197 - val_mae: 0.1382 - lr: 1.0000e-04\n",
      "Epoch 61/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1214 - mae: 0.1429\n",
      "Epoch 61: val_loss improved from 0.11970 to 0.11809, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1215 - mae: 0.1430 - val_loss: 0.1181 - val_mae: 0.1355 - lr: 1.0000e-04\n",
      "Epoch 62/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1200 - mae: 0.1409\n",
      "Epoch 62: val_loss improved from 0.11809 to 0.11546, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1200 - mae: 0.1409 - val_loss: 0.1155 - val_mae: 0.1302 - lr: 1.0000e-04\n",
      "Epoch 63/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1177 - mae: 0.1361\n",
      "Epoch 63: val_loss improved from 0.11546 to 0.11240, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1177 - mae: 0.1362 - val_loss: 0.1124 - val_mae: 0.1232 - lr: 1.0000e-04\n",
      "Epoch 64/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1160 - mae: 0.1327\n",
      "Epoch 64: val_loss improved from 0.11240 to 0.10973, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.1160 - mae: 0.1327 - val_loss: 0.1097 - val_mae: 0.1169 - lr: 1.0000e-04\n",
      "Epoch 65/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1137 - mae: 0.1277\n",
      "Epoch 65: val_loss improved from 0.10973 to 0.10692, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1137 - mae: 0.1277 - val_loss: 0.1069 - val_mae: 0.1097 - lr: 1.0000e-04\n",
      "Epoch 66/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1112 - mae: 0.1222\n",
      "Epoch 66: val_loss improved from 0.10692 to 0.10440, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1112 - mae: 0.1222 - val_loss: 0.1044 - val_mae: 0.1032 - lr: 1.0000e-04\n",
      "Epoch 67/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1088 - mae: 0.1167\n",
      "Epoch 67: val_loss improved from 0.10440 to 0.10236, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.1088 - mae: 0.1167 - val_loss: 0.1024 - val_mae: 0.0984 - lr: 1.0000e-04\n",
      "Epoch 68/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.1069 - mae: 0.1137\n",
      "Epoch 68: val_loss improved from 0.10236 to 0.10039, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1069 - mae: 0.1137 - val_loss: 0.1004 - val_mae: 0.0933 - lr: 1.0000e-04\n",
      "Epoch 69/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.1044 - mae: 0.1087\n",
      "Epoch 69: val_loss improved from 0.10039 to 0.09875, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.1044 - mae: 0.1087 - val_loss: 0.0988 - val_mae: 0.0899 - lr: 1.0000e-04\n",
      "Epoch 70/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.1031 - mae: 0.1069\n",
      "Epoch 70: val_loss improved from 0.09875 to 0.09746, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.1031 - mae: 0.1069 - val_loss: 0.0975 - val_mae: 0.0881 - lr: 1.0000e-04\n",
      "Epoch 71/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1013 - mae: 0.1028\n",
      "Epoch 71: val_loss improved from 0.09746 to 0.09597, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.1012 - mae: 0.1028 - val_loss: 0.0960 - val_mae: 0.0852 - lr: 1.0000e-04\n",
      "Epoch 72/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0992 - mae: 0.1000\n",
      "Epoch 72: val_loss improved from 0.09597 to 0.09447, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0992 - mae: 0.1000 - val_loss: 0.0945 - val_mae: 0.0826 - lr: 1.0000e-04\n",
      "Epoch 73/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0976 - mae: 0.0970\n",
      "Epoch 73: val_loss improved from 0.09447 to 0.09294, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0976 - mae: 0.0970 - val_loss: 0.0929 - val_mae: 0.0801 - lr: 1.0000e-04\n",
      "Epoch 74/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0959 - mae: 0.0939\n",
      "Epoch 74: val_loss improved from 0.09294 to 0.09152, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0959 - mae: 0.0940 - val_loss: 0.0915 - val_mae: 0.0784 - lr: 1.0000e-04\n",
      "Epoch 75/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0944 - mae: 0.0927\n",
      "Epoch 75: val_loss improved from 0.09152 to 0.08981, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0944 - mae: 0.0928 - val_loss: 0.0898 - val_mae: 0.0747 - lr: 1.0000e-04\n",
      "Epoch 76/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0928 - mae: 0.0895\n",
      "Epoch 76: val_loss improved from 0.08981 to 0.08918, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0928 - mae: 0.0894 - val_loss: 0.0892 - val_mae: 0.0767 - lr: 1.0000e-04\n",
      "Epoch 77/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0914 - mae: 0.0889\n",
      "Epoch 77: val_loss improved from 0.08918 to 0.08769, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0914 - mae: 0.0889 - val_loss: 0.0877 - val_mae: 0.0751 - lr: 1.0000e-04\n",
      "Epoch 78/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0893 - mae: 0.0857\n",
      "Epoch 78: val_loss improved from 0.08769 to 0.08610, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0893 - mae: 0.0857 - val_loss: 0.0861 - val_mae: 0.0726 - lr: 1.0000e-04\n",
      "Epoch 79/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0877 - mae: 0.0842\n",
      "Epoch 79: val_loss improved from 0.08610 to 0.08465, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0877 - mae: 0.0842 - val_loss: 0.0847 - val_mae: 0.0713 - lr: 1.0000e-04\n",
      "Epoch 80/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0867 - mae: 0.0840\n",
      "Epoch 80: val_loss improved from 0.08465 to 0.08322, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0867 - mae: 0.0840 - val_loss: 0.0832 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 81/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0846 - mae: 0.0808\n",
      "Epoch 81: val_loss improved from 0.08322 to 0.08182, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0846 - mae: 0.0808 - val_loss: 0.0818 - val_mae: 0.0693 - lr: 1.0000e-04\n",
      "Epoch 82/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0832 - mae: 0.0802\n",
      "Epoch 82: val_loss improved from 0.08182 to 0.08035, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0832 - mae: 0.0803 - val_loss: 0.0803 - val_mae: 0.0686 - lr: 1.0000e-04\n",
      "Epoch 83/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0818 - mae: 0.0801\n",
      "Epoch 83: val_loss improved from 0.08035 to 0.07858, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0818 - mae: 0.0801 - val_loss: 0.0786 - val_mae: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 84/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0800 - mae: 0.0778\n",
      "Epoch 84: val_loss improved from 0.07858 to 0.07746, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0800 - mae: 0.0778 - val_loss: 0.0775 - val_mae: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 85/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0783 - mae: 0.0766\n",
      "Epoch 85: val_loss improved from 0.07746 to 0.07558, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0783 - mae: 0.0766 - val_loss: 0.0756 - val_mae: 0.0652 - lr: 1.0000e-04\n",
      "Epoch 86/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0764 - mae: 0.0744\n",
      "Epoch 86: val_loss improved from 0.07558 to 0.07425, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0764 - mae: 0.0744 - val_loss: 0.0742 - val_mae: 0.0659 - lr: 1.0000e-04\n",
      "Epoch 87/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0754 - mae: 0.0763\n",
      "Epoch 87: val_loss improved from 0.07425 to 0.07281, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.0754 - mae: 0.0763 - val_loss: 0.0728 - val_mae: 0.0660 - lr: 1.0000e-04\n",
      "Epoch 88/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0731 - mae: 0.0730\n",
      "Epoch 88: val_loss improved from 0.07281 to 0.07100, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0731 - mae: 0.0730 - val_loss: 0.0710 - val_mae: 0.0640 - lr: 1.0000e-04\n",
      "Epoch 89/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0717 - mae: 0.0729\n",
      "Epoch 89: val_loss improved from 0.07100 to 0.06978, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 3s 18ms/step - loss: 0.0717 - mae: 0.0730 - val_loss: 0.0698 - val_mae: 0.0662 - lr: 1.0000e-04\n",
      "Epoch 90/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0702 - mae: 0.0730\n",
      "Epoch 90: val_loss improved from 0.06978 to 0.06792, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0702 - mae: 0.0730 - val_loss: 0.0679 - val_mae: 0.0646 - lr: 1.0000e-04\n",
      "Epoch 91/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0686 - mae: 0.0730\n",
      "Epoch 91: val_loss improved from 0.06792 to 0.06625, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0686 - mae: 0.0730 - val_loss: 0.0662 - val_mae: 0.0641 - lr: 1.0000e-04\n",
      "Epoch 92/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0661 - mae: 0.0698\n",
      "Epoch 92: val_loss improved from 0.06625 to 0.06444, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0661 - mae: 0.0698 - val_loss: 0.0644 - val_mae: 0.0628 - lr: 1.0000e-04\n",
      "Epoch 93/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0644 - mae: 0.0694\n",
      "Epoch 93: val_loss improved from 0.06444 to 0.06264, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0644 - mae: 0.0694 - val_loss: 0.0626 - val_mae: 0.0622 - lr: 1.0000e-04\n",
      "Epoch 94/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0627 - mae: 0.0684\n",
      "Epoch 94: val_loss improved from 0.06264 to 0.06120, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0627 - mae: 0.0684 - val_loss: 0.0612 - val_mae: 0.0630 - lr: 1.0000e-04\n",
      "Epoch 95/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0616 - mae: 0.0698\n",
      "Epoch 95: val_loss improved from 0.06120 to 0.05938, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0616 - mae: 0.0699 - val_loss: 0.0594 - val_mae: 0.0613 - lr: 1.0000e-04\n",
      "Epoch 96/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0595 - mae: 0.0673\n",
      "Epoch 96: val_loss improved from 0.05938 to 0.05777, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0595 - mae: 0.0672 - val_loss: 0.0578 - val_mae: 0.0616 - lr: 1.0000e-04\n",
      "Epoch 97/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0579 - mae: 0.0677\n",
      "Epoch 97: val_loss improved from 0.05777 to 0.05645, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0579 - mae: 0.0678 - val_loss: 0.0565 - val_mae: 0.0628 - lr: 1.0000e-04\n",
      "Epoch 98/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0560 - mae: 0.0664\n",
      "Epoch 98: val_loss improved from 0.05645 to 0.05465, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0560 - mae: 0.0664 - val_loss: 0.0546 - val_mae: 0.0611 - lr: 1.0000e-04\n",
      "Epoch 99/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0542 - mae: 0.0646\n",
      "Epoch 99: val_loss improved from 0.05465 to 0.05288, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0542 - mae: 0.0646 - val_loss: 0.0529 - val_mae: 0.0598 - lr: 1.0000e-04\n",
      "Epoch 100/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0525 - mae: 0.0643\n",
      "Epoch 100: val_loss improved from 0.05288 to 0.05131, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0525 - mae: 0.0643 - val_loss: 0.0513 - val_mae: 0.0595 - lr: 1.0000e-04\n",
      "Epoch 101/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0511 - mae: 0.0644\n",
      "Epoch 101: val_loss improved from 0.05131 to 0.05020, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0511 - mae: 0.0644 - val_loss: 0.0502 - val_mae: 0.0619 - lr: 1.0000e-04\n",
      "Epoch 102/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0496 - mae: 0.0650\n",
      "Epoch 102: val_loss improved from 0.05020 to 0.04845, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0496 - mae: 0.0650 - val_loss: 0.0484 - val_mae: 0.0603 - lr: 1.0000e-04\n",
      "Epoch 103/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0478 - mae: 0.0637\n",
      "Epoch 103: val_loss improved from 0.04845 to 0.04653, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0478 - mae: 0.0637 - val_loss: 0.0465 - val_mae: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 104/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0462 - mae: 0.0628\n",
      "Epoch 104: val_loss improved from 0.04653 to 0.04525, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0462 - mae: 0.0629 - val_loss: 0.0453 - val_mae: 0.0593 - lr: 1.0000e-04\n",
      "Epoch 105/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0445 - mae: 0.0613\n",
      "Epoch 105: val_loss improved from 0.04525 to 0.04362, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0445 - mae: 0.0613 - val_loss: 0.0436 - val_mae: 0.0577 - lr: 1.0000e-04\n",
      "Epoch 106/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0434 - mae: 0.0626\n",
      "Epoch 106: val_loss improved from 0.04362 to 0.04209, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0434 - mae: 0.0626 - val_loss: 0.0421 - val_mae: 0.0576 - lr: 1.0000e-04\n",
      "Epoch 107/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0413 - mae: 0.0599\n",
      "Epoch 107: val_loss improved from 0.04209 to 0.04080, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0413 - mae: 0.0599 - val_loss: 0.0408 - val_mae: 0.0580 - lr: 1.0000e-04\n",
      "Epoch 108/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0400 - mae: 0.0597\n",
      "Epoch 108: val_loss improved from 0.04080 to 0.03946, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0400 - mae: 0.0597 - val_loss: 0.0395 - val_mae: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 109/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0385 - mae: 0.0596\n",
      "Epoch 109: val_loss improved from 0.03946 to 0.03814, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0385 - mae: 0.0596 - val_loss: 0.0381 - val_mae: 0.0576 - lr: 1.0000e-04\n",
      "Epoch 110/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0375 - mae: 0.0602\n",
      "Epoch 110: val_loss improved from 0.03814 to 0.03686, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 29ms/step - loss: 0.0375 - mae: 0.0602 - val_loss: 0.0369 - val_mae: 0.0578 - lr: 1.0000e-04\n",
      "Epoch 111/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0361 - mae: 0.0596\n",
      "Epoch 111: val_loss improved from 0.03686 to 0.03545, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 6s 32ms/step - loss: 0.0361 - mae: 0.0596 - val_loss: 0.0355 - val_mae: 0.0571 - lr: 1.0000e-04\n",
      "Epoch 112/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0345 - mae: 0.0580\n",
      "Epoch 112: val_loss improved from 0.03545 to 0.03412, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 6s 32ms/step - loss: 0.0345 - mae: 0.0581 - val_loss: 0.0341 - val_mae: 0.0561 - lr: 1.0000e-04\n",
      "Epoch 113/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0334 - mae: 0.0581\n",
      "Epoch 113: val_loss improved from 0.03412 to 0.03289, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 26ms/step - loss: 0.0335 - mae: 0.0582 - val_loss: 0.0329 - val_mae: 0.0560 - lr: 1.0000e-04\n",
      "Epoch 114/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0319 - mae: 0.0562\n",
      "Epoch 114: val_loss improved from 0.03289 to 0.03162, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 28ms/step - loss: 0.0319 - mae: 0.0562 - val_loss: 0.0316 - val_mae: 0.0548 - lr: 1.0000e-04\n",
      "Epoch 115/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0311 - mae: 0.0574\n",
      "Epoch 115: val_loss improved from 0.03162 to 0.03077, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0311 - mae: 0.0574 - val_loss: 0.0308 - val_mae: 0.0563 - lr: 1.0000e-04\n",
      "Epoch 116/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0302 - mae: 0.0586\n",
      "Epoch 116: val_loss improved from 0.03077 to 0.02931, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0302 - mae: 0.0586 - val_loss: 0.0293 - val_mae: 0.0531 - lr: 1.0000e-04\n",
      "Epoch 117/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0287 - mae: 0.0566\n",
      "Epoch 117: val_loss improved from 0.02931 to 0.02834, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0287 - mae: 0.0566 - val_loss: 0.0283 - val_mae: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 118/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0279 - mae: 0.0566\n",
      "Epoch 118: val_loss improved from 0.02834 to 0.02717, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0279 - mae: 0.0566 - val_loss: 0.0272 - val_mae: 0.0526 - lr: 1.0000e-04\n",
      "Epoch 119/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0267 - mae: 0.0555\n",
      "Epoch 119: val_loss improved from 0.02717 to 0.02640, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0267 - mae: 0.0556 - val_loss: 0.0264 - val_mae: 0.0534 - lr: 1.0000e-04\n",
      "Epoch 120/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0258 - mae: 0.0555\n",
      "Epoch 120: val_loss improved from 0.02640 to 0.02546, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0258 - mae: 0.0555 - val_loss: 0.0255 - val_mae: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 121/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0249 - mae: 0.0556\n",
      "Epoch 121: val_loss improved from 0.02546 to 0.02470, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0249 - mae: 0.0556 - val_loss: 0.0247 - val_mae: 0.0536 - lr: 1.0000e-04\n",
      "Epoch 122/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0239 - mae: 0.0543\n",
      "Epoch 122: val_loss improved from 0.02470 to 0.02360, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0239 - mae: 0.0543 - val_loss: 0.0236 - val_mae: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 123/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0232 - mae: 0.0544\n",
      "Epoch 123: val_loss improved from 0.02360 to 0.02322, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0232 - mae: 0.0544 - val_loss: 0.0232 - val_mae: 0.0542 - lr: 1.0000e-04\n",
      "Epoch 124/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0225 - mae: 0.0549\n",
      "Epoch 124: val_loss improved from 0.02322 to 0.02221, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0225 - mae: 0.0550 - val_loss: 0.0222 - val_mae: 0.0522 - lr: 1.0000e-04\n",
      "Epoch 125/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0213 - mae: 0.0525\n",
      "Epoch 125: val_loss improved from 0.02221 to 0.02145, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0213 - mae: 0.0525 - val_loss: 0.0214 - val_mae: 0.0514 - lr: 1.0000e-04\n",
      "Epoch 126/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0206 - mae: 0.0528\n",
      "Epoch 126: val_loss improved from 0.02145 to 0.02110, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0206 - mae: 0.0528 - val_loss: 0.0211 - val_mae: 0.0543 - lr: 1.0000e-04\n",
      "Epoch 127/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0200 - mae: 0.0527\n",
      "Epoch 127: val_loss improved from 0.02110 to 0.02004, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0200 - mae: 0.0527 - val_loss: 0.0200 - val_mae: 0.0512 - lr: 1.0000e-04\n",
      "Epoch 128/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0195 - mae: 0.0530\n",
      "Epoch 128: val_loss improved from 0.02004 to 0.01928, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0196 - mae: 0.0531 - val_loss: 0.0193 - val_mae: 0.0494 - lr: 1.0000e-04\n",
      "Epoch 129/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0189 - mae: 0.0530\n",
      "Epoch 129: val_loss improved from 0.01928 to 0.01885, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0189 - mae: 0.0529 - val_loss: 0.0189 - val_mae: 0.0511 - lr: 1.0000e-04\n",
      "Epoch 130/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0182 - mae: 0.0521\n",
      "Epoch 130: val_loss improved from 0.01885 to 0.01853, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0182 - mae: 0.0522 - val_loss: 0.0185 - val_mae: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 131/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0173 - mae: 0.0512\n",
      "Epoch 131: val_loss improved from 0.01853 to 0.01730, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0173 - mae: 0.0512 - val_loss: 0.0173 - val_mae: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 132/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0170 - mae: 0.0518\n",
      "Epoch 132: val_loss did not improve from 0.01730\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0170 - mae: 0.0518 - val_loss: 0.0174 - val_mae: 0.0521 - lr: 1.0000e-04\n",
      "Epoch 133/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0162 - mae: 0.0505\n",
      "Epoch 133: val_loss improved from 0.01730 to 0.01671, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0162 - mae: 0.0506 - val_loss: 0.0167 - val_mae: 0.0507 - lr: 1.0000e-04\n",
      "Epoch 134/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0161 - mae: 0.0521\n",
      "Epoch 134: val_loss improved from 0.01671 to 0.01608, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0161 - mae: 0.0522 - val_loss: 0.0161 - val_mae: 0.0492 - lr: 1.0000e-04\n",
      "Epoch 135/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0156 - mae: 0.0523\n",
      "Epoch 135: val_loss improved from 0.01608 to 0.01552, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0156 - mae: 0.0524 - val_loss: 0.0155 - val_mae: 0.0484 - lr: 1.0000e-04\n",
      "Epoch 136/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0150 - mae: 0.0512\n",
      "Epoch 136: val_loss improved from 0.01552 to 0.01531, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0150 - mae: 0.0512 - val_loss: 0.0153 - val_mae: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 137/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0147 - mae: 0.0513\n",
      "Epoch 137: val_loss improved from 0.01531 to 0.01460, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0147 - mae: 0.0513 - val_loss: 0.0146 - val_mae: 0.0472 - lr: 1.0000e-04\n",
      "Epoch 138/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0143 - mae: 0.0514\n",
      "Epoch 138: val_loss did not improve from 0.01460\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0143 - mae: 0.0515 - val_loss: 0.0147 - val_mae: 0.0516 - lr: 1.0000e-04\n",
      "Epoch 139/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0141 - mae: 0.0520\n",
      "Epoch 139: val_loss improved from 0.01460 to 0.01386, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0141 - mae: 0.0521 - val_loss: 0.0139 - val_mae: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 140/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0136 - mae: 0.0520\n",
      "Epoch 140: val_loss improved from 0.01386 to 0.01337, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0136 - mae: 0.0520 - val_loss: 0.0134 - val_mae: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 141/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0132 - mae: 0.0506\n",
      "Epoch 141: val_loss did not improve from 0.01337\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0132 - mae: 0.0506 - val_loss: 0.0137 - val_mae: 0.0508 - lr: 1.0000e-04\n",
      "Epoch 142/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0130 - mae: 0.0512\n",
      "Epoch 142: val_loss improved from 0.01337 to 0.01315, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0130 - mae: 0.0512 - val_loss: 0.0132 - val_mae: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 143/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0123 - mae: 0.0501\n",
      "Epoch 143: val_loss improved from 0.01315 to 0.01238, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0123 - mae: 0.0501 - val_loss: 0.0124 - val_mae: 0.0452 - lr: 1.0000e-04\n",
      "Epoch 144/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0124 - mae: 0.0512\n",
      "Epoch 144: val_loss improved from 0.01238 to 0.01234, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0124 - mae: 0.0512 - val_loss: 0.0123 - val_mae: 0.0473 - lr: 1.0000e-04\n",
      "Epoch 145/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0122 - mae: 0.0517\n",
      "Epoch 145: val_loss improved from 0.01234 to 0.01199, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0122 - mae: 0.0517 - val_loss: 0.0120 - val_mae: 0.0467 - lr: 1.0000e-04\n",
      "Epoch 146/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0120 - mae: 0.0513\n",
      "Epoch 146: val_loss improved from 0.01199 to 0.01188, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0120 - mae: 0.0514 - val_loss: 0.0119 - val_mae: 0.0476 - lr: 1.0000e-04\n",
      "Epoch 147/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0113 - mae: 0.0493\n",
      "Epoch 147: val_loss improved from 0.01188 to 0.01177, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0113 - mae: 0.0494 - val_loss: 0.0118 - val_mae: 0.0482 - lr: 1.0000e-04\n",
      "Epoch 148/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0111 - mae: 0.0495\n",
      "Epoch 148: val_loss improved from 0.01177 to 0.01132, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0111 - mae: 0.0495 - val_loss: 0.0113 - val_mae: 0.0460 - lr: 1.0000e-04\n",
      "Epoch 149/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0487\n",
      "Epoch 149: val_loss improved from 0.01132 to 0.01116, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0107 - mae: 0.0487 - val_loss: 0.0112 - val_mae: 0.0461 - lr: 1.0000e-04\n",
      "Epoch 150/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0107 - mae: 0.0498\n",
      "Epoch 150: val_loss improved from 0.01116 to 0.01081, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0107 - mae: 0.0498 - val_loss: 0.0108 - val_mae: 0.0454 - lr: 1.0000e-04\n",
      "Epoch 151/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0106 - mae: 0.0499\n",
      "Epoch 151: val_loss did not improve from 0.01081\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0106 - mae: 0.0500 - val_loss: 0.0108 - val_mae: 0.0464 - lr: 1.0000e-04\n",
      "Epoch 152/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0104 - mae: 0.0505\n",
      "Epoch 152: val_loss improved from 0.01081 to 0.01047, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0105 - mae: 0.0505 - val_loss: 0.0105 - val_mae: 0.0446 - lr: 1.0000e-04\n",
      "Epoch 153/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0102 - mae: 0.0500\n",
      "Epoch 153: val_loss did not improve from 0.01047\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0102 - mae: 0.0500 - val_loss: 0.0105 - val_mae: 0.0470 - lr: 1.0000e-04\n",
      "Epoch 154/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0101 - mae: 0.0507\n",
      "Epoch 154: val_loss improved from 0.01047 to 0.01032, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0101 - mae: 0.0507 - val_loss: 0.0103 - val_mae: 0.0459 - lr: 1.0000e-04\n",
      "Epoch 155/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0464\n",
      "Epoch 155: val_loss improved from 0.01032 to 0.01007, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0092 - mae: 0.0464 - val_loss: 0.0101 - val_mae: 0.0451 - lr: 1.0000e-04\n",
      "Epoch 156/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0095 - mae: 0.0490\n",
      "Epoch 156: val_loss improved from 0.01007 to 0.00979, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0095 - mae: 0.0490 - val_loss: 0.0098 - val_mae: 0.0438 - lr: 1.0000e-04\n",
      "Epoch 157/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0092 - mae: 0.0477\n",
      "Epoch 157: val_loss improved from 0.00979 to 0.00960, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0092 - mae: 0.0477 - val_loss: 0.0096 - val_mae: 0.0432 - lr: 1.0000e-04\n",
      "Epoch 158/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0093 - mae: 0.0488\n",
      "Epoch 158: val_loss did not improve from 0.00960\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0093 - mae: 0.0488 - val_loss: 0.0097 - val_mae: 0.0462 - lr: 1.0000e-04\n",
      "Epoch 159/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0092 - mae: 0.0495\n",
      "Epoch 159: val_loss did not improve from 0.00960\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0092 - mae: 0.0495 - val_loss: 0.0097 - val_mae: 0.0469 - lr: 1.0000e-04\n",
      "Epoch 160/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0088 - mae: 0.0476\n",
      "Epoch 160: val_loss improved from 0.00960 to 0.00907, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0088 - mae: 0.0477 - val_loss: 0.0091 - val_mae: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 161/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0086 - mae: 0.0470\n",
      "Epoch 161: val_loss improved from 0.00907 to 0.00901, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0086 - mae: 0.0471 - val_loss: 0.0090 - val_mae: 0.0419 - lr: 1.0000e-04\n",
      "Epoch 162/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0469\n",
      "Epoch 162: val_loss improved from 0.00901 to 0.00891, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0085 - mae: 0.0470 - val_loss: 0.0089 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 163/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0482\n",
      "Epoch 163: val_loss improved from 0.00891 to 0.00881, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0085 - mae: 0.0482 - val_loss: 0.0088 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 164/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0084 - mae: 0.0483\n",
      "Epoch 164: val_loss did not improve from 0.00881\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0084 - mae: 0.0483 - val_loss: 0.0090 - val_mae: 0.0459 - lr: 1.0000e-04\n",
      "Epoch 165/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0085 - mae: 0.0489\n",
      "Epoch 165: val_loss improved from 0.00881 to 0.00860, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0085 - mae: 0.0489 - val_loss: 0.0086 - val_mae: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 166/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0467\n",
      "Epoch 166: val_loss improved from 0.00860 to 0.00848, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0080 - mae: 0.0467 - val_loss: 0.0085 - val_mae: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 167/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0477\n",
      "Epoch 167: val_loss improved from 0.00848 to 0.00832, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0081 - mae: 0.0477 - val_loss: 0.0083 - val_mae: 0.0414 - lr: 1.0000e-04\n",
      "Epoch 168/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0082 - mae: 0.0486\n",
      "Epoch 168: val_loss improved from 0.00832 to 0.00832, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0082 - mae: 0.0486 - val_loss: 0.0083 - val_mae: 0.0415 - lr: 1.0000e-04\n",
      "Epoch 169/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0081 - mae: 0.0480\n",
      "Epoch 169: val_loss improved from 0.00832 to 0.00825, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0081 - mae: 0.0480 - val_loss: 0.0083 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 170/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0473\n",
      "Epoch 170: val_loss did not improve from 0.00825\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0078 - mae: 0.0473 - val_loss: 0.0086 - val_mae: 0.0463 - lr: 1.0000e-04\n",
      "Epoch 171/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0476\n",
      "Epoch 171: val_loss improved from 0.00825 to 0.00825, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0078 - mae: 0.0477 - val_loss: 0.0082 - val_mae: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 172/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0475\n",
      "Epoch 172: val_loss improved from 0.00825 to 0.00805, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0078 - mae: 0.0475 - val_loss: 0.0081 - val_mae: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 173/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0077 - mae: 0.0473\n",
      "Epoch 173: val_loss did not improve from 0.00805\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0077 - mae: 0.0473 - val_loss: 0.0081 - val_mae: 0.0439 - lr: 1.0000e-04\n",
      "Epoch 174/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0078 - mae: 0.0485\n",
      "Epoch 174: val_loss improved from 0.00805 to 0.00790, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0078 - mae: 0.0485 - val_loss: 0.0079 - val_mae: 0.0416 - lr: 1.0000e-04\n",
      "Epoch 175/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0078 - mae: 0.0490\n",
      "Epoch 175: val_loss did not improve from 0.00790\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0078 - mae: 0.0490 - val_loss: 0.0079 - val_mae: 0.0420 - lr: 1.0000e-04\n",
      "Epoch 176/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0073 - mae: 0.0466\n",
      "Epoch 176: val_loss improved from 0.00790 to 0.00789, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0073 - mae: 0.0466 - val_loss: 0.0079 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 177/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0458\n",
      "Epoch 177: val_loss improved from 0.00789 to 0.00767, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0072 - mae: 0.0459 - val_loss: 0.0077 - val_mae: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 178/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0478\n",
      "Epoch 178: val_loss did not improve from 0.00767\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0072 - mae: 0.0478 - val_loss: 0.0078 - val_mae: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 179/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0074 - mae: 0.0483\n",
      "Epoch 179: val_loss improved from 0.00767 to 0.00760, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0074 - mae: 0.0483 - val_loss: 0.0076 - val_mae: 0.0417 - lr: 1.0000e-04\n",
      "Epoch 180/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0072 - mae: 0.0478\n",
      "Epoch 180: val_loss did not improve from 0.00760\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0072 - mae: 0.0478 - val_loss: 0.0079 - val_mae: 0.0437 - lr: 1.0000e-04\n",
      "Epoch 181/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0464\n",
      "Epoch 181: val_loss improved from 0.00760 to 0.00748, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0070 - mae: 0.0464 - val_loss: 0.0075 - val_mae: 0.0402 - lr: 1.0000e-04\n",
      "Epoch 182/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0468\n",
      "Epoch 182: val_loss did not improve from 0.00748\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0070 - mae: 0.0468 - val_loss: 0.0075 - val_mae: 0.0415 - lr: 1.0000e-04\n",
      "Epoch 183/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0069 - mae: 0.0470\n",
      "Epoch 183: val_loss improved from 0.00748 to 0.00731, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0070 - mae: 0.0471 - val_loss: 0.0073 - val_mae: 0.0407 - lr: 1.0000e-04\n",
      "Epoch 184/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0456\n",
      "Epoch 184: val_loss improved from 0.00731 to 0.00727, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0068 - mae: 0.0457 - val_loss: 0.0073 - val_mae: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 185/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0451\n",
      "Epoch 185: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0067 - mae: 0.0451 - val_loss: 0.0074 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 186/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0068 - mae: 0.0462\n",
      "Epoch 186: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0068 - mae: 0.0462 - val_loss: 0.0074 - val_mae: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 187/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0465\n",
      "Epoch 187: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0067 - mae: 0.0465 - val_loss: 0.0074 - val_mae: 0.0428 - lr: 1.0000e-04\n",
      "Epoch 188/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0463\n",
      "Epoch 188: val_loss did not improve from 0.00727\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0067 - mae: 0.0463 - val_loss: 0.0074 - val_mae: 0.0424 - lr: 1.0000e-04\n",
      "Epoch 189/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0471\n",
      "Epoch 189: val_loss improved from 0.00727 to 0.00719, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0068 - mae: 0.0471 - val_loss: 0.0072 - val_mae: 0.0406 - lr: 1.0000e-04\n",
      "Epoch 190/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0469\n",
      "Epoch 190: val_loss improved from 0.00719 to 0.00711, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0067 - mae: 0.0468 - val_loss: 0.0071 - val_mae: 0.0397 - lr: 1.0000e-04\n",
      "Epoch 191/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0470\n",
      "Epoch 191: val_loss did not improve from 0.00711\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0067 - mae: 0.0470 - val_loss: 0.0073 - val_mae: 0.0409 - lr: 1.0000e-04\n",
      "Epoch 192/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0064 - mae: 0.0452\n",
      "Epoch 192: val_loss did not improve from 0.00711\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0064 - mae: 0.0452 - val_loss: 0.0072 - val_mae: 0.0410 - lr: 1.0000e-04\n",
      "Epoch 193/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0068 - mae: 0.0480\n",
      "Epoch 193: val_loss improved from 0.00711 to 0.00710, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0068 - mae: 0.0482 - val_loss: 0.0071 - val_mae: 0.0408 - lr: 1.0000e-04\n",
      "Epoch 194/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0064 - mae: 0.0458\n",
      "Epoch 194: val_loss did not improve from 0.00710\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0064 - mae: 0.0458 - val_loss: 0.0073 - val_mae: 0.0422 - lr: 1.0000e-04\n",
      "Epoch 195/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0066 - mae: 0.0474\n",
      "Epoch 195: val_loss did not improve from 0.00710\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0066 - mae: 0.0474 - val_loss: 0.0072 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 196/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0452\n",
      "Epoch 196: val_loss improved from 0.00710 to 0.00690, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0062 - mae: 0.0451 - val_loss: 0.0069 - val_mae: 0.0390 - lr: 5.0000e-05\n",
      "Epoch 197/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0063 - mae: 0.0456\n",
      "Epoch 197: val_loss improved from 0.00690 to 0.00685, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0063 - mae: 0.0456 - val_loss: 0.0069 - val_mae: 0.0403 - lr: 5.0000e-05\n",
      "Epoch 198/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0062 - mae: 0.0452\n",
      "Epoch 198: val_loss did not improve from 0.00685\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0062 - mae: 0.0453 - val_loss: 0.0070 - val_mae: 0.0412 - lr: 5.0000e-05\n",
      "Epoch 199/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0060 - mae: 0.0444\n",
      "Epoch 199: val_loss did not improve from 0.00685\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0060 - mae: 0.0444 - val_loss: 0.0069 - val_mae: 0.0397 - lr: 5.0000e-05\n",
      "Epoch 200/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0429\n",
      "Epoch 200: val_loss improved from 0.00685 to 0.00672, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0058 - mae: 0.0430 - val_loss: 0.0067 - val_mae: 0.0386 - lr: 5.0000e-05\n",
      "Epoch 201/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0431\n",
      "Epoch 201: val_loss improved from 0.00672 to 0.00660, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0057 - mae: 0.0431 - val_loss: 0.0066 - val_mae: 0.0376 - lr: 5.0000e-05\n",
      "Epoch 202/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0062 - mae: 0.0468\n",
      "Epoch 202: val_loss did not improve from 0.00660\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0062 - mae: 0.0468 - val_loss: 0.0067 - val_mae: 0.0394 - lr: 5.0000e-05\n",
      "Epoch 203/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0060 - mae: 0.0450\n",
      "Epoch 203: val_loss did not improve from 0.00660\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0060 - mae: 0.0450 - val_loss: 0.0067 - val_mae: 0.0397 - lr: 5.0000e-05\n",
      "Epoch 204/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0063 - mae: 0.0464\n",
      "Epoch 204: val_loss improved from 0.00660 to 0.00655, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0063 - mae: 0.0465 - val_loss: 0.0065 - val_mae: 0.0381 - lr: 5.0000e-05\n",
      "Epoch 205/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0442\n",
      "Epoch 205: val_loss did not improve from 0.00655\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0059 - mae: 0.0444 - val_loss: 0.0066 - val_mae: 0.0397 - lr: 5.0000e-05\n",
      "Epoch 206/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0438\n",
      "Epoch 206: val_loss improved from 0.00655 to 0.00644, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0057 - mae: 0.0438 - val_loss: 0.0064 - val_mae: 0.0377 - lr: 5.0000e-05\n",
      "Epoch 207/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0436\n",
      "Epoch 207: val_loss did not improve from 0.00644\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0057 - mae: 0.0436 - val_loss: 0.0067 - val_mae: 0.0416 - lr: 5.0000e-05\n",
      "Epoch 208/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0058 - mae: 0.0448\n",
      "Epoch 208: val_loss did not improve from 0.00644\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0058 - mae: 0.0448 - val_loss: 0.0065 - val_mae: 0.0390 - lr: 5.0000e-05\n",
      "Epoch 209/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0451\n",
      "Epoch 209: val_loss did not improve from 0.00644\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0058 - mae: 0.0451 - val_loss: 0.0066 - val_mae: 0.0397 - lr: 5.0000e-05\n",
      "Epoch 210/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0056 - mae: 0.0441\n",
      "Epoch 210: val_loss improved from 0.00644 to 0.00642, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0056 - mae: 0.0441 - val_loss: 0.0064 - val_mae: 0.0380 - lr: 5.0000e-05\n",
      "Epoch 211/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0441\n",
      "Epoch 211: val_loss did not improve from 0.00642\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0056 - mae: 0.0441 - val_loss: 0.0066 - val_mae: 0.0402 - lr: 5.0000e-05\n",
      "Epoch 212/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0057 - mae: 0.0447\n",
      "Epoch 212: val_loss did not improve from 0.00642\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0057 - mae: 0.0447 - val_loss: 0.0065 - val_mae: 0.0390 - lr: 5.0000e-05\n",
      "Epoch 213/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0058 - mae: 0.0454\n",
      "Epoch 213: val_loss improved from 0.00642 to 0.00640, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0059 - mae: 0.0458 - val_loss: 0.0064 - val_mae: 0.0380 - lr: 2.5000e-05\n",
      "Epoch 214/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0423\n",
      "Epoch 214: val_loss improved from 0.00640 to 0.00635, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 25ms/step - loss: 0.0054 - mae: 0.0424 - val_loss: 0.0064 - val_mae: 0.0379 - lr: 2.5000e-05\n",
      "Epoch 215/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0056 - mae: 0.0447\n",
      "Epoch 215: val_loss did not improve from 0.00635\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0056 - mae: 0.0446 - val_loss: 0.0064 - val_mae: 0.0376 - lr: 2.5000e-05\n",
      "Epoch 216/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0429\n",
      "Epoch 216: val_loss improved from 0.00635 to 0.00626, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0054 - mae: 0.0429 - val_loss: 0.0063 - val_mae: 0.0374 - lr: 2.5000e-05\n",
      "Epoch 217/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0413\n",
      "Epoch 217: val_loss improved from 0.00626 to 0.00625, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 5s 27ms/step - loss: 0.0052 - mae: 0.0413 - val_loss: 0.0063 - val_mae: 0.0379 - lr: 2.5000e-05\n",
      "Epoch 218/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0415\n",
      "Epoch 218: val_loss did not improve from 0.00625\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0051 - mae: 0.0416 - val_loss: 0.0063 - val_mae: 0.0391 - lr: 2.5000e-05\n",
      "Epoch 219/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0438\n",
      "Epoch 219: val_loss improved from 0.00625 to 0.00617, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0438 - val_loss: 0.0062 - val_mae: 0.0376 - lr: 2.5000e-05\n",
      "Epoch 220/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0436\n",
      "Epoch 220: val_loss improved from 0.00617 to 0.00609, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0054 - mae: 0.0436 - val_loss: 0.0061 - val_mae: 0.0370 - lr: 2.5000e-05\n",
      "Epoch 221/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0053 - mae: 0.0426\n",
      "Epoch 221: val_loss improved from 0.00609 to 0.00605, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 2.5000e-05\n",
      "Epoch 222/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0439\n",
      "Epoch 222: val_loss did not improve from 0.00605\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0054 - mae: 0.0439 - val_loss: 0.0061 - val_mae: 0.0375 - lr: 2.5000e-05\n",
      "Epoch 223/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0425\n",
      "Epoch 223: val_loss did not improve from 0.00605\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0052 - mae: 0.0426 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 2.5000e-05\n",
      "Epoch 224/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0425\n",
      "Epoch 224: val_loss improved from 0.00605 to 0.00601, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0052 - mae: 0.0426 - val_loss: 0.0060 - val_mae: 0.0370 - lr: 2.5000e-05\n",
      "Epoch 225/250\n",
      "180/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0425\n",
      "Epoch 225: val_loss improved from 0.00601 to 0.00600, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0052 - mae: 0.0425 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 2.5000e-05\n",
      "Epoch 226/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0054 - mae: 0.0450\n",
      "Epoch 226: val_loss did not improve from 0.00600\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0055 - mae: 0.0451 - val_loss: 0.0061 - val_mae: 0.0380 - lr: 2.5000e-05\n",
      "Epoch 227/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0424\n",
      "Epoch 227: val_loss did not improve from 0.00600\n",
      "183/183 [==============================] - 4s 23ms/step - loss: 0.0052 - mae: 0.0425 - val_loss: 0.0060 - val_mae: 0.0376 - lr: 1.2500e-05\n",
      "Epoch 228/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0054 - mae: 0.0445\n",
      "Epoch 228: val_loss improved from 0.00600 to 0.00595, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0054 - mae: 0.0445 - val_loss: 0.0059 - val_mae: 0.0367 - lr: 1.2500e-05\n",
      "Epoch 229/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0443\n",
      "Epoch 229: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0053 - mae: 0.0441 - val_loss: 0.0060 - val_mae: 0.0369 - lr: 1.2500e-05\n",
      "Epoch 230/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0435\n",
      "Epoch 230: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0052 - mae: 0.0434 - val_loss: 0.0061 - val_mae: 0.0378 - lr: 1.2500e-05\n",
      "Epoch 231/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0430\n",
      "Epoch 231: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0051 - mae: 0.0431 - val_loss: 0.0060 - val_mae: 0.0373 - lr: 1.2500e-05\n",
      "Epoch 232/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0050 - mae: 0.0420\n",
      "Epoch 232: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 24ms/step - loss: 0.0050 - mae: 0.0420 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 1.2500e-05\n",
      "Epoch 233/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0416\n",
      "Epoch 233: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0050 - mae: 0.0416 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 1.2500e-05\n",
      "Epoch 234/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0421\n",
      "Epoch 234: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0051 - mae: 0.0421 - val_loss: 0.0060 - val_mae: 0.0373 - lr: 1.2500e-05\n",
      "Epoch 235/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0415\n",
      "Epoch 235: val_loss did not improve from 0.00595\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0415 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 6.2500e-06\n",
      "Epoch 236/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0438\n",
      "Epoch 236: val_loss improved from 0.00595 to 0.00592, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0052 - mae: 0.0438 - val_loss: 0.0059 - val_mae: 0.0367 - lr: 6.2500e-06\n",
      "Epoch 237/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0429\n",
      "Epoch 237: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0051 - mae: 0.0428 - val_loss: 0.0059 - val_mae: 0.0368 - lr: 6.2500e-06\n",
      "Epoch 238/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0422\n",
      "Epoch 238: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0422 - val_loss: 0.0059 - val_mae: 0.0364 - lr: 6.2500e-06\n",
      "Epoch 239/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0414\n",
      "Epoch 239: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0050 - mae: 0.0414 - val_loss: 0.0059 - val_mae: 0.0367 - lr: 6.2500e-06\n",
      "Epoch 240/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0421\n",
      "Epoch 240: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0050 - mae: 0.0422 - val_loss: 0.0060 - val_mae: 0.0366 - lr: 6.2500e-06\n",
      "Epoch 241/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0432\n",
      "Epoch 241: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0052 - mae: 0.0432 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 3.1250e-06\n",
      "Epoch 242/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0435\n",
      "Epoch 242: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0053 - mae: 0.0436 - val_loss: 0.0059 - val_mae: 0.0367 - lr: 3.1250e-06\n",
      "Epoch 243/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0049 - mae: 0.0420\n",
      "Epoch 243: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0049 - mae: 0.0420 - val_loss: 0.0060 - val_mae: 0.0367 - lr: 3.1250e-06\n",
      "Epoch 244/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0436\n",
      "Epoch 244: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0052 - mae: 0.0436 - val_loss: 0.0060 - val_mae: 0.0364 - lr: 3.1250e-06\n",
      "Epoch 245/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0438\n",
      "Epoch 245: val_loss did not improve from 0.00592\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0052 - mae: 0.0438 - val_loss: 0.0060 - val_mae: 0.0368 - lr: 3.1250e-06\n",
      "Epoch 246/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0407\n",
      "Epoch 246: val_loss improved from 0.00592 to 0.00587, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0048 - mae: 0.0408 - val_loss: 0.0059 - val_mae: 0.0369 - lr: 3.1250e-06\n",
      "Epoch 247/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0422\n",
      "Epoch 247: val_loss improved from 0.00587 to 0.00587, saving model to models\\date_predictor-v25_fold5.h5\n",
      "183/183 [==============================] - 4s 19ms/step - loss: 0.0051 - mae: 0.0422 - val_loss: 0.0059 - val_mae: 0.0366 - lr: 1.5625e-06\n",
      "Epoch 248/250\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0428\n",
      "Epoch 248: val_loss did not improve from 0.00587\n",
      "183/183 [==============================] - 4s 20ms/step - loss: 0.0051 - mae: 0.0428 - val_loss: 0.0059 - val_mae: 0.0365 - lr: 1.5625e-06\n",
      "Epoch 249/250\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.0049 - mae: 0.0418\n",
      "Epoch 249: val_loss did not improve from 0.00587\n",
      "183/183 [==============================] - 4s 22ms/step - loss: 0.0049 - mae: 0.0418 - val_loss: 0.0059 - val_mae: 0.0366 - lr: 1.5625e-06\n",
      "Epoch 250/250\n",
      "181/183 [============================>.] - ETA: 0s - loss: 0.0050 - mae: 0.0422\n",
      "Epoch 250: val_loss did not improve from 0.00587\n",
      "183/183 [==============================] - 4s 21ms/step - loss: 0.0050 - mae: 0.0423 - val_loss: 0.0059 - val_mae: 0.0366 - lr: 1.5625e-06\n",
      "46/46 [==============================] - 0s 3ms/step\n",
      "Fold 5 MAE: 55.0732991592313\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average MAE: 56.254435962290074\n",
      "Standard Deviation of MAE: 1.3483842686266592\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define K-Fold Cross-Validation Parameters\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for fold performance metrics\n",
    "fold_mae_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train2)):  # Replace with your full dataset variables\n",
    "    print(f\"\\n--- Training Fold {fold + 1}/{k} ---\")\n",
    "    \n",
    "    # Splitting data for this fold\n",
    "    X_train_fold, X_val_fold = X_train2[train_index], X_train2[val_index]\n",
    "    y_train_fold, y_val_fold = y_train2[train_index], y_train2[val_index]\n",
    "    \n",
    "    # Create scalers\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    # Scale features (X)\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_fold)\n",
    "    X_val_scaled = scaler_X.transform(X_val_fold)\n",
    "    \n",
    "    # Scale target to [0, 1] range (y)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train_fold.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = scaler_y.transform(y_val_fold.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Define the model\n",
    "    model = create_model(input_dim=X_train_scaled.shape[1], learning_rate=1e-4)\n",
    "\n",
    "    # Define callbacks\n",
    "    fold_model_file = f\"{os.path.splitext(file_name)[0]}_fold{fold + 1}{os.path.splitext(file_name)[1]}\"\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=30,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=6, \n",
    "            min_lr=1e-8\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=fold_model_file,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        validation_data=(X_val_scaled, y_val_scaled),\n",
    "        epochs=250,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    y_val_pred_scaled = model.predict(X_val_scaled)\n",
    "    y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)\n",
    "    y_val_actual = scaler_y.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate MAE for this fold\n",
    "    mae_score = np.mean(np.abs(y_val_pred.flatten() - y_val_actual))\n",
    "    fold_mae_scores.append(mae_score)\n",
    "    print(f\"Fold {fold + 1} MAE: {mae_score}\")\n",
    "\n",
    "# Summarize Cross-Validation Results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average MAE: {np.mean(fold_mae_scores)}\")\n",
    "print(f\"Standard Deviation of MAE: {np.std(fold_mae_scores)}\")\n",
    "\n",
    "# Visualization (optional)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, k+1), fold_mae_scores)\n",
    "plt.axhline(np.mean(fold_mae_scores), color='r', linestyle='--', label='Mean MAE')\n",
    "plt.title('MAE per Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
