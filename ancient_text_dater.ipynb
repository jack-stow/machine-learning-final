{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are some common modules used in machine learning for importing/modifying data as well as visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the encoded_df_blanks_as_na.csv file for our features dataframe. features_df contains the independent variables used to train our machine learning model. These are the characterisitcs of the data that our model will analyze to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- A one-hot encoded matrix of features, with 11599 person names (nam_id_XXXX)\n",
    "    and 4784 place names (geo_id_XXXX) as columns.\n",
    "- The first column contains unique text_id corresponding to individual texts.\n",
    "'''\n",
    "features_df = pd.read_csv('encoded_df_blanks_as_na.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the 20240103_texts_with_dates.csv file for our labels dataframe. We then preview the dataframe. labels_df contains the dependent variables (target values) our model is trying to predict. These are the outputs corresponding to observations in the features_df. y1 and y1 willl serve as the ground truth our model learns to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tex_id</th>\n",
       "      <th>geotex_id</th>\n",
       "      <th>written</th>\n",
       "      <th>found</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>language_text</th>\n",
       "      <th>material_text</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>remark</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12042</td>\n",
       "      <td>8388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>117.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y1 = earliest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12054</td>\n",
       "      <td>8391</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y2 = latest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12063</td>\n",
       "      <td>8393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if y1 = y2, then we are certain of the date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12064</td>\n",
       "      <td>8394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17239</td>\n",
       "      <td>9507</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>967240</td>\n",
       "      <td>1021239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-263.0</td>\n",
       "      <td>-229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29246</th>\n",
       "      <td>5910</td>\n",
       "      <td>5438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29247</th>\n",
       "      <td>3506</td>\n",
       "      <td>4066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29248</th>\n",
       "      <td>7491</td>\n",
       "      <td>6778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29249</th>\n",
       "      <td>7491</td>\n",
       "      <td>77875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29250 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tex_id  geotex_id  written  found  geo_id    language_text  \\\n",
       "0       12042       8388        1      1    1008            Greek   \n",
       "1       12054       8391        1      1    1008            Greek   \n",
       "2       12063       8393        1      1    1008            Greek   \n",
       "3       12064       8394        1      1    1008            Greek   \n",
       "4       17239       9507        0      1    1008            Greek   \n",
       "...       ...        ...      ...    ...     ...              ...   \n",
       "29245  967240    1021239        1      0     332            Greek   \n",
       "29246    5910       5438        1      1    1344            Greek   \n",
       "29247    3506       4066        1      1    1344  Demotic / Greek   \n",
       "29248    7491       6778        0      1     720  Demotic / Greek   \n",
       "29249    7491      77875        1      0    1780  Demotic / Greek   \n",
       "\n",
       "      material_text     y1     y2 remark  Unnamed: 10  \\\n",
       "0           papyrus  117.0  118.0    NaN          NaN   \n",
       "1           papyrus  119.0  119.0    NaN          NaN   \n",
       "2           papyrus   96.0   98.0    NaN          NaN   \n",
       "3           papyrus  131.0  131.0    NaN          NaN   \n",
       "4           papyrus  108.0  108.0    NaN          NaN   \n",
       "...             ...    ...    ...    ...          ...   \n",
       "29245       papyrus -263.0 -229.0    NaN          NaN   \n",
       "29246       papyrus -148.0 -148.0    NaN          NaN   \n",
       "29247       papyrus -150.0 -150.0    NaN          NaN   \n",
       "29248       papyrus -236.0 -236.0    new          NaN   \n",
       "29249       papyrus -236.0 -236.0    new          NaN   \n",
       "\n",
       "                                         Unnamed: 11  \n",
       "0           - y1 = earliest possible year of writing  \n",
       "1             - y2 = latest possible year of writing  \n",
       "2      [if y1 = y2, then we are certain of the date]  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "...                                              ...  \n",
       "29245                                            NaN  \n",
       "29246                                            NaN  \n",
       "29247                                            NaN  \n",
       "29248                                            NaN  \n",
       "29249                                            NaN  \n",
       "\n",
       "[29250 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- text_id: same as the text id in the features dataset\n",
    "- y1: The earliest possible date the text was written.\n",
    "- y2: The latest possible date the text was written.\n",
    "- If y1 and y2 are equal, the date of writing is known with certainty.\n",
    "'''\n",
    "\n",
    "labels_df = pd.read_csv('20240103_texts_with_dates.csv')\n",
    "\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding**\n",
    "\n",
    "Looking at the features file, it is one-hot encoded. One-hot encoding is a method used to represent categorical data as binary values. It is commonly used to convert non-numerical data into a format that algorithms can process effectively as well as avoiding implicit ordinal relationships. It works by identifying all unique categories and creating binary columns for all of them. We assign 1 to the column corresponding to the presence of that data and 0 for all other related columns.\n",
    "\n",
    "This file was not fully one-hot encoded at first. It had a 1 for all values, but a NULL for the absence of features. All NULL values should be filled with a 0 to represent the absence of that feature and clean the data more.\n",
    "\n",
    "After filling all NaN columns, we preview the first 5 columns of the features_df. We are setting the text_id as the index of the features_df because although it is a column, it is not actually a feature but rather a unique identifier for each row.\n",
    "\n",
    "Lastly, we print information about the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       text_id  nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "29976   851611            0.0             0.0           0.0           0.0   \n",
      "29141   220342            0.0             0.0           0.0           0.0   \n",
      "9013     15900            0.0             0.0           0.0           0.0   \n",
      "27334    91732            0.0             0.0           0.0           0.0   \n",
      "29792   765547            0.0             0.0           0.0           0.0   \n",
      "\n",
      "       nam_id_2227.0  nam_id_726.0  nam_id_761.0  nam_id_731.0  nam_id_1246.0  \n",
      "29976            0.0           0.0           0.0           0.0            0.0  \n",
      "29141            0.0           0.0           0.0           0.0            0.0  \n",
      "9013             0.0           0.0           0.0           0.0            0.0  \n",
      "27334            0.0           0.0           0.0           0.0            0.0  \n",
      "29792            0.0           0.0           0.0           0.0            0.0  \n",
      "Number of rows: 30324\n",
      "Number of columns: 16383\n"
     ]
    }
   ],
   "source": [
    "# Printing information about the features dataset\n",
    "features_df = features_df.fillna(0)\n",
    "\n",
    "print(features_df.sample(5).iloc[:, :10])\n",
    "\n",
    "features_df.set_index('text_id', inplace=True)\n",
    "\n",
    "\n",
    "print(f'Number of rows: {features_df.shape[0]}')\n",
    "print(f'Number of columns: {features_df.shape[1]}')\n",
    "print(labels_df['y1'].min())\n",
    "print(labels_df['y2'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Data preprocessing is the stage in machine learning where raw data is cleaned and prepared for analysis or model training. Firstly, we cleaned the data by filling NULLS with 0's for one-hot encoding. Now, we are removing duplicate rows from features_df and labels_df to ensure the dataset is clean, consistent, and free from redundancy. Duplicates can influence bias which would negatively impact the performance of our model.\n",
    "\n",
    "<font color='red'>Remove outliers?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 30324\n",
      "Number of duplicate rows: 1478\n",
      "Number of rows after removing duplicates: 28846\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {features_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {features_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "features_df = features_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {features_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 29250\n",
      "Number of duplicate rows: 2\n",
      "Number of rows after removing duplicates: 29248\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {labels_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {labels_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "labels_df = labels_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {labels_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are further preparing the data by removing features with low variance. Variance refers to how much the values in a feature vary. Features with very little variation don't provide value for prediction because there aren't enough differences in the data for the model to learn any patterns. By removing these low-variance features, we simplify the model, reduce overfitting, and improve computational efficiency.\n",
    "\n",
    "The threshold we chose is arbitrary since it is a hyper parameter. We tuned our models by setting various thresholds and seeing which one resulted in the lowest MAE. A threshold of 0.001 produced our best result.\n",
    "\n",
    "fit_transform calculates the variance of each feature in features_df and removes features whose variance is below our specified threshold.\n",
    "\n",
    "Printing our original and reduced shape shows how we are able to reduce our dimensions by 15,038. There were diminishing returns with feature reduction. After a certain point, it was better to have more features. Originally, we reduced to 143 columns but 1345 performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 16383)\n",
      "Reduced shape: (28846, 1345)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.001)  # Adjust threshold as needed\n",
    "features_reduced_df = selector.fit_transform(features_df)\n",
    "\n",
    "print(f\"Original shape: {features_df.shape}\")\n",
    "print(f\"Reduced shape: {features_reduced_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Singular Value Decomposition (SVD) for dimensionality reduction because it is good at reducing the number of features in high-dimensional datasets and sparse matrixes in particular. Our data was an extremely sparse matrix. SVD reduces the number of features while retaining the most important information. We chose 1000 principal components to keep because it performed well after testing various values for this hyperparameter. The explained variance ratio shows that even after reducing our features by over 15000, we still manage to capture 95% of the variance in 1000 components.\n",
    "\n",
    "<font color='red'>EXPLAIN SVD MORE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY SVD INSTEAD OF PCA?**\n",
    "\n",
    "We chose SVD over PCA for several reasons such as nonlinear relationships, sparse matrixes, and large datasets. SVD is more flexible in handling non-linear relationships and can be used as a general-purpose dimensionality reduction technique. We were able to determine our data was non-linear by comparing the results of models based on linear and nonlinear approaches. Our nonlinear models outperformed our linear models by far. SVD is also useful when dealing with sparse data, which is the case for our data. SVD seemed like a better option due to the size of our dataset as well. PCA involves calculating a covariance matrix which would take longer than SVD to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 1345)\n",
      "Reduced shape: (28846, 1000)\n",
      "[0.02962784 0.02500479 0.01815715 0.01565095 0.01279698 0.01168515\n",
      " 0.01004947 0.00987946 0.00931344 0.00879833 0.00840652 0.00839601\n",
      " 0.00809594 0.00763554 0.00718988 0.00665971 0.00635726 0.00601058\n",
      " 0.00589252 0.00575561 0.00555587 0.00538438 0.00514929 0.00505432\n",
      " 0.00497139 0.00482803 0.00466713 0.00462055 0.00449435 0.00450015\n",
      " 0.00440983 0.00434999 0.00419037 0.00417054 0.00398282 0.00395789\n",
      " 0.00388713 0.00385748 0.00381505 0.00375992 0.00364783 0.00358395\n",
      " 0.00353121 0.00346126 0.00344036 0.00339893 0.00334313 0.00329448\n",
      " 0.00327808 0.00323764 0.00320789 0.00318611 0.00315127 0.00310233\n",
      " 0.00307426 0.00303943 0.00302351 0.00295952 0.00290862 0.00287503\n",
      " 0.00286709 0.00282286 0.00280273 0.00276819 0.0027324  0.00270148\n",
      " 0.00267772 0.00264733 0.00259831 0.00260706 0.00255261 0.00252425\n",
      " 0.00250912 0.00249147 0.00248418 0.00243584 0.00241878 0.00241629\n",
      " 0.0023637  0.00236463 0.00233179 0.00229635 0.00228166 0.00225857\n",
      " 0.00224113 0.00223507 0.00221433 0.00216822 0.00216044 0.00214007\n",
      " 0.0021378  0.00212099 0.00208709 0.00207497 0.00204668 0.00202634\n",
      " 0.00200302 0.00199654 0.00198246 0.00195187 0.00194184 0.00193582\n",
      " 0.00191782 0.00191005 0.00188926 0.00188412 0.00185195 0.0018437\n",
      " 0.00182582 0.00181916 0.00180053 0.00178326 0.00177847 0.00176378\n",
      " 0.00175679 0.00174502 0.001743   0.00172772 0.00171223 0.0017037\n",
      " 0.00169173 0.00167589 0.00166211 0.00165484 0.00164778 0.00163779\n",
      " 0.00163567 0.00161588 0.00161173 0.00160585 0.00158542 0.00157443\n",
      " 0.00156666 0.00155057 0.00154559 0.00154159 0.00152186 0.00151723\n",
      " 0.00151531 0.00150442 0.00150194 0.00148275 0.00147956 0.00147411\n",
      " 0.00146562 0.00146156 0.00144689 0.00144006 0.00143127 0.00142173\n",
      " 0.00141775 0.00140446 0.00140016 0.00138989 0.00138197 0.00137071\n",
      " 0.00136443 0.00135231 0.00134061 0.00133727 0.00132714 0.00132262\n",
      " 0.00131568 0.00131186 0.00130676 0.00129947 0.00129457 0.00129199\n",
      " 0.00128523 0.00126934 0.00126305 0.00125654 0.00124983 0.00124684\n",
      " 0.00124092 0.00123485 0.00123066 0.00122139 0.00121182 0.00120593\n",
      " 0.00120524 0.00119915 0.0011903  0.001188   0.0011812  0.00117479\n",
      " 0.0011642  0.00116151 0.00115514 0.00115141 0.00114309 0.00113945\n",
      " 0.00113318 0.00112978 0.00112695 0.00111917 0.00111065 0.00110649\n",
      " 0.00110064 0.00109866 0.00109367 0.00108941 0.00108616 0.00107959\n",
      " 0.00107384 0.00107201 0.00106612 0.00106302 0.00105637 0.00104979\n",
      " 0.00104409 0.00104029 0.00103716 0.0010303  0.00102824 0.0010206\n",
      " 0.00101626 0.00101211 0.00101178 0.00100711 0.00099836 0.00099433\n",
      " 0.00098945 0.00098651 0.00098289 0.00097967 0.00097859 0.00096931\n",
      " 0.00096778 0.00096421 0.00095958 0.00095839 0.00095295 0.00095025\n",
      " 0.00094696 0.00094517 0.00094064 0.00093261 0.00093119 0.00092933\n",
      " 0.00092559 0.00092087 0.00092043 0.00091074 0.00090887 0.00090603\n",
      " 0.00090186 0.00090043 0.00089549 0.00089265 0.00089108 0.00088713\n",
      " 0.00088517 0.0008792  0.00087612 0.00087039 0.00086978 0.00086757\n",
      " 0.00086477 0.00085736 0.00085721 0.00085465 0.00085238 0.00084742\n",
      " 0.00084479 0.00083796 0.00083656 0.00083485 0.00083159 0.00082867\n",
      " 0.00082658 0.00082545 0.00082154 0.00081836 0.00081752 0.00081339\n",
      " 0.00080909 0.00080777 0.00080545 0.00079925 0.00079555 0.00079343\n",
      " 0.00078982 0.00078938 0.00078853 0.00078228 0.00078108 0.00077864\n",
      " 0.00077319 0.00077183 0.00076927 0.00076499 0.000761   0.00075921\n",
      " 0.00075564 0.00075307 0.00074955 0.00074666 0.00074549 0.00074283\n",
      " 0.00073908 0.00073728 0.00073256 0.00073199 0.00072878 0.00072685\n",
      " 0.00072405 0.00071763 0.00071669 0.00071483 0.00071161 0.00071127\n",
      " 0.00070813 0.0007063  0.00070446 0.00070292 0.00069969 0.00069532\n",
      " 0.00069378 0.0006884  0.0006857  0.00068546 0.00068178 0.00068125\n",
      " 0.00067995 0.0006772  0.00067479 0.00067264 0.00066831 0.000666\n",
      " 0.00066433 0.00066306 0.00066088 0.0006598  0.00065859 0.0006568\n",
      " 0.00065581 0.00065382 0.00064902 0.00064732 0.00064615 0.00064333\n",
      " 0.00064197 0.00063897 0.00063729 0.00063551 0.00063468 0.00063295\n",
      " 0.00063184 0.00063118 0.00062873 0.00062643 0.00062495 0.0006231\n",
      " 0.00062151 0.00062043 0.00061989 0.00061801 0.00061699 0.00061498\n",
      " 0.00061019 0.00060773 0.00060545 0.00060482 0.00060333 0.00060024\n",
      " 0.00059884 0.00059703 0.00059545 0.00059485 0.0005922  0.00059049\n",
      " 0.00059006 0.00058825 0.00058687 0.00058544 0.00058402 0.00058138\n",
      " 0.00057967 0.00057895 0.00057677 0.00057435 0.00057342 0.00057149\n",
      " 0.00057023 0.00056925 0.00056735 0.00056469 0.00056449 0.00056315\n",
      " 0.00056219 0.00055968 0.00055838 0.00055756 0.00055683 0.00055468\n",
      " 0.00055294 0.00055254 0.00055049 0.00054961 0.0005484  0.00054601\n",
      " 0.00054561 0.00054289 0.00054112 0.00054022 0.00053836 0.00053683\n",
      " 0.00053639 0.00053572 0.0005344  0.00053289 0.00053123 0.0005306\n",
      " 0.00052771 0.00052634 0.00052531 0.00052375 0.00052231 0.00052051\n",
      " 0.000519   0.00051754 0.00051607 0.00051489 0.00051435 0.00051308\n",
      " 0.00051227 0.00051077 0.00050909 0.00050792 0.00050675 0.0005062\n",
      " 0.00050476 0.00050418 0.00050228 0.00050155 0.00049852 0.00049755\n",
      " 0.00049592 0.00049541 0.00049401 0.00049378 0.00049233 0.00049015\n",
      " 0.00048921 0.00048841 0.00048745 0.00048494 0.00048501 0.00048379\n",
      " 0.00048255 0.00048149 0.0004806  0.00047851 0.00047741 0.00047629\n",
      " 0.00047556 0.00047536 0.00047318 0.000472   0.00047172 0.00046998\n",
      " 0.00046853 0.00046734 0.00046724 0.00046603 0.00046373 0.00046329\n",
      " 0.00046211 0.00046149 0.0004609  0.00045834 0.00045765 0.00045624\n",
      " 0.00045482 0.00045374 0.0004525  0.00045158 0.00045052 0.00044975\n",
      " 0.00044814 0.0004469  0.00044649 0.00044545 0.00044467 0.00044399\n",
      " 0.00044365 0.0004413  0.00043997 0.00043918 0.00043788 0.00043625\n",
      " 0.00043567 0.00043376 0.00043283 0.00043177 0.00043151 0.00043128\n",
      " 0.00042996 0.00042901 0.00042779 0.00042632 0.00042602 0.00042442\n",
      " 0.00042341 0.0004231  0.00042183 0.00042068 0.00042014 0.00041887\n",
      " 0.0004179  0.00041698 0.00041622 0.00041563 0.00041475 0.00041294\n",
      " 0.00041293 0.00041135 0.00041097 0.00040962 0.000409   0.00040826\n",
      " 0.00040681 0.00040587 0.00040547 0.00040517 0.00040359 0.00040265\n",
      " 0.00040227 0.00040145 0.00040072 0.00040033 0.00039987 0.00039794\n",
      " 0.0003976  0.00039678 0.00039568 0.00039515 0.00039348 0.00039221\n",
      " 0.00039119 0.00039053 0.0003898  0.00038944 0.00038845 0.00038768\n",
      " 0.00038712 0.0003864  0.00038533 0.0003849  0.00038326 0.00038237\n",
      " 0.00038129 0.00038065 0.00038061 0.00037865 0.00037828 0.00037761\n",
      " 0.00037731 0.00037576 0.00037547 0.00037469 0.00037392 0.00037306\n",
      " 0.00037172 0.00037119 0.00037059 0.00036873 0.00036834 0.00036774\n",
      " 0.00036743 0.00036663 0.00036525 0.000365   0.0003644  0.00036382\n",
      " 0.00036266 0.00036193 0.00036149 0.000361   0.00035984 0.00035913\n",
      " 0.00035862 0.00035782 0.00035671 0.00035591 0.00035495 0.00035463\n",
      " 0.00035409 0.00035384 0.00035236 0.00035205 0.00035157 0.0003514\n",
      " 0.00034937 0.0003488  0.00034866 0.00034796 0.00034751 0.00034658\n",
      " 0.00034599 0.00034564 0.00034483 0.00034406 0.00034319 0.00034275\n",
      " 0.00034183 0.00034103 0.00034073 0.00033951 0.00033913 0.0003386\n",
      " 0.00033828 0.00033766 0.00033678 0.00033615 0.00033498 0.00033491\n",
      " 0.00033401 0.00033361 0.00033275 0.00033239 0.00033189 0.00033113\n",
      " 0.00033077 0.00033002 0.00032857 0.00032813 0.00032752 0.00032696\n",
      " 0.00032671 0.00032638 0.00032533 0.00032447 0.00032414 0.00032373\n",
      " 0.0003228  0.00032241 0.00032204 0.00032079 0.00032071 0.00031972\n",
      " 0.00031911 0.00031856 0.00031803 0.00031724 0.00031651 0.00031596\n",
      " 0.00031556 0.00031476 0.00031432 0.00031411 0.00031347 0.00031289\n",
      " 0.00031241 0.00031153 0.00031122 0.00031084 0.00031037 0.00030962\n",
      " 0.00030917 0.00030886 0.00030803 0.00030764 0.0003063  0.00030614\n",
      " 0.00030595 0.00030538 0.00030372 0.00030335 0.0003029  0.00030278\n",
      " 0.00030219 0.00030166 0.00030078 0.00029995 0.00029943 0.00029925\n",
      " 0.00029884 0.00029837 0.00029743 0.00029719 0.0002969  0.00029617\n",
      " 0.00029528 0.00029439 0.00029385 0.00029357 0.00029317 0.00029278\n",
      " 0.00029253 0.00029159 0.00029116 0.0002909  0.00029062 0.00028964\n",
      " 0.000289   0.00028845 0.00028816 0.00028789 0.00028695 0.00028674\n",
      " 0.00028636 0.00028559 0.00028461 0.00028435 0.00028374 0.00028341\n",
      " 0.00028291 0.00028263 0.00028219 0.00028142 0.00028136 0.00028063\n",
      " 0.00028027 0.00027988 0.0002792  0.00027846 0.0002782  0.00027711\n",
      " 0.00027677 0.0002759  0.00027539 0.00027488 0.0002747  0.00027442\n",
      " 0.00027406 0.00027336 0.0002729  0.00027212 0.00027169 0.00027114\n",
      " 0.0002711  0.00027031 0.00027003 0.00026959 0.0002685  0.00026814\n",
      " 0.00026782 0.00026732 0.00026692 0.00026664 0.00026618 0.00026586\n",
      " 0.00026484 0.00026482 0.00026429 0.00026389 0.0002636  0.00026339\n",
      " 0.00026309 0.00026215 0.00026178 0.00026157 0.0002607  0.00026034\n",
      " 0.00026009 0.00025922 0.00025877 0.00025849 0.00025797 0.00025779\n",
      " 0.00025732 0.00025673 0.00025635 0.00025552 0.00025496 0.00025469\n",
      " 0.00025396 0.00025382 0.0002533  0.00025293 0.00025237 0.00025197\n",
      " 0.00025195 0.00025146 0.00025127 0.00025013 0.00025009 0.00024976\n",
      " 0.00024919 0.00024888 0.00024854 0.00024795 0.00024758 0.00024705\n",
      " 0.00024687 0.0002466  0.00024607 0.00024534 0.00024514 0.00024484\n",
      " 0.00024444 0.00024356 0.00024329 0.0002425  0.00024196 0.00024171\n",
      " 0.00024166 0.0002411  0.00024068 0.00023999 0.00023949 0.00023943\n",
      " 0.00023879 0.00023839 0.00023749 0.00023724 0.00023708 0.0002368\n",
      " 0.00023633 0.00023562 0.00023496 0.00023509 0.00023447 0.00023393\n",
      " 0.00023359 0.00023329 0.0002328  0.00023243 0.00023222 0.00023112\n",
      " 0.00023075 0.00023058 0.00023039 0.00022979 0.00022956 0.00022925\n",
      " 0.00022838 0.00022803 0.00022783 0.00022763 0.0002276  0.00022674\n",
      " 0.00022625 0.00022601 0.00022576 0.00022554 0.00022478 0.00022415\n",
      " 0.000224   0.00022392 0.00022342 0.0002233  0.00022261 0.00022229\n",
      " 0.00022169 0.00022114 0.00022065 0.00022057 0.00022005 0.00021951\n",
      " 0.00021953 0.00021848 0.00021842 0.00021811 0.00021785 0.00021723\n",
      " 0.0002167  0.00021649 0.0002163  0.0002159  0.00021531 0.00021472\n",
      " 0.00021465 0.00021425 0.00021367 0.00021343 0.00021282 0.00021259\n",
      " 0.00021234 0.00021222 0.00021185 0.00021143 0.00021044 0.00021025\n",
      " 0.00020984 0.00020972 0.00020929 0.00020922 0.00020878 0.0002082\n",
      " 0.00020802 0.00020746 0.00020721 0.00020653 0.00020634 0.00020582\n",
      " 0.00020561 0.00020523 0.00020449 0.000204   0.00020378 0.00020344\n",
      " 0.00020337 0.00020316 0.00020295 0.00020217 0.00020203 0.00020152\n",
      " 0.00020094 0.00020067 0.00019979 0.00019986 0.00019948 0.00019936\n",
      " 0.00019896 0.0001984  0.00019824 0.00019814 0.00019792 0.00019715\n",
      " 0.000197   0.00019654 0.00019607 0.00019553 0.00019471 0.00019486\n",
      " 0.00019419 0.00019404 0.0001936  0.00019286 0.00019257 0.00019231\n",
      " 0.00019198 0.00019193 0.00019135 0.00019123 0.00019106 0.00019061\n",
      " 0.00019029 0.00018998 0.00018986 0.00018946 0.00018898 0.00018879\n",
      " 0.000188   0.00018763 0.00018699 0.0001864  0.00018583 0.00018559\n",
      " 0.00018549 0.00018477 0.00018478 0.00018437 0.00018399 0.00018395\n",
      " 0.00018362 0.00018331 0.00018283 0.00018256 0.00018241 0.00018189\n",
      " 0.00018181 0.0001813  0.00018072 0.0001801  0.00017934 0.00017936\n",
      " 0.00017876 0.00017832 0.00017819 0.00017784 0.00017712 0.00017697\n",
      " 0.0001765  0.00017559 0.00017527 0.00017495 0.00017449 0.00017423\n",
      " 0.00017426 0.00017368 0.00017281 0.00017223 0.00017161 0.00017146\n",
      " 0.00017088 0.00017093 0.00016981 0.00016909 0.00016936 0.00016881\n",
      " 0.00016812 0.00016749 0.00016754 0.00016678 0.00016636 0.00016634\n",
      " 0.00016614 0.00016551 0.00016519 0.00016427 0.00016383 0.00016269\n",
      " 0.00016261 0.0001622  0.00016202 0.00016076]\n",
      "0.9536659040503459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "# svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "features_svd = svd.fit_transform(features_reduced_df)\n",
    "\n",
    "print(f\"Original shape: {features_reduced_df.shape}\")\n",
    "print(f\"Reduced shape: {features_svd.shape}\")\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(sum(svd.explained_variance_ratio_))  # Total variance retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell converts our features_svd array back into a dataframe features_svd_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming reduced_features is your NumPy array from SVD or Variance Thresholding\n",
    "# features_df['text_id'] contains the IDs you need to reattach\n",
    "features_df.reset_index(inplace=True)\n",
    "features_svd_df = pd.DataFrame(features_svd, columns=[f'feature_{i}' for i in range(features_svd.shape[1])])\n",
    "features_svd_df['text_id'] = features_df['text_id'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important cells because it prepares our training set. Our original label and features files have around 20k and 30k rows respectively. Not every row of data in the features dataset has a corresponding row of data in the labels dataset and vice versa. In order to train our model, we need to give it data that it already knows the answers for - the ground truths. This is the metric we will compare our predictions against to guage the performance of our models. Although we cleaned our data for duplicates before, we wanted to ensure the data was thoroughly prepared as we inch closer to training our model.\n",
    "\n",
    "After removing duplicates and data that didn't appear in both the label and features datasets, we were left with 8565 datapoints with corresponding labels. This is the data we use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text IDs missing in labels: 20281\n",
      "Text IDs missing in features: 12381\n",
      "Updated features shape: (8565, 1001)\n",
      "Updated labels shape: (13013, 12)\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 0\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 4448\n",
      "Aligned features shape: (8565, 1001)\n",
      "Aligned labels shape: (8565, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get text_ids in each dataset\n",
    "features_text_ids = set(features_svd_df['text_id'])\n",
    "labels_text_ids = set(labels_df['tex_id'])\n",
    "\n",
    "# Find unmatched text_ids\n",
    "missing_in_labels = features_text_ids - labels_text_ids\n",
    "missing_in_features = labels_text_ids - features_text_ids\n",
    "\n",
    "print(f\"Text IDs missing in labels: {len(missing_in_labels)}\")\n",
    "print(f\"Text IDs missing in features: {len(missing_in_features)}\")\n",
    "\n",
    "# Keep only common text_ids\n",
    "common_text_ids = features_text_ids & labels_text_ids\n",
    "\n",
    "# Filter features and labels datasets\n",
    "features_common_df = features_svd_df[features_svd_df['text_id'].isin(common_text_ids)]\n",
    "labels_common_df = labels_df[labels_df['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "# Check the updated shapes\n",
    "print(f\"Updated features shape: {features_common_df.shape}\")\n",
    "print(f\"Updated labels shape: {labels_common_df.shape}\")\n",
    "\n",
    "# Check for duplicated rows\n",
    "duplicate_rows = features_common_df.duplicated()\n",
    "\n",
    "# Check for duplicated text_ids\n",
    "duplicate_text_ids = features_common_df['text_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_text_ids.sum()}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_label_rows = labels_common_df.duplicated()\n",
    "\n",
    "# Check for duplicate text_ids\n",
    "duplicate_label_text_ids = labels_common_df['tex_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_label_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_label_text_ids.sum()}\")\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "features = features_common_df.drop_duplicates(subset='text_id', keep='first')\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "labels = labels_common_df.drop_duplicates(subset='tex_id', keep='first')\n",
    "\n",
    "\n",
    "# Ensure alignment of text_ids between features and labels\n",
    "common_text_ids = set(features['text_id']) & set(labels['tex_id'])\n",
    "\n",
    "# Filter again if necessary\n",
    "features_filtered_df = features[features['text_id'].isin(common_text_ids)]\n",
    "labels_filtered_df = labels[labels['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "print(f\"Aligned features shape: {features_filtered_df.shape}\")\n",
    "print(f\"Aligned labels shape: {labels_filtered_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the index of our dataframes to the text_ids since are not considered features, but rather unique identifiers of each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_df.set_index('text_id', inplace=True)\n",
    "\n",
    "labels_filtered_df.rename(columns={'tex_id': 'text_id'}, inplace=True)\n",
    "labels_filtered_df.set_index('text_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task involves predicting the year a text was written, retaining the other ground truth labels was unnecessary. Removing them avoids confusion and ensures our process is focused solely on the data relevant to our task.\n",
    "\n",
    "<font color=\"red\"> Should we convert the other labels into features before dimension reduction?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12042</th>\n",
       "      <td>117.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>119.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12064</th>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17239</th>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703104</th>\n",
       "      <td>-250.0</td>\n",
       "      <td>-175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703317</th>\n",
       "      <td>-263.0</td>\n",
       "      <td>-229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>-148.0</td>\n",
       "      <td>-148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>-150.0</td>\n",
       "      <td>-150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>-236.0</td>\n",
       "      <td>-236.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            y1     y2\n",
       "text_id              \n",
       "12042    117.0  118.0\n",
       "12054    119.0  119.0\n",
       "12063     96.0   98.0\n",
       "12064    131.0  131.0\n",
       "17239    108.0  108.0\n",
       "...        ...    ...\n",
       "703104  -250.0 -175.0\n",
       "703317  -263.0 -229.0\n",
       "5910    -148.0 -148.0\n",
       "3506    -150.0 -150.0\n",
       "7491    -236.0 -236.0\n",
       "\n",
       "[8565 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_final_df = labels_filtered_df[['y1','y2']]\n",
    "labels_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this step is redundant, we wanted to ensure all of our datapoints were merged with a corresponding label via an inner join. This could be skipped altogether since we are separating the y label from the X features when training our model, but having it as one dataframe in the beginning aligned with our approaches throughout the course.\n",
    "\n",
    "<font color=\"red\">Can we replace the cell above where we filter for records in both features_df and labels_df by only using this method?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8565, 1002)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge on text_id\n",
    "merged_df = features_filtered_df.merge(labels_final_df, on='text_id', how='inner')\n",
    "\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification or Regression?</h1>\n",
    "\n",
    "Here we are creating two variables that indicate how many rows exist where y1=y2 and y1!=y2 and displaying those results.\n",
    "\n",
    "Since we have more rows where an exact year is known (5,239) we believe regression was the better approach because predicting an exact year (a continuous variable) aligns with the strengths of regression.\n",
    "\n",
    "If we had more cases where y1!=y2, then there would be more uncertainty about the exact date of our texts, justifying the use of a classifcation model where we simplify the task by binning the ranges into predifined categories (100-199AD, 200-299AD).\n",
    "\n",
    "Based on the context of our training data, we believed regression was the better approach because the majority of our ground truths were a single continous year vs a range of years.\n",
    "\n",
    "<b>We chose Regression</b>\n",
    "\n",
    "<font color=\"red\"> Should we create classification models anyway for comparison?</font>\n",
    "\n",
    "<font color=\"red\">Should we combine classifcation and regression models into one model?\n",
    "ex. classifcation followed by regression\n",
    "ex. regression followed by classification\n",
    "ex. multi-output neural network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where y1 = y2: 5239\n",
      "Number of rows where y1 != y2: 3326\n"
     ]
    }
   ],
   "source": [
    "equal_rows = merged_df[merged_df['y1'] == merged_df['y2']].shape[0]\n",
    "unequal_rows = merged_df[merged_df['y1'] != merged_df['y2']].shape[0]\n",
    "\n",
    "print(f\"Number of rows where y1 = y2: {equal_rows}\")\n",
    "print(f\"Number of rows where y1 != y2: {unequal_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose regression, we created a target column that handles both cases when y1 equals y2 (continuous) and when y1 didn't equal y2 (range of years).\n",
    "\n",
    "y_target simply became y1 if they were equal\n",
    "y_target took the midpoint of y1 and y2 if they weren't equal. The midpoint serves as a reasonable estimate for a range when an exact year isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>500.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows × 1003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_993  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...     0.006794   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.029750   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.022786   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...    -0.035404   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.002199   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...    -0.002129   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.000316   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.000879   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.005298   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.014535   \n",
       "\n",
       "         feature_994  feature_995  feature_996  feature_997  feature_998  \\\n",
       "text_id                                                                    \n",
       "1           0.075361    -0.012450     0.011055     0.047048    -0.014151   \n",
       "2          -0.032232     0.014291    -0.030107    -0.024161    -0.011760   \n",
       "3          -0.009252     0.038818    -0.002101     0.028129    -0.016734   \n",
       "4          -0.097845     0.001688     0.080584     0.043998     0.040462   \n",
       "5          -0.002162     0.007168     0.001697    -0.003452    -0.001445   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002368     0.003550     0.002539     0.004470    -0.000576   \n",
       "981644      0.002716     0.007476     0.001311     0.000043     0.009732   \n",
       "981646      0.002465     0.000985     0.000319     0.000345    -0.001285   \n",
       "981648      0.002257     0.007938    -0.000986     0.003599     0.002233   \n",
       "981666      0.005975    -0.001595     0.008034    -0.005991     0.010769   \n",
       "\n",
       "         feature_999     y1     y2  y_target  \n",
       "text_id                                       \n",
       "1          -0.024547 -124.0 -124.0    -124.0  \n",
       "2           0.020541 -112.0 -112.0    -112.0  \n",
       "3           0.019266 -109.0 -109.0    -109.0  \n",
       "4           0.092145 -108.0 -108.0    -108.0  \n",
       "5           0.011756 -106.0 -106.0    -106.0  \n",
       "...              ...    ...    ...       ...  \n",
       "981643      0.000890  548.0  548.0     548.0  \n",
       "981644     -0.003175  553.0  553.0     553.0  \n",
       "981646      0.004310  549.0  549.0     549.0  \n",
       "981648      0.001810  500.0  599.0     549.5  \n",
       "981666     -0.002167  133.0  133.0     133.0  \n",
       "\n",
       "[8565 rows x 1003 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['y_target'] = merged_df.apply(lambda row: row['y1'] if row['y1'] == row['y2'] else (row['y1'] + row['y2']) / 2, axis=1)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our new ground truth y_target, we no longer needed the labels y1 and y2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_991</th>\n",
       "      <th>feature_992</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036339</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>-0.023396</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_991  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...    -0.036339   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.025149   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.015310   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...     0.063221   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.005871   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...     0.005598   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.002949   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.001661   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.001269   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.005230   \n",
       "\n",
       "         feature_992  feature_993  feature_994  feature_995  feature_996  \\\n",
       "text_id                                                                    \n",
       "1           0.028035     0.006794     0.075361    -0.012450     0.011055   \n",
       "2          -0.023396     0.029750    -0.032232     0.014291    -0.030107   \n",
       "3           0.003670     0.022786    -0.009252     0.038818    -0.002101   \n",
       "4           0.044466    -0.035404    -0.097845     0.001688     0.080584   \n",
       "5          -0.008616     0.002199    -0.002162     0.007168     0.001697   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002627    -0.002129    -0.002368     0.003550     0.002539   \n",
       "981644     -0.002140    -0.000316     0.002716     0.007476     0.001311   \n",
       "981646      0.002340     0.000879     0.002465     0.000985     0.000319   \n",
       "981648      0.001269    -0.005298     0.002257     0.007938    -0.000986   \n",
       "981666     -0.001312     0.014535     0.005975    -0.001595     0.008034   \n",
       "\n",
       "         feature_997  feature_998  feature_999  y_target  \n",
       "text_id                                                   \n",
       "1           0.047048    -0.014151    -0.024547    -124.0  \n",
       "2          -0.024161    -0.011760     0.020541    -112.0  \n",
       "3           0.028129    -0.016734     0.019266    -109.0  \n",
       "4           0.043998     0.040462     0.092145    -108.0  \n",
       "5          -0.003452    -0.001445     0.011756    -106.0  \n",
       "...              ...          ...          ...       ...  \n",
       "981643      0.004470    -0.000576     0.000890     548.0  \n",
       "981644      0.000043     0.009732    -0.003175     553.0  \n",
       "981646      0.000345    -0.001285     0.004310     549.0  \n",
       "981648      0.003599     0.002233     0.001810     549.5  \n",
       "981666     -0.005991     0.010769    -0.002167     133.0  \n",
       "\n",
       "[8565 rows x 1001 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['y1','y2'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TRAINING & COMPARING OUR MODELS</h1>\n",
    "\n",
    "This code splits our dataset into training and testing subsets to prepare it for our machine learning algorithms.\n",
    "\n",
    "X contains all of our features\n",
    "y contains our label\n",
    "\n",
    "We are then splitting the data into training and testing subsets, using 20% of the data for testing and 80% for training. We set random_sate =42 for reproducibility.\n",
    "\n",
    "The purpose of splitting our data is to evaluate how well the model generalizes to unseen data. Our training sets are used to train the model while the testing set is used afterward to assess the model's performance on data it hasn't seen during training.\n",
    "\n",
    "We did not scale our data because the original data was one-hot encoded and reducced using Variance Thresholding and SVD. Variance Thresholding only removes features with low variance and doesn't change their scales. SVD produces principle components that are linear combinations of our original features, but SVD already scales them to optimize variance. The components were normalized internally in the decomposition process of the SVD class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])\n",
    "y = merged_df['y_target']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LINEAR OR NONLINEAR RELATIONSHIPS?</h1>\n",
    "After training all of our models, it is apparent our the relationships in our data may be nonlinear.\n",
    "\n",
    "Our Lasso and Ridge Regression lienar models performed significantly worse. These models assume that the relationship between the features and target is linear.\n",
    "\n",
    "Our RandomForest and XGBoost ensemble nonlinear models performed far better. These models capture nonlinear relationships in the data. They work by splitting the features into smaller tress and makes decisions based on those trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LASSO</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - Best alpha: 0.11463516894989338\n",
      "Train MAE: 142.75912648399776\n",
      "Test MAE: 144.72150059681678\n",
      "Selected features by Lasso:\n",
      "feature_0       36.478300\n",
      "feature_1      285.691928\n",
      "feature_2       18.183640\n",
      "feature_3       65.560417\n",
      "feature_4     -126.042950\n",
      "                  ...    \n",
      "feature_104    -25.799100\n",
      "feature_105    -13.182620\n",
      "feature_106    -19.839579\n",
      "feature_108     37.442107\n",
      "feature_109    -29.204855\n",
      "Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Lasso with cross-validation\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Lasso - Best alpha:\", lasso.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, lasso.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, lasso.predict(X_test)))\n",
    "\n",
    "lasso_features = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\"Selected features by Lasso:\")\n",
    "print(lasso_features[lasso_features != 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RIDGE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - Best alpha: 10.0\n",
      "Train MAE: 142.69147866233317\n",
      "Test MAE: 144.62340916417358\n",
      "Ridge coefficients:\n",
      "feature_1      284.001682\n",
      "feature_5      231.785481\n",
      "feature_12     119.447728\n",
      "feature_22     111.392607\n",
      "feature_65      98.502349\n",
      "feature_79      70.167565\n",
      "feature_3       66.364979\n",
      "feature_85      66.199689\n",
      "feature_77      54.787702\n",
      "feature_100     53.352236\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ridge with cross-validation\n",
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Ridge - Best alpha:\", ridge.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, ridge.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, ridge.predict(X_test)))\n",
    "\n",
    "ridge_features = pd.Series(ridge.coef_, index=X.columns)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_features.sort_values(ascending=False).head(10))  # Top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 30.740977718639943\n",
      "Random Forest - Test MAE: 78.56731082965455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OPTIMIZING RANDOM FOREST</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "Best Random Forest Test MAE: 92.32550495382549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Random Forest Test MAE:\", mean_absolute_error(y_test, grid_search.best_estimator_.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 59.81336336248054\n",
      "Random Forest - Test MAE: 92.32550495382549\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(max_depth=20, min_samples_split=5, n_estimators=300, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBOOST (EXTREME GRADIENT BOOSTING)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 89.78133348274217\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7, learning_rate = 0.031,\n",
    "                          max_depth = 7, alpha = 25, n_estimators = 350)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 89.78133348274217\n",
      "XGBoost R-squared: 0.7394333114829847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate MAE and R-squared\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio by each component: [0.02009968 0.01721329 0.01706886 0.01563002 0.01416983 0.01400168\n",
      " 0.01363093 0.01350333 0.01342391 0.01325897 0.01304782 0.01291276\n",
      " 0.01264846 0.01242928 0.01235066 0.01214336 0.01195931 0.01183328\n",
      " 0.01158328 0.01138039 0.01135438 0.01128614 0.01111041 0.01109109\n",
      " 0.01099495 0.01084793 0.01083958 0.01074549 0.0106687  0.01060011\n",
      " 0.01043792 0.01039694 0.01035413 0.01031585 0.01006652 0.01002971\n",
      " 0.00998617 0.00991306 0.00984947 0.00973248 0.00970367 0.00956474\n",
      " 0.00952874 0.00945024 0.00941634 0.00928925 0.00926897 0.00921159\n",
      " 0.00913221 0.00906736 0.00897531 0.00892098 0.00890046 0.00882999\n",
      " 0.00881589 0.0087819  0.00862541 0.00861903 0.0085754  0.00854557\n",
      " 0.0084766  0.00844001 0.0083891  0.00834661 0.00826329 0.00819992\n",
      " 0.00811499 0.00807654 0.00804795 0.00801258 0.00794356 0.00783884\n",
      " 0.00778971 0.00777591 0.00765647 0.00761074 0.00754618 0.00748518\n",
      " 0.00738814 0.00727699 0.0071861  0.00712556 0.00709235 0.00704214]\n",
      "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
      "0 -0.820754  0.340402  4.654427 -2.438092  2.139213 -0.420409 -1.785240   \n",
      "1 -1.597292 -0.472658  4.712277 -4.848287  1.662482 -2.154621 -2.939657   \n",
      "2 -0.327313  0.068884  0.290807  0.518311  0.561232 -0.419608  0.113755   \n",
      "3 -1.506471 -0.775984  2.879149 -2.819396  1.895018 -0.104759 -3.335058   \n",
      "4 -0.913868 -0.079953  3.889464 -2.216912  2.673940 -1.975262 -3.891413   \n",
      "\n",
      "        PC8       PC9      PC10  ...      PC76      PC77      PC78      PC79  \\\n",
      "0  4.024922  2.999747 -0.421999  ... -1.159229  0.533207  0.191367 -0.245067   \n",
      "1  3.168307  2.000997 -0.683305  ... -2.170398  1.473903 -0.589178  0.282601   \n",
      "2  0.031049  0.362168  0.313422  ...  0.546230 -0.757509 -0.422710 -0.523487   \n",
      "3 -2.587006  1.864713  0.893686  ... -0.527491 -0.542564  0.086975  1.382881   \n",
      "4  2.869315  5.237813 -0.153925  ... -0.179359  0.468213 -0.281964 -0.161642   \n",
      "\n",
      "       PC80      PC81      PC82      PC83      PC84  y_target  \n",
      "0  1.082412 -0.235799 -0.883343 -0.091821 -0.903443       NaN  \n",
      "1  0.941697  0.452394  0.085303  0.692685  2.115440    -124.0  \n",
      "2 -0.392644 -0.098259  0.245250 -0.156389 -0.066764    -112.0  \n",
      "3  0.281376  0.452817 -2.667235 -0.472532 -0.427230    -109.0  \n",
      "4 -0.179456  0.334853 -0.562427  0.162160  0.255262    -108.0  \n",
      "\n",
      "[5 rows x 85 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'merged_df' is your dataframe and it's already loaded.\n",
    "\n",
    "# Step 1: Standardize the features\n",
    "# Exclude the target variable if it's part of the dataframe\n",
    "X = merged_df.drop(columns=['y_target'])  # Replace 'target_column' with the actual name of your target column\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "# Define the number of components you want to keep. If unsure, start with the total number of features.\n",
    "pca = PCA(n_components=0.85)  # This will keep enough components to explain 95% of the variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Convert the PCA results to a DataFrame (optional)\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "\n",
    "# Step 4: You can join the PCA results back with the target variable if needed.\n",
    "merged_df_pca = pd.concat([pca_df, merged_df['y_target']], axis=1)\n",
    "\n",
    "# Print the explained variance ratio (optional)\n",
    "print(\"Explained variance ratio by each component:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Optionally, print the new dataframe with PCA components\n",
    "print(merged_df_pca.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.concat([pca_df, merged_df['y_target']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])  # Replace with your feature dataframe\n",
    "y = merged_df['y_target']  # Replace with your target column\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 89.78133348274217\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7, learning_rate = 0.031,\n",
    "                          max_depth = 7, alpha = 25, n_estimators = 350)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 71.1634089561213\n",
      "XGBoost MAE: 71.1634089561213\n",
      "XGBoost R-squared: 0.8406828639172944\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBRegressor\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.7, learning_rate = 0.031,\n",
    "                          max_depth = 7, alpha = 25, n_estimators = 350)\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRY USING LIGHTGBM GRADIENT BOOSTING\n",
    "\n",
    "TRY SUPPORT VECTOR MACHINE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
