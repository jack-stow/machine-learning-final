{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are some common modules used in machine learning for importing/modifying data as well as visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3080, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import dask.dataframe as dd\n",
    "import modin.pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the encoded_df_blanks_as_na.csv file for our features dataframe. features_df contains the independent variables used to train our machine learning model. These are the characterisitcs of the data that our model will analyze to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 13:54:01,445\tINFO worker.py:1821 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- A one-hot encoded matrix of features, with 11599 person names (nam_id_XXXX)\n",
    "    and 4784 place names (geo_id_XXXX) as columns.\n",
    "- The first column contains unique text_id corresponding to individual texts.\n",
    "'''\n",
    "features_df = pd.read_csv('encoded_df_blanks_as_na.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the pandas module to create a dataframe using the 20240103_texts_with_dates.csv file for our labels dataframe. We then preview the dataframe. labels_df contains the dependent variables (target values) our model is trying to predict. These are the outputs corresponding to observations in the features_df. y1 and y1 willl serve as the ground truth our model learns to predict.\n",
    "\n",
    "We later switched to the modin pandas module to leverage multithreading, as it took forever to run without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tex_id</th>\n",
       "      <th>geotex_id</th>\n",
       "      <th>written</th>\n",
       "      <th>found</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>language_text</th>\n",
       "      <th>material_text</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>remark</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12042</td>\n",
       "      <td>8388</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y1 = earliest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12054</td>\n",
       "      <td>8391</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- y2 = latest possible year of writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12063</td>\n",
       "      <td>8393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if y1 = y2, then we are certain of the date]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12064</td>\n",
       "      <td>8394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17239</td>\n",
       "      <td>9507</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1008</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29245</th>\n",
       "      <td>967240</td>\n",
       "      <td>1021239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-263</td>\n",
       "      <td>-229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29246</th>\n",
       "      <td>5910</td>\n",
       "      <td>5438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-148</td>\n",
       "      <td>-148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29247</th>\n",
       "      <td>3506</td>\n",
       "      <td>4066</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1344</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-150</td>\n",
       "      <td>-150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29248</th>\n",
       "      <td>7491</td>\n",
       "      <td>6778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29249</th>\n",
       "      <td>7491</td>\n",
       "      <td>77875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1780</td>\n",
       "      <td>Demotic / Greek</td>\n",
       "      <td>papyrus</td>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29250 rows x 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tex_id  geotex_id  written  found  geo_id    language_text  \\\n",
       "0       12042       8388        1      1    1008            Greek   \n",
       "1       12054       8391        1      1    1008            Greek   \n",
       "2       12063       8393        1      1    1008            Greek   \n",
       "3       12064       8394        1      1    1008            Greek   \n",
       "4       17239       9507        0      1    1008            Greek   \n",
       "...       ...        ...      ...    ...     ...              ...   \n",
       "29245  967240    1021239        1      0     332            Greek   \n",
       "29246    5910       5438        1      1    1344            Greek   \n",
       "29247    3506       4066        1      1    1344  Demotic / Greek   \n",
       "29248    7491       6778        0      1     720  Demotic / Greek   \n",
       "29249    7491      77875        1      0    1780  Demotic / Greek   \n",
       "\n",
       "      material_text   y1   y2 remark  Unnamed: 10  \\\n",
       "0           papyrus  117  118    NaN          NaN   \n",
       "1           papyrus  119  119    NaN          NaN   \n",
       "2           papyrus   96   98    NaN          NaN   \n",
       "3           papyrus  131  131    NaN          NaN   \n",
       "4           papyrus  108  108    NaN          NaN   \n",
       "...             ...  ...  ...    ...          ...   \n",
       "29245       papyrus -263 -229    NaN          NaN   \n",
       "29246       papyrus -148 -148    NaN          NaN   \n",
       "29247       papyrus -150 -150    NaN          NaN   \n",
       "29248       papyrus -236 -236    new          NaN   \n",
       "29249       papyrus -236 -236    new          NaN   \n",
       "\n",
       "                                         Unnamed: 11  \n",
       "0           - y1 = earliest possible year of writing  \n",
       "1             - y2 = latest possible year of writing  \n",
       "2      [if y1 = y2, then we are certain of the date]  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "...                                              ...  \n",
       "29245                                            NaN  \n",
       "29246                                            NaN  \n",
       "29247                                            NaN  \n",
       "29248                                            NaN  \n",
       "29249                                            NaN  \n",
       "\n",
       "[29250 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- text_id: same as the text id in the features dataset\n",
    "- y1: The earliest possible date the text was written.\n",
    "- y2: The latest possible date the text was written.\n",
    "- If y1 and y2 are equal, the date of writing is known with certainty.\n",
    "'''\n",
    "\n",
    "labels_df = pd.read_csv('20240103_texts_with_dates.csv')\n",
    "\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot Encoding**\n",
    "\n",
    "Looking at the features file, it is one-hot encoded. One-hot encoding is a method used to represent categorical data as binary values. It is commonly used to convert non-numerical data into a format that algorithms can process effectively as well as avoiding implicit ordinal relationships. It works by identifying all unique categories and creating binary columns for all of them. We assign 1 to the column corresponding to the presence of that data and 0 for all other related columns.\n",
    "\n",
    "This file was not fully one-hot encoded at first. It had a 1 for all values, but a NULL for the absence of features. All NULL values should be filled with a 0 to represent the absence of that feature and clean the data more.\n",
    "\n",
    "After filling all NaN columns, we preview the first 5 columns of the features_df. We are setting the text_id as the index of the features_df because although it is a column, it is not actually a feature but rather a unique identifier for each row.\n",
    "\n",
    "Lastly, we print information about the shape of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       text_id  nam_id_1057.0  nam_id_19683.0  nam_id_356.0  nam_id_904.0  \\\n",
      "10827    18982            0.0             0.0           0.0           0.0   \n",
      "29437   697586            0.0             0.0           0.0           0.0   \n",
      "26140    79029            0.0             0.0           0.0           0.0   \n",
      "19775    43444            0.0             0.0           0.0           0.0   \n",
      "14320    26847            0.0             0.0           0.0           0.0   \n",
      "\n",
      "       nam_id_2227.0  nam_id_726.0  nam_id_761.0  nam_id_731.0  nam_id_1246.0  \n",
      "10827            0.0           0.0           0.0           0.0            0.0  \n",
      "29437            0.0           0.0           0.0           0.0            0.0  \n",
      "26140            0.0           0.0           0.0           0.0            0.0  \n",
      "19775            0.0           0.0           0.0           0.0            0.0  \n",
      "14320            0.0           0.0           0.0           0.0            0.0  \n",
      "Number of rows: 30324\n",
      "Number of columns: 16383\n",
      "-1292.0\n",
      "1099.0\n"
     ]
    }
   ],
   "source": [
    "# Printing information about the features dataset\n",
    "features_df = features_df.fillna(0)\n",
    "\n",
    "# gets 5 random rows, only first 10 columns\n",
    "print(features_df.sample(5).iloc[:, :10])\n",
    "\n",
    "features_df.set_index('text_id', inplace=True)\n",
    "\n",
    "\n",
    "print(f'Number of rows: {features_df.shape[0]}')\n",
    "print(f'Number of columns: {features_df.shape[1]}')\n",
    "print(labels_df['y1'].min())\n",
    "print(labels_df['y2'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Data preprocessing is the stage in machine learning where raw data is cleaned and prepared for analysis or model training. Firstly, we cleaned the data by filling NULLS with 0's for one-hot encoding. Now, we are removing duplicate rows from features_df and labels_df to ensure the dataset is clean, consistent, and free from redundancy. Duplicates can influence bias which would negatively impact the performance of our model.\n",
    "\n",
    "<font color='red'>Remove outliers?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 30324\n",
      "Number of duplicate rows: 1478\n",
      "Number of rows after removing duplicates: 28846\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the features dataframe\n",
    "print(f\"Number of rows before removing duplicates: {features_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {features_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "features_df = features_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {features_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 29250\n",
      "Number of duplicate rows: 2\n",
      "Number of rows after removing duplicates: 29248\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the labels dataframe\n",
    "print(f\"Number of rows before removing duplicates: {labels_df.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {labels_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "labels_df = labels_df.drop_duplicates()\n",
    "\n",
    "# Verify duplicates are removed\n",
    "print(f\"Number of rows after removing duplicates: {labels_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are further preparing the data by removing features with low variance. Variance refers to how much the values in a feature vary. Features with very little variation don't provide value for prediction because there aren't enough differences in the data for the model to learn any patterns. By removing these low-variance features, we simplify the model, reduce overfitting, and improve computational efficiency.\n",
    "\n",
    "The threshold we chose is arbitrary since it is a hyper parameter. We tuned our models by setting various thresholds and seeing which one resulted in the lowest MAE. A threshold of 0.001 produced our best result.\n",
    "\n",
    "fit_transform calculates the variance of each feature in features_df and removes features whose variance is below our specified threshold.\n",
    "\n",
    "Printing our original and reduced shape shows how we are able to reduce our dimensions by 15,038. There were diminishing returns with feature reduction. After a certain point, it was better to have more features. Originally, we reduced to 143 columns but 1345 performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 16383)\n",
      "Reduced shape: (28846, 1345)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove low-variance features\n",
    "selector = VarianceThreshold(threshold=0.001)  # Adjust threshold as needed\n",
    "features_reduced_df = selector.fit_transform(features_df)\n",
    "\n",
    "print(f\"Original shape: {features_df.shape}\")\n",
    "print(f\"Reduced shape: {features_reduced_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Singular Value Decomposition (SVD) for dimensionality reduction because it is good at reducing the number of features in high-dimensional datasets and sparse matrixes in particular. Our data was an extremely sparse matrix. SVD reduces the number of features while retaining the most important information. We chose 1000 principal components to keep because it performed well after testing various values for this hyperparameter. The explained variance ratio shows that even after reducing our features by over 15000, we still manage to capture 95% of the variance in 1000 components.\n",
    "\n",
    "<font color='red'>EXPLAIN SVD MORE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY SVD INSTEAD OF PCA?**\n",
    "\n",
    "We chose SVD over PCA for several reasons such as nonlinear relationships, sparse matrixes, and large datasets. SVD is more flexible in handling non-linear relationships and can be used as a general-purpose dimensionality reduction technique. We were able to determine our data was non-linear by comparing the results of models based on linear and nonlinear approaches. Our nonlinear models outperformed our linear models by far. SVD is also useful when dealing with sparse data, which is the case for our data. SVD seemed like a better option due to the size of our dataset as well. PCA involves calculating a covariance matrix which would take longer than SVD to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (28846, 1345)\n",
      "Reduced shape: (28846, 1000)\n",
      "0.9536659040503457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1000, random_state=42)\n",
    "# svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "features_svd = svd.fit_transform(features_reduced_df)\n",
    "\n",
    "print(f\"Original shape: {features_reduced_df.shape}\")\n",
    "print(f\"Reduced shape: {features_svd.shape}\")\n",
    "\n",
    "#print(svd.explained_variance_ratio_)\n",
    "print(sum(svd.explained_variance_ratio_))  # Total variance retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell converts our features_svd array back into a dataframe features_svd_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "# Assuming reduced_features is your NumPy array from SVD or Variance Thresholding\n",
    "# features_df['text_id'] contains the IDs you need to reattach\n",
    "features_df.reset_index(inplace=True)\n",
    "features_svd_df = pd.DataFrame(features_svd, columns=[f'feature_{i}' for i in range(features_svd.shape[1])])\n",
    "features_svd_df['text_id'] = features_df['text_id'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important cells because it prepares our training set. Our original label and features files have around 20k and 30k rows respectively. Not every row of data in the features dataset has a corresponding row of data in the labels dataset and vice versa. In order to train our model, we need to give it data that it already knows the answers for - the ground truths. This is the metric we will compare our predictions against to guage the performance of our models. Although we cleaned our data for duplicates before, we wanted to ensure the data was thoroughly prepared as we inch closer to training our model.\n",
    "\n",
    "After removing duplicates and data that didn't appear in both the label and features datasets, we were left with 8565 datapoints with corresponding labels. This is the data we use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text IDs missing in labels: 20281\n",
      "Text IDs missing in features: 12381\n",
      "Updated features shape: (8565, 1001)\n",
      "Updated labels shape: (13013, 12)\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 0\n",
      "Total duplicate rows: 0\n",
      "Total duplicate text_ids: 4448\n",
      "Aligned features shape: (8565, 1001)\n",
      "Aligned labels shape: (8565, 12)\n"
     ]
    }
   ],
   "source": [
    "# Get text_ids in each dataset\n",
    "features_text_ids = set(features_svd_df['text_id'])\n",
    "labels_text_ids = set(labels_df['tex_id'])\n",
    "\n",
    "# Find unmatched text_ids\n",
    "missing_in_labels = features_text_ids - labels_text_ids\n",
    "missing_in_features = labels_text_ids - features_text_ids\n",
    "\n",
    "print(f\"Text IDs missing in labels: {len(missing_in_labels)}\")\n",
    "print(f\"Text IDs missing in features: {len(missing_in_features)}\")\n",
    "\n",
    "# Keep only common text_ids\n",
    "common_text_ids = features_text_ids & labels_text_ids\n",
    "\n",
    "# Filter features and labels datasets\n",
    "features_common_df = features_svd_df[features_svd_df['text_id'].isin(common_text_ids)]\n",
    "labels_common_df = labels_df[labels_df['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "# Check the updated shapes\n",
    "print(f\"Updated features shape: {features_common_df.shape}\")\n",
    "print(f\"Updated labels shape: {labels_common_df.shape}\")\n",
    "\n",
    "# Check for duplicated rows\n",
    "duplicate_rows = features_common_df.duplicated()\n",
    "\n",
    "# Check for duplicated text_ids\n",
    "duplicate_text_ids = features_common_df['text_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_text_ids.sum()}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_label_rows = labels_common_df.duplicated()\n",
    "\n",
    "# Check for duplicate text_ids\n",
    "duplicate_label_text_ids = labels_common_df['tex_id'].duplicated()\n",
    "\n",
    "print(f\"Total duplicate rows: {duplicate_label_rows.sum()}\")\n",
    "print(f\"Total duplicate text_ids: {duplicate_label_text_ids.sum()}\")\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "features = features_common_df.drop_duplicates(subset='text_id', keep='first')\n",
    "\n",
    "# Drop duplicate text_ids, keeping the first occurrence\n",
    "labels = labels_common_df.drop_duplicates(subset='tex_id', keep='first')\n",
    "\n",
    "\n",
    "# Ensure alignment of text_ids between features and labels\n",
    "common_text_ids = set(features['text_id']) & set(labels['tex_id'])\n",
    "\n",
    "# Filter again if necessary\n",
    "features_filtered_df = features[features['text_id'].isin(common_text_ids)]\n",
    "labels_filtered_df = labels[labels['tex_id'].isin(common_text_ids)]\n",
    "\n",
    "print(f\"Aligned features shape: {features_filtered_df.shape}\")\n",
    "print(f\"Aligned labels shape: {labels_filtered_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the index of our dataframes to the text_ids since are not considered features, but rather unique identifiers of each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_df.set_index('text_id', inplace=True)\n",
    "\n",
    "labels_filtered_df.rename(columns={'tex_id': 'text_id'}, inplace=True)\n",
    "labels_filtered_df.set_index('text_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our task involves predicting the year a text was written, retaining the other ground truth labels was unnecessary. Removing them avoids confusion and ensures our process is focused solely on the data relevant to our task.\n",
    "\n",
    "<font color=\"red\"> Should we convert the other labels into features before dimension reduction?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12042</th>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12054</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12064</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17239</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703104</th>\n",
       "      <td>-250</td>\n",
       "      <td>-175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703317</th>\n",
       "      <td>-263</td>\n",
       "      <td>-229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>-148</td>\n",
       "      <td>-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>-150</td>\n",
       "      <td>-150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>-236</td>\n",
       "      <td>-236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows x 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          y1   y2\n",
       "text_id          \n",
       "12042    117  118\n",
       "12054    119  119\n",
       "12063     96   98\n",
       "12064    131  131\n",
       "17239    108  108\n",
       "...      ...  ...\n",
       "703104  -250 -175\n",
       "703317  -263 -229\n",
       "5910    -148 -148\n",
       "3506    -150 -150\n",
       "7491    -236 -236\n",
       "\n",
       "[8565 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_final_df = labels_filtered_df[['y1','y2']]\n",
    "labels_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this step is redundant, we wanted to ensure all of our datapoints were merged with a corresponding label via an inner join. This could be skipped altogether since we are separating the y label from the X features when training our model, but having it as one dataframe in the beginning aligned with our approaches throughout the course.\n",
    "\n",
    "<font color=\"red\">Can we replace the cell above where we filter for records in both features_df and labels_df by only using this method?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8565, 1002)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge on text_id\n",
    "merged_df = features_filtered_df.merge(labels_final_df, on='text_id', how='inner')\n",
    "\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification or Regression?</h1>\n",
    "\n",
    "Here we are creating two variables that indicate how many rows exist where y1=y2 and y1!=y2 and displaying those results.\n",
    "\n",
    "Since we have more rows where an exact year is known (5,239) we believe regression was the better approach because predicting an exact year (a continuous variable) aligns with the strengths of regression.\n",
    "\n",
    "If we had more cases where y1!=y2, then there would be more uncertainty about the exact date of our texts, justifying the use of a classifcation model where we simplify the task by binning the ranges into predifined categories (100-199AD, 200-299AD).\n",
    "\n",
    "Based on the context of our training data, we believed regression was the better approach because the majority of our ground truths were a single continous year vs a range of years.\n",
    "\n",
    "<b>We chose Regression</b>\n",
    "\n",
    "<font color=\"red\"> Should we create classification models anyway for comparison?</font>\n",
    "\n",
    "<font color=\"red\">Should we combine classifcation and regression models into one model?\n",
    "ex. classifcation followed by regression\n",
    "ex. regression followed by classification\n",
    "ex. multi-output neural network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where y1 = y2: 5239\n",
      "Number of rows where y1 != y2: 3326\n"
     ]
    }
   ],
   "source": [
    "equal_rows = merged_df[merged_df['y1'] == merged_df['y2']].shape[0]\n",
    "unequal_rows = merged_df[merged_df['y1'] != merged_df['y2']].shape[0]\n",
    "\n",
    "print(f\"Number of rows where y1 = y2: {equal_rows}\")\n",
    "print(f\"Number of rows where y1 != y2: {unequal_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we chose regression, we created a target column that handles both cases when y1 equals y2 (continuous) and when y1 didn't equal y2 (range of years).\n",
    "\n",
    "y_target simply became y1 if they were equal\n",
    "y_target took the midpoint of y1 and y2 if they weren't equal. The midpoint serves as a reasonable estimate for a range when an exact year isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>500.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows x 1003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_993  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...     0.006794   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.029750   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.022786   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...    -0.035404   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.002199   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...    -0.002129   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.000316   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.000879   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.005298   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.014535   \n",
       "\n",
       "         feature_994  feature_995  feature_996  feature_997  feature_998  \\\n",
       "text_id                                                                    \n",
       "1           0.075361    -0.012450     0.011055     0.047048    -0.014151   \n",
       "2          -0.032232     0.014291    -0.030107    -0.024161    -0.011760   \n",
       "3          -0.009252     0.038818    -0.002101     0.028129    -0.016734   \n",
       "4          -0.097845     0.001688     0.080584     0.043998     0.040462   \n",
       "5          -0.002162     0.007168     0.001697    -0.003452    -0.001445   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002368     0.003550     0.002539     0.004470    -0.000576   \n",
       "981644      0.002716     0.007476     0.001311     0.000043     0.009732   \n",
       "981646      0.002465     0.000985     0.000319     0.000345    -0.001285   \n",
       "981648      0.002257     0.007938    -0.000986     0.003599     0.002233   \n",
       "981666      0.005975    -0.001595     0.008034    -0.005991     0.010769   \n",
       "\n",
       "         feature_999     y1     y2  y_target  \n",
       "text_id                                       \n",
       "1          -0.024547 -124.0 -124.0    -124.0  \n",
       "2           0.020541 -112.0 -112.0    -112.0  \n",
       "3           0.019266 -109.0 -109.0    -109.0  \n",
       "4           0.092145 -108.0 -108.0    -108.0  \n",
       "5           0.011756 -106.0 -106.0    -106.0  \n",
       "...              ...    ...    ...       ...  \n",
       "981643      0.000890  548.0  548.0     548.0  \n",
       "981644     -0.003175  553.0  553.0     553.0  \n",
       "981646      0.004310  549.0  549.0     549.0  \n",
       "981648      0.001810  500.0  599.0     549.5  \n",
       "981666     -0.002167  133.0  133.0     133.0  \n",
       "\n",
       "[8565 rows x 1003 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['y_target'] = merged_df.apply(lambda row: row['y1'] if row['y1'] == row['y2'] else (row['y1'] + row['y2']) / 2, axis=1)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our new ground truth y_target, we no longer needed the labels y1 and y2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_991</th>\n",
       "      <th>feature_992</th>\n",
       "      <th>feature_993</th>\n",
       "      <th>feature_994</th>\n",
       "      <th>feature_995</th>\n",
       "      <th>feature_996</th>\n",
       "      <th>feature_997</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_999</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404936</td>\n",
       "      <td>-0.332399</td>\n",
       "      <td>0.569584</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>0.118471</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.233105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036339</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.075361</td>\n",
       "      <td>-0.012450</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>0.047048</td>\n",
       "      <td>-0.014151</td>\n",
       "      <td>-0.024547</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.194844</td>\n",
       "      <td>-0.455850</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>-0.084557</td>\n",
       "      <td>0.912922</td>\n",
       "      <td>0.938616</td>\n",
       "      <td>0.366342</td>\n",
       "      <td>-0.087693</td>\n",
       "      <td>-0.983342</td>\n",
       "      <td>-0.075471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025149</td>\n",
       "      <td>-0.023396</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.030107</td>\n",
       "      <td>-0.024161</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.669130</td>\n",
       "      <td>-0.418529</td>\n",
       "      <td>0.354021</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.230967</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>-0.131145</td>\n",
       "      <td>-0.388602</td>\n",
       "      <td>0.684473</td>\n",
       "      <td>0.321883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>-0.009252</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.016734</td>\n",
       "      <td>0.019266</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.226901</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>0.716642</td>\n",
       "      <td>-0.143475</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>0.681385</td>\n",
       "      <td>0.182460</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>-0.469498</td>\n",
       "      <td>-0.267133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>-0.035404</td>\n",
       "      <td>-0.097845</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.080584</td>\n",
       "      <td>0.043998</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403593</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>0.475889</td>\n",
       "      <td>0.177825</td>\n",
       "      <td>0.139036</td>\n",
       "      <td>-0.021976</td>\n",
       "      <td>0.124563</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>0.217894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>-0.008616</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981643</th>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.012553</td>\n",
       "      <td>-0.044002</td>\n",
       "      <td>0.041111</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.002129</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.004470</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981644</th>\n",
       "      <td>0.222981</td>\n",
       "      <td>0.390343</td>\n",
       "      <td>0.176930</td>\n",
       "      <td>-0.095436</td>\n",
       "      <td>-0.083316</td>\n",
       "      <td>0.120702</td>\n",
       "      <td>0.464449</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.487788</td>\n",
       "      <td>-0.111622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>-0.002140</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981646</th>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.009494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>-0.001285</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981648</th>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>-0.261060</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.062749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>549.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981666</th>\n",
       "      <td>0.325492</td>\n",
       "      <td>-0.261670</td>\n",
       "      <td>0.346592</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.137728</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.076177</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.001312</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8565 rows x 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "text_id                                                                     \n",
       "1         0.404936  -0.332399   0.569584   0.244467   0.118471  -0.012269   \n",
       "2         1.194844  -0.455850   0.790932  -0.084557   0.912922   0.938616   \n",
       "3         0.669130  -0.418529   0.354021   0.003984   0.230967   0.093175   \n",
       "4         1.226901  -0.607306   0.716642  -0.143475   0.932176   0.681385   \n",
       "5         0.403593  -0.306048   0.475889   0.177825   0.139036  -0.021976   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "981643    0.008250   0.016243   0.018408   0.012553  -0.044002   0.041111   \n",
       "981644    0.222981   0.390343   0.176930  -0.095436  -0.083316   0.120702   \n",
       "981646    0.019691   0.037447   0.039414   0.022553  -0.083950   0.074489   \n",
       "981648    0.072397   0.109943   0.118935   0.083189  -0.261060   0.207830   \n",
       "981666    0.325492  -0.261670   0.346592   0.218479  -0.000641  -0.137728   \n",
       "\n",
       "         feature_6  feature_7  feature_8  feature_9  ...  feature_991  \\\n",
       "text_id                                              ...                \n",
       "1         0.153942   0.019276   0.003331   0.233105  ...    -0.036339   \n",
       "2         0.366342  -0.087693  -0.983342  -0.075471  ...     0.025149   \n",
       "3        -0.131145  -0.388602   0.684473   0.321883  ...     0.015310   \n",
       "4         0.182460   0.103260  -0.469498  -0.267133  ...     0.063221   \n",
       "5         0.124563   0.050058  -0.034562   0.217894  ...     0.005871   \n",
       "...            ...        ...        ...        ...  ...          ...   \n",
       "981643    0.018783   0.001615   0.011903   0.007014  ...     0.005598   \n",
       "981644    0.464449   0.153280   0.487788  -0.111622  ...    -0.002949   \n",
       "981646    0.025522   0.007548   0.021971   0.009494  ...     0.001661   \n",
       "981648    0.096346   0.025396   0.049978   0.062749  ...    -0.001269   \n",
       "981666    0.036744   0.155500   0.076177   0.163047  ...     0.005230   \n",
       "\n",
       "         feature_992  feature_993  feature_994  feature_995  feature_996  \\\n",
       "text_id                                                                    \n",
       "1           0.028035     0.006794     0.075361    -0.012450     0.011055   \n",
       "2          -0.023396     0.029750    -0.032232     0.014291    -0.030107   \n",
       "3           0.003670     0.022786    -0.009252     0.038818    -0.002101   \n",
       "4           0.044466    -0.035404    -0.097845     0.001688     0.080584   \n",
       "5          -0.008616     0.002199    -0.002162     0.007168     0.001697   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "981643     -0.002627    -0.002129    -0.002368     0.003550     0.002539   \n",
       "981644     -0.002140    -0.000316     0.002716     0.007476     0.001311   \n",
       "981646      0.002340     0.000879     0.002465     0.000985     0.000319   \n",
       "981648      0.001269    -0.005298     0.002257     0.007938    -0.000986   \n",
       "981666     -0.001312     0.014535     0.005975    -0.001595     0.008034   \n",
       "\n",
       "         feature_997  feature_998  feature_999  y_target  \n",
       "text_id                                                   \n",
       "1           0.047048    -0.014151    -0.024547    -124.0  \n",
       "2          -0.024161    -0.011760     0.020541    -112.0  \n",
       "3           0.028129    -0.016734     0.019266    -109.0  \n",
       "4           0.043998     0.040462     0.092145    -108.0  \n",
       "5          -0.003452    -0.001445     0.011756    -106.0  \n",
       "...              ...          ...          ...       ...  \n",
       "981643      0.004470    -0.000576     0.000890     548.0  \n",
       "981644      0.000043     0.009732    -0.003175     553.0  \n",
       "981646      0.000345    -0.001285     0.004310     549.0  \n",
       "981648      0.003599     0.002233     0.001810     549.5  \n",
       "981666     -0.005991     0.010769    -0.002167     133.0  \n",
       "\n",
       "[8565 rows x 1001 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['y1','y2'])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TRAINING & COMPARING OUR MODELS</h1>\n",
    "\n",
    "This code splits our dataset into training and testing subsets to prepare it for our machine learning algorithms.\n",
    "\n",
    "X contains all of our features\n",
    "y contains our label\n",
    "\n",
    "We are then splitting the data into training and testing subsets, using 20% of the data for testing and 80% for training. We set random_sate =42 for reproducibility.\n",
    "\n",
    "The purpose of splitting our data is to evaluate how well the model generalizes to unseen data. Our training sets are used to train the model while the testing set is used afterward to assess the model's performance on data it hasn't seen during training.\n",
    "\n",
    "We did not scale our data because the original data was one-hot encoded and reducced using Variance Thresholding and SVD. Variance Thresholding only removes features with low variance and doesn't change their scales. SVD produces principle components that are linear combinations of our original features, but SVD already scales them to optimize variance. The components were normalized internally in the decomposition process of the SVD class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6852, 1000)\n",
      "(1713, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])\n",
    "y = merged_df['y_target']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LINEAR OR NONLINEAR RELATIONSHIPS?</h1>\n",
    "After training all of our models, it is apparent our the relationships in our data may be nonlinear.\n",
    "\n",
    "Our Lasso and Ridge Regression lienar models performed significantly worse. These models assume that the relationship between the features and target is linear.\n",
    "\n",
    "Our RandomForest and XGBoost ensemble nonlinear models performed far better. These models capture nonlinear relationships in the data. They work by splitting the features into smaller tress and makes decisions based on those trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LASSO</h1>\n",
    "\n",
    "We are using Lasso  linear regression model with cross validation, using 5 folds. Lasso adds an L1 regularization term to the update function. Lasso is able to shrink some coefficients to 0, basically reducing some of our features. Lasso was our poorest performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso - Best alpha: 0.10914421422151911\n",
      "Train MAE: 113.21444925516636\n",
      "Test MAE: 125.15683946474402\n",
      "Selected features by Lasso:\n",
      "feature_0       19.243864\n",
      "feature_1      285.816029\n",
      "feature_2       21.054207\n",
      "feature_3       53.341782\n",
      "feature_4     -262.346585\n",
      "                  ...    \n",
      "feature_988      0.911295\n",
      "feature_990     33.784021\n",
      "feature_995    -99.722730\n",
      "feature_996    -16.106377\n",
      "feature_999    -44.783744\n",
      "Length: 565, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Lasso with cross-validation\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Lasso - Best alpha:\", lasso.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, lasso.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, lasso.predict(X_test)))\n",
    "\n",
    "lasso_features = pd.Series(lasso.coef_, index=X.columns)\n",
    "print(\"Selected features by Lasso:\")\n",
    "print(lasso_features[lasso_features != 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RIDGE</h1>\n",
    "\n",
    "The code below performs ridge regression with cross-validation. Ridge regression adds an L2 regularization to the linear regression model, which penalizes large coefficient values and helps prevent overfitting. The strength of the regularization is controlled by the alpha parameter when instantiating the RidgeCV class. We are also performing 5-fold cross validation, which means it splits the training data into 5 parts, trains the model of 4 parts and validates on the remaing part, and repeats this process 5 times as it iterates through each fold.\n",
    "\n",
    "The mean absolute error, MAE, measures the average absolute difference between our predicted values and the ground truths. The lower the MAE, the better. After printing the MAE of our best Ridge model's best parameters, we had a Train and TEST MAE of 109.62 and 124.12. These are some of our lowest scores after comparing all of our models. The poor results on Ridge and Lasso compared to our other models suggests that our data is nonlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - Best alpha: 10.0\n",
      "Train MAE: 109.61849945515897\n",
      "Test MAE: 124.12087692614307\n",
      "Ridge coefficients:\n",
      "feature_1      282.559384\n",
      "feature_35     154.172779\n",
      "feature_459    139.881412\n",
      "feature_139    136.531244\n",
      "feature_458    113.989942\n",
      "feature_711    102.902482\n",
      "feature_635    100.589513\n",
      "feature_36      97.584438\n",
      "feature_713     97.335522\n",
      "feature_578     94.693208\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Ridge with cross-validation\n",
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Ridge - Best alpha:\", ridge.alpha_)\n",
    "print(\"Train MAE:\", mean_absolute_error(y_train, ridge.predict(X_train)))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, ridge.predict(X_test)))\n",
    "\n",
    "ridge_features = pd.Series(ridge.coef_, index=X.columns)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_features.sort_values(ascending=False).head(10))  # Top 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RANDOM FOREST</h1>\n",
    "\n",
    "This code implements a RandomForest model designed for regression tasks. It builds 100 decision tress in the \"forest\" (model).\n",
    "\n",
    "A random forest is an ensemble learning algorithm that combines multiple decision tress to improve prediction accuracy and reduce overfitting. The algorithm does something called bootstrap sampling, which randomly selects samples with replacement from the training data to create several datasets for training the individual trees we specified in the parameter. For each decision tree, the algorithm selects a random subset of features at each split. Each tree in the forest predicts a value and the final prediction is the average of all tree predictions in the forest. Random Forest is effective at solving tasks because it uses many tress instead of a single deicision tree to make an accurate prediction. This ensemble reduces variance in the predictions. It also handles complex, nonlinear relationships in the data which seems to be the case in our dataset.\n",
    "\n",
    "After 8m 15s of execution, our initial RandomForest produced results far better than our Linear and Ridge models, having a training MAE of 30.7 and test MAE of 78.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 30.740977718639943\n",
      "Random Forest - Test MAE: 78.56731082965455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, \n",
    "                                random_state=42, \n",
    "                                #Tried playing with these parameters. They get faster results, and less overfitting, but the Test MAE is worse so i've left them commented out.\n",
    "                                #max_depth=10, \n",
    "                                #min_samples_split=10, \n",
    "                                #min_samples_leaf=5,  \n",
    "                                #max_features='sqrt', \n",
    "                                n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OPTIMIZING RANDOM FOREST</h1>\n",
    "\n",
    "The RandomForest regressor was one of our highest performing models. It performed far better than Lasso and Ridge, so we wanted to improve the model by searching for better hyper parameters.\n",
    "\n",
    "We decided to use GridSearch cross validation with performs a systematic search over a range of hyperparameter combinations we assign for our RandomForestRegressor model. We are passing in parameters to control the amount of trees in the forest, the maximum depth of each tree, and the minimum number of samples required to split a node in each tree. We are also using 5 fold cross validation.\n",
    "\n",
    "After execution, we found our best parameters were:\n",
    "\n",
    "Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "\n",
    "Best Random Forest Test MAE: 92.32550495382549\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# RandomizedSearchCV doesn't like modin. convert to pandas \n",
    "X_train2 = X_train._to_pandas()\n",
    "y_train2 = y_train._to_pandas()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BEWARE!! RUNNING THIS CELL TAKES 3 HOURS TO EXECUTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "randomized_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions=param_grid, n_iter=5, cv=3, n_jobs=-1, verbose=3)\n",
    "randomized_search.fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Best Parameters:\", randomized_search.best_params_)\n",
    "print(\"Best Random Forest Test MAE:\", mean_absolute_error(y_test, randomized_search.best_estimator_.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding our best hyper parameters, we reran the RandomForestRegressor with the new hyper parameters to see our results. Expectedly, they improved. However, we wanted to explore other models to see if there was even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Train MAE: 31.50316119708191\n",
      "Random Forest - Test MAE: 77.9619964072269\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(max_depth=20, min_samples_split=5, n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Random Forest - Train MAE:\", mean_absolute_error(y_train, rf_model.predict(X_train)))\n",
    "print(\"Random Forest - Test MAE:\", mean_absolute_error(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBOOST (EXTREME GRADIENT BOOSTING)</h1>\n",
    "\n",
    "This model is a gradient boosting ensemble learning algorithm. XGBoost is an efficient and accurate gradient boosintg framework for machine learning. We experimented with a bunch of different parameters manually, slowly improving our model from 81 MAE down to 71 MAE.\n",
    "\n",
    "Boosting is an ensemble learning technique that combines weak learners (like shallow decision trees) sequentially to form a strong learner. Each new tree is trained to correct errors made by the previous shallow trees. Gradient boosting minimizes the update function by computing gradients of the loss with respect to the model's predictions. The XGBoost frameowkr in particular has regularization to prevent overfitting, is optimized for speed and low memory usage, and supports sparse data.\n",
    "\n",
    "Only taking 20 seconds to run, our XGBoost model was our most successful and most efficient compared to the 18 minutes it took our RandomForest model. We were able to adjust our model to achieve an MAE of 71.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 71.17598053324105\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize XGBRegressor with multithreading enabled\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.7, learning_rate=0.031,\n",
    "                        max_depth=7, alpha=25, n_estimators=350, n_jobs=-1)  # -1 enables multithreading\n",
    "\n",
    "# Train the model\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 value for our XGBoost model was 0.84 which indicates that 84% of the variation in our target variable was captured by the model. This indicates that the features we reduced to are highly predictive of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 76.36277432829115\n",
      "XGBoost R-squared: 0.8310994221283935\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate MAE and R-squared\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGBoost MAE: {mae}\")\n",
    "print(f\"XGBoost R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = merged_df.drop(columns=['y_target'])  # Replace with your feature dataframe\n",
    "y = merged_df['y_target']  # Replace with your target column\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255000\n",
      "[LightGBM] [Info] Number of data points in the train set: 6852, number of used features: 1000\n",
      "[LightGBM] [Info] Start training from score 117.680093\n",
      "LightGBM MAE: 70.7658086392666\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize LightGBM Regressor\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=42,\n",
    "    learning_rate=0.04,\n",
    "    n_estimators=600,\n",
    "    max_depth=-1,  # No max depth\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_lgb = lgb_reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lgb_mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "print(f\"LightGBM MAE: {lgb_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM MAE: 226.18691305100143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Create a pipeline to scale the data and apply SVR\n",
    "svm_pipeline = make_pipeline(\n",
    "    StandardScaler(),  # SVM benefits from scaling the data\n",
    "    SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Radial Basis Function kernel\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "svm_mae = mean_absolute_error(y_test, y_pred_svm)\n",
    "print(f\"SVM MAE: {svm_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_0': 322825.0625,\n",
       " 'feature_1': 9725992.0,\n",
       " 'feature_2': 3431802.0,\n",
       " 'feature_3': 1816672.125,\n",
       " 'feature_4': 3995230.25,\n",
       " 'feature_5': 2326536.25,\n",
       " 'feature_6': 742038.375,\n",
       " 'feature_7': 333515.78125,\n",
       " 'feature_8': 913658.625,\n",
       " 'feature_9': 339007.0,\n",
       " 'feature_10': 314713.0625,\n",
       " 'feature_11': 258528.5,\n",
       " 'feature_12': 599103.9375,\n",
       " 'feature_13': 1095576.625,\n",
       " 'feature_14': 777939.625,\n",
       " 'feature_15': 838467.625,\n",
       " 'feature_16': 1298503.25,\n",
       " 'feature_17': 1030746.625,\n",
       " 'feature_18': 554730.3125,\n",
       " 'feature_19': 954739.875,\n",
       " 'feature_20': 133635.640625,\n",
       " 'feature_21': 314010.0,\n",
       " 'feature_22': 954127.6875,\n",
       " 'feature_23': 343362.1875,\n",
       " 'feature_24': 409321.125,\n",
       " 'feature_25': 426316.96875,\n",
       " 'feature_26': 841948.3125,\n",
       " 'feature_27': 721909.6875,\n",
       " 'feature_28': 333732.0625,\n",
       " 'feature_29': 242103.15625,\n",
       " 'feature_30': 180934.609375,\n",
       " 'feature_31': 1924600.5,\n",
       " 'feature_32': 302842.46875,\n",
       " 'feature_33': 449117.46875,\n",
       " 'feature_34': 643965.3125,\n",
       " 'feature_35': 1148245.25,\n",
       " 'feature_36': 845407.0625,\n",
       " 'feature_37': 473056.21875,\n",
       " 'feature_38': 189297.703125,\n",
       " 'feature_39': 719446.6875,\n",
       " 'feature_40': 271797.125,\n",
       " 'feature_41': 189236.46875,\n",
       " 'feature_42': 190631.140625,\n",
       " 'feature_43': 182777.78125,\n",
       " 'feature_44': 231894.09375,\n",
       " 'feature_45': 182593.40625,\n",
       " 'feature_46': 294037.875,\n",
       " 'feature_47': 134992.921875,\n",
       " 'feature_48': 110214.3828125,\n",
       " 'feature_49': 202518.78125,\n",
       " 'feature_50': 125524.15625,\n",
       " 'feature_51': 110733.515625,\n",
       " 'feature_52': 196182.125,\n",
       " 'feature_53': 199433.859375,\n",
       " 'feature_54': 412295.6875,\n",
       " 'feature_55': 413050.5625,\n",
       " 'feature_56': 218691.890625,\n",
       " 'feature_57': 312845.53125,\n",
       " 'feature_58': 239349.3125,\n",
       " 'feature_59': 163122.640625,\n",
       " 'feature_60': 139043.921875,\n",
       " 'feature_61': 306213.40625,\n",
       " 'feature_62': 162596.109375,\n",
       " 'feature_63': 271391.78125,\n",
       " 'feature_64': 345310.34375,\n",
       " 'feature_65': 95317.7109375,\n",
       " 'feature_66': 93481.5859375,\n",
       " 'feature_67': 395381.5625,\n",
       " 'feature_68': 90862.0234375,\n",
       " 'feature_69': 102526.4921875,\n",
       " 'feature_70': 323468.25,\n",
       " 'feature_71': 229080.0625,\n",
       " 'feature_72': 377976.75,\n",
       " 'feature_73': 344982.9375,\n",
       " 'feature_74': 269439.65625,\n",
       " 'feature_75': 92378.46875,\n",
       " 'feature_76': 143335.9375,\n",
       " 'feature_77': 107269.2109375,\n",
       " 'feature_78': 147711.671875,\n",
       " 'feature_79': 88505.7890625,\n",
       " 'feature_80': 191565.953125,\n",
       " 'feature_81': 123541.109375,\n",
       " 'feature_82': 89452.8359375,\n",
       " 'feature_83': 109728.625,\n",
       " 'feature_84': 229174.21875,\n",
       " 'feature_85': 72170.9453125,\n",
       " 'feature_86': 144600.171875,\n",
       " 'feature_87': 133039.21875,\n",
       " 'feature_88': 135174.34375,\n",
       " 'feature_89': 104388.1171875,\n",
       " 'feature_90': 359612.3125,\n",
       " 'feature_91': 169527.359375,\n",
       " 'feature_92': 135113.484375,\n",
       " 'feature_93': 188133.5,\n",
       " 'feature_94': 143778.640625,\n",
       " 'feature_95': 241975.125,\n",
       " 'feature_96': 112976.6484375,\n",
       " 'feature_97': 66316.09375,\n",
       " 'feature_98': 176961.71875,\n",
       " 'feature_99': 89961.1484375,\n",
       " 'feature_100': 101076.359375,\n",
       " 'feature_101': 104922.640625,\n",
       " 'feature_102': 175313.0625,\n",
       " 'feature_103': 271746.46875,\n",
       " 'feature_104': 313869.34375,\n",
       " 'feature_105': 122353.9921875,\n",
       " 'feature_106': 106987.0390625,\n",
       " 'feature_107': 180415.84375,\n",
       " 'feature_108': 188261.53125,\n",
       " 'feature_109': 161330.21875,\n",
       " 'feature_110': 94114.453125,\n",
       " 'feature_111': 123373.484375,\n",
       " 'feature_112': 88587.8671875,\n",
       " 'feature_113': 124661.5234375,\n",
       " 'feature_114': 146588.34375,\n",
       " 'feature_115': 192283.46875,\n",
       " 'feature_116': 96461.0234375,\n",
       " 'feature_117': 201158.328125,\n",
       " 'feature_118': 205031.171875,\n",
       " 'feature_119': 286626.625,\n",
       " 'feature_120': 193362.96875,\n",
       " 'feature_121': 178897.484375,\n",
       " 'feature_122': 161509.453125,\n",
       " 'feature_123': 72899.7890625,\n",
       " 'feature_124': 434408.96875,\n",
       " 'feature_125': 290541.78125,\n",
       " 'feature_126': 37985.99609375,\n",
       " 'feature_127': 75395.5,\n",
       " 'feature_128': 180128.546875,\n",
       " 'feature_129': 170047.75,\n",
       " 'feature_130': 162326.328125,\n",
       " 'feature_131': 218495.03125,\n",
       " 'feature_132': 49470.28515625,\n",
       " 'feature_133': 79864.4921875,\n",
       " 'feature_134': 44321.0703125,\n",
       " 'feature_135': 250501.234375,\n",
       " 'feature_136': 168507.921875,\n",
       " 'feature_137': 108375.921875,\n",
       " 'feature_138': 113806.8828125,\n",
       " 'feature_139': 667920.0625,\n",
       " 'feature_140': 134307.15625,\n",
       " 'feature_141': 69758.1015625,\n",
       " 'feature_142': 135456.21875,\n",
       " 'feature_143': 95002.2890625,\n",
       " 'feature_144': 103314.875,\n",
       " 'feature_145': 109504.3515625,\n",
       " 'feature_146': 67919.859375,\n",
       " 'feature_147': 397570.96875,\n",
       " 'feature_148': 83281.7578125,\n",
       " 'feature_149': 195573.515625,\n",
       " 'feature_150': 84463.328125,\n",
       " 'feature_151': 195984.21875,\n",
       " 'feature_152': 90264.171875,\n",
       " 'feature_153': 244415.65625,\n",
       " 'feature_154': 51376.59375,\n",
       " 'feature_155': 76477.34375,\n",
       " 'feature_156': 212489.515625,\n",
       " 'feature_157': 285775.65625,\n",
       " 'feature_158': 201300.1875,\n",
       " 'feature_159': 93231.8359375,\n",
       " 'feature_160': 290658.84375,\n",
       " 'feature_161': 235408.40625,\n",
       " 'feature_162': 93771.953125,\n",
       " 'feature_163': 98502.515625,\n",
       " 'feature_164': 239294.984375,\n",
       " 'feature_165': 82960.2734375,\n",
       " 'feature_166': 20978.9609375,\n",
       " 'feature_167': 58152.08984375,\n",
       " 'feature_168': 123714.3671875,\n",
       " 'feature_169': 165128.015625,\n",
       " 'feature_170': 166795.015625,\n",
       " 'feature_171': 301104.15625,\n",
       " 'feature_172': 186215.484375,\n",
       " 'feature_173': 225696.59375,\n",
       " 'feature_174': 99458.15625,\n",
       " 'feature_175': 120619.59375,\n",
       " 'feature_176': 137152.6875,\n",
       " 'feature_177': 129909.96875,\n",
       " 'feature_178': 320035.5625,\n",
       " 'feature_179': 272567.0,\n",
       " 'feature_180': 298506.34375,\n",
       " 'feature_181': 107442.546875,\n",
       " 'feature_182': 62006.3984375,\n",
       " 'feature_183': 178709.90625,\n",
       " 'feature_184': 67857.515625,\n",
       " 'feature_185': 174389.421875,\n",
       " 'feature_186': 105332.203125,\n",
       " 'feature_187': 196814.796875,\n",
       " 'feature_188': 104228.671875,\n",
       " 'feature_189': 67627.8984375,\n",
       " 'feature_190': 78351.734375,\n",
       " 'feature_191': 79590.359375,\n",
       " 'feature_192': 140430.671875,\n",
       " 'feature_193': 140533.359375,\n",
       " 'feature_194': 159380.46875,\n",
       " 'feature_195': 218809.953125,\n",
       " 'feature_196': 128557.8515625,\n",
       " 'feature_197': 114904.09375,\n",
       " 'feature_198': 219757.3125,\n",
       " 'feature_199': 125109.0859375,\n",
       " 'feature_200': 198061.46875,\n",
       " 'feature_201': 138590.0,\n",
       " 'feature_202': 84828.4921875,\n",
       " 'feature_203': 220747.34375,\n",
       " 'feature_204': 129247.3046875,\n",
       " 'feature_205': 128952.640625,\n",
       " 'feature_206': 102030.71875,\n",
       " 'feature_207': 86973.7890625,\n",
       " 'feature_208': 74225.9609375,\n",
       " 'feature_209': 188847.4375,\n",
       " 'feature_210': 66525.015625,\n",
       " 'feature_211': 104508.8359375,\n",
       " 'feature_212': 182187.984375,\n",
       " 'feature_213': 163337.0,\n",
       " 'feature_214': 111997.9765625,\n",
       " 'feature_215': 70769.4375,\n",
       " 'feature_216': 71410.015625,\n",
       " 'feature_217': 72300.0390625,\n",
       " 'feature_218': 97853.8671875,\n",
       " 'feature_219': 99053.2109375,\n",
       " 'feature_220': 79849.921875,\n",
       " 'feature_221': 82688.828125,\n",
       " 'feature_222': 107887.1484375,\n",
       " 'feature_223': 66847.2265625,\n",
       " 'feature_224': 101783.8125,\n",
       " 'feature_225': 172184.171875,\n",
       " 'feature_226': 150244.203125,\n",
       " 'feature_227': 79334.125,\n",
       " 'feature_228': 159569.46875,\n",
       " 'feature_229': 95139.5546875,\n",
       " 'feature_230': 139352.0625,\n",
       " 'feature_231': 161195.40625,\n",
       " 'feature_232': 52623.12890625,\n",
       " 'feature_233': 170127.703125,\n",
       " 'feature_234': 147555.21875,\n",
       " 'feature_235': 115997.0078125,\n",
       " 'feature_236': 127446.7890625,\n",
       " 'feature_237': 65438.48828125,\n",
       " 'feature_238': 143970.765625,\n",
       " 'feature_239': 114546.890625,\n",
       " 'feature_240': 150823.9375,\n",
       " 'feature_241': 117411.7109375,\n",
       " 'feature_242': 66346.875,\n",
       " 'feature_243': 275175.53125,\n",
       " 'feature_244': 83012.25,\n",
       " 'feature_245': 60015.4609375,\n",
       " 'feature_246': 150562.15625,\n",
       " 'feature_247': 98773.734375,\n",
       " 'feature_248': 137145.453125,\n",
       " 'feature_249': 124237.5390625,\n",
       " 'feature_250': 57464.39453125,\n",
       " 'feature_251': 99753.765625,\n",
       " 'feature_252': 91210.7734375,\n",
       " 'feature_253': 126577.0390625,\n",
       " 'feature_254': 32398.287109375,\n",
       " 'feature_255': 99032.359375,\n",
       " 'feature_256': 220802.53125,\n",
       " 'feature_257': 70967.6484375,\n",
       " 'feature_258': 339220.625,\n",
       " 'feature_259': 71842.6875,\n",
       " 'feature_260': 67626.8359375,\n",
       " 'feature_261': 161222.140625,\n",
       " 'feature_262': 99413.1015625,\n",
       " 'feature_263': 128924.171875,\n",
       " 'feature_264': 152131.75,\n",
       " 'feature_265': 89410.65625,\n",
       " 'feature_266': 129674.421875,\n",
       " 'feature_267': 150221.015625,\n",
       " 'feature_268': 101452.7421875,\n",
       " 'feature_269': 127859.46875,\n",
       " 'feature_270': 386608.375,\n",
       " 'feature_271': 94833.0,\n",
       " 'feature_272': 79865.015625,\n",
       " 'feature_273': 153005.359375,\n",
       " 'feature_274': 52561.28515625,\n",
       " 'feature_275': 163255.796875,\n",
       " 'feature_276': 254016.21875,\n",
       " 'feature_277': 50090.4921875,\n",
       " 'feature_278': 70355.203125,\n",
       " 'feature_279': 67182.453125,\n",
       " 'feature_280': 101753.953125,\n",
       " 'feature_281': 169177.578125,\n",
       " 'feature_282': 120061.5859375,\n",
       " 'feature_283': 102577.296875,\n",
       " 'feature_284': 52984.07421875,\n",
       " 'feature_285': 88395.0703125,\n",
       " 'feature_286': 284325.96875,\n",
       " 'feature_287': 78688.6328125,\n",
       " 'feature_288': 73562.515625,\n",
       " 'feature_289': 109231.90625,\n",
       " 'feature_291': 56248.7578125,\n",
       " 'feature_292': 72062.34375,\n",
       " 'feature_293': 68139.6953125,\n",
       " 'feature_294': 84830.859375,\n",
       " 'feature_295': 142513.6875,\n",
       " 'feature_296': 131318.265625,\n",
       " 'feature_297': 67701.5546875,\n",
       " 'feature_298': 85586.8046875,\n",
       " 'feature_299': 31347.310546875,\n",
       " 'feature_300': 74483.5546875,\n",
       " 'feature_301': 62770.7890625,\n",
       " 'feature_302': 103479.46875,\n",
       " 'feature_303': 79947.0859375,\n",
       " 'feature_304': 141811.4375,\n",
       " 'feature_305': 133662.265625,\n",
       " 'feature_306': 72392.703125,\n",
       " 'feature_307': 65920.125,\n",
       " 'feature_308': 75308.4609375,\n",
       " 'feature_309': 87644.34375,\n",
       " 'feature_310': 158180.78125,\n",
       " 'feature_311': 52204.390625,\n",
       " 'feature_312': 42633.26171875,\n",
       " 'feature_313': 68886.640625,\n",
       " 'feature_314': 115019.3515625,\n",
       " 'feature_315': 122610.0703125,\n",
       " 'feature_316': 113196.0,\n",
       " 'feature_317': 81314.6171875,\n",
       " 'feature_318': 105361.9296875,\n",
       " 'feature_319': 93600.9296875,\n",
       " 'feature_320': 164972.234375,\n",
       " 'feature_321': 184249.125,\n",
       " 'feature_322': 14300.2890625,\n",
       " 'feature_323': 119112.8046875,\n",
       " 'feature_324': 92641.859375,\n",
       " 'feature_325': 136106.40625,\n",
       " 'feature_326': 89950.6953125,\n",
       " 'feature_327': 217621.59375,\n",
       " 'feature_328': 40521.796875,\n",
       " 'feature_329': 66345.7265625,\n",
       " 'feature_330': 54967.6484375,\n",
       " 'feature_331': 69486.2734375,\n",
       " 'feature_332': 56181.3046875,\n",
       " 'feature_333': 52755.94140625,\n",
       " 'feature_334': 85196.1015625,\n",
       " 'feature_335': 165135.21875,\n",
       " 'feature_336': 149754.984375,\n",
       " 'feature_337': 162345.1875,\n",
       " 'feature_338': 87752.34375,\n",
       " 'feature_339': 68069.1953125,\n",
       " 'feature_340': 121263.2265625,\n",
       " 'feature_341': 60413.11328125,\n",
       " 'feature_342': 247225.828125,\n",
       " 'feature_343': 85547.3984375,\n",
       " 'feature_344': 63294.73828125,\n",
       " 'feature_345': 366989.84375,\n",
       " 'feature_346': 117032.3828125,\n",
       " 'feature_347': 80018.2890625,\n",
       " 'feature_348': 182177.40625,\n",
       " 'feature_349': 89753.796875,\n",
       " 'feature_350': 67129.109375,\n",
       " 'feature_351': 80134.1484375,\n",
       " 'feature_352': 121517.8046875,\n",
       " 'feature_353': 66741.4296875,\n",
       " 'feature_354': 89164.8671875,\n",
       " 'feature_355': 58125.625,\n",
       " 'feature_356': 267048.34375,\n",
       " 'feature_357': 132606.75,\n",
       " 'feature_358': 102862.8203125,\n",
       " 'feature_359': 121725.25,\n",
       " 'feature_360': 82565.7265625,\n",
       " 'feature_361': 91121.75,\n",
       " 'feature_362': 59478.41015625,\n",
       " 'feature_363': 104170.9765625,\n",
       " 'feature_364': 105547.5078125,\n",
       " 'feature_365': 84296.0625,\n",
       " 'feature_366': 56325.66015625,\n",
       " 'feature_367': 291406.1875,\n",
       " 'feature_368': 216978.546875,\n",
       " 'feature_369': 181219.171875,\n",
       " 'feature_370': 130229.7109375,\n",
       " 'feature_371': 169626.40625,\n",
       " 'feature_372': 112116.421875,\n",
       " 'feature_373': 118974.9609375,\n",
       " 'feature_374': 71754.4765625,\n",
       " 'feature_375': 66854.15625,\n",
       " 'feature_376': 110670.4609375,\n",
       " 'feature_377': 78987.6328125,\n",
       " 'feature_378': 76785.875,\n",
       " 'feature_379': 87510.46875,\n",
       " 'feature_380': 79710.09375,\n",
       " 'feature_381': 94371.25,\n",
       " 'feature_382': 88685.609375,\n",
       " 'feature_383': 254370.53125,\n",
       " 'feature_384': 74537.3828125,\n",
       " 'feature_385': 86000.0390625,\n",
       " 'feature_386': 82056.7890625,\n",
       " 'feature_387': 76422.28125,\n",
       " 'feature_388': 92432.9921875,\n",
       " 'feature_389': 72255.53125,\n",
       " 'feature_390': 113571.3671875,\n",
       " 'feature_391': 163811.5,\n",
       " 'feature_392': 60747.15625,\n",
       " 'feature_393': 112711.3671875,\n",
       " 'feature_394': 56065.140625,\n",
       " 'feature_395': 136196.234375,\n",
       " 'feature_396': 71164.015625,\n",
       " 'feature_397': 146278.375,\n",
       " 'feature_398': 36985.3046875,\n",
       " 'feature_399': 113528.6796875,\n",
       " 'feature_400': 86705.0,\n",
       " 'feature_401': 116799.203125,\n",
       " 'feature_402': 100792.078125,\n",
       " 'feature_403': 91000.8359375,\n",
       " 'feature_404': 140802.296875,\n",
       " 'feature_405': 114078.25,\n",
       " 'feature_406': 90725.6328125,\n",
       " 'feature_407': 39198.2734375,\n",
       " 'feature_408': 78087.65625,\n",
       " 'feature_409': 123619.1015625,\n",
       " 'feature_410': 89978.671875,\n",
       " 'feature_411': 77001.296875,\n",
       " 'feature_412': 76410.7421875,\n",
       " 'feature_413': 95131.8671875,\n",
       " 'feature_414': 138058.8125,\n",
       " 'feature_415': 79847.2265625,\n",
       " 'feature_416': 142709.84375,\n",
       " 'feature_417': 353878.15625,\n",
       " 'feature_418': 102034.015625,\n",
       " 'feature_419': 27356.451171875,\n",
       " 'feature_420': 127465.984375,\n",
       " 'feature_421': 99020.4296875,\n",
       " 'feature_422': 2956.40283203125,\n",
       " 'feature_423': 115145.046875,\n",
       " 'feature_424': 72015.9921875,\n",
       " 'feature_425': 36569.5703125,\n",
       " 'feature_426': 89044.359375,\n",
       " 'feature_427': 103316.8203125,\n",
       " 'feature_428': 80161.7734375,\n",
       " 'feature_429': 40629.046875,\n",
       " 'feature_430': 97833.421875,\n",
       " 'feature_431': 129105.1171875,\n",
       " 'feature_432': 128095.015625,\n",
       " 'feature_433': 75907.0,\n",
       " 'feature_434': 62915.4375,\n",
       " 'feature_435': 98658.78125,\n",
       " 'feature_436': 88476.3515625,\n",
       " 'feature_437': 102364.640625,\n",
       " 'feature_438': 54457.77734375,\n",
       " 'feature_439': 83141.40625,\n",
       " 'feature_440': 269728.21875,\n",
       " 'feature_441': 70605.2734375,\n",
       " 'feature_442': 189940.65625,\n",
       " 'feature_443': 53759.6796875,\n",
       " 'feature_444': 47184.5,\n",
       " 'feature_445': 82608.6875,\n",
       " 'feature_446': 89717.8828125,\n",
       " 'feature_447': 51253.53515625,\n",
       " 'feature_448': 65892.890625,\n",
       " 'feature_449': 72209.2578125,\n",
       " 'feature_450': 132703.125,\n",
       " 'feature_451': 77415.8046875,\n",
       " 'feature_452': 105842.265625,\n",
       " 'feature_453': 79877.890625,\n",
       " 'feature_454': 81652.4140625,\n",
       " 'feature_455': 53844.421875,\n",
       " 'feature_456': 99131.7890625,\n",
       " 'feature_457': 149884.03125,\n",
       " 'feature_458': 77079.0,\n",
       " 'feature_459': 148430.625,\n",
       " 'feature_460': 70755.25,\n",
       " 'feature_461': 51525.3203125,\n",
       " 'feature_463': 200654.125,\n",
       " 'feature_464': 63667.75,\n",
       " 'feature_465': 76293.0,\n",
       " 'feature_466': 230610.0625,\n",
       " 'feature_467': 72481.234375,\n",
       " 'feature_468': 146450.0,\n",
       " 'feature_469': 101617.9140625,\n",
       " 'feature_470': 128941.4296875,\n",
       " 'feature_471': 111693.3828125,\n",
       " 'feature_472': 106238.71875,\n",
       " 'feature_473': 103160.1484375,\n",
       " 'feature_474': 124147.7421875,\n",
       " 'feature_475': 60987.8203125,\n",
       " 'feature_476': 17415.998046875,\n",
       " 'feature_477': 181146.578125,\n",
       " 'feature_478': 88252.15625,\n",
       " 'feature_479': 149515.84375,\n",
       " 'feature_480': 226769.375,\n",
       " 'feature_481': 71923.8671875,\n",
       " 'feature_482': 12776.0185546875,\n",
       " 'feature_483': 234071.78125,\n",
       " 'feature_484': 134583.46875,\n",
       " 'feature_485': 120604.296875,\n",
       " 'feature_486': 103372.71875,\n",
       " 'feature_487': 417151.875,\n",
       " 'feature_488': 84423.2578125,\n",
       " 'feature_489': 119815.4609375,\n",
       " 'feature_490': 38002.05859375,\n",
       " 'feature_491': 199867.578125,\n",
       " 'feature_492': 33357.51953125,\n",
       " 'feature_493': 280676.4375,\n",
       " 'feature_494': 96544.96875,\n",
       " 'feature_495': 93805.7734375,\n",
       " 'feature_496': 139801.5,\n",
       " 'feature_497': 227479.671875,\n",
       " 'feature_498': 92741.40625,\n",
       " 'feature_499': 60071.3359375,\n",
       " 'feature_500': 143394.171875,\n",
       " 'feature_501': 101925.2109375,\n",
       " 'feature_502': 103603.796875,\n",
       " 'feature_503': 131165.40625,\n",
       " 'feature_504': 280851.3125,\n",
       " 'feature_505': 29239.2421875,\n",
       " 'feature_506': 78344.5,\n",
       " 'feature_507': 83119.90625,\n",
       " 'feature_508': 96803.84375,\n",
       " 'feature_509': 101041.8359375,\n",
       " 'feature_510': 183895.703125,\n",
       " 'feature_511': 54983.7265625,\n",
       " 'feature_512': 88267.234375,\n",
       " 'feature_513': 206239.875,\n",
       " 'feature_514': 77100.3203125,\n",
       " 'feature_515': 142480.53125,\n",
       " 'feature_516': 75531.0390625,\n",
       " 'feature_517': 100033.03125,\n",
       " 'feature_518': 97790.25,\n",
       " 'feature_519': 193052.46875,\n",
       " 'feature_520': 124773.3203125,\n",
       " 'feature_521': 72089.9140625,\n",
       " 'feature_522': 61956.8515625,\n",
       " 'feature_523': 41285.7578125,\n",
       " 'feature_524': 103351.046875,\n",
       " 'feature_525': 111172.671875,\n",
       " 'feature_526': 80012.25,\n",
       " 'feature_527': 67169.5546875,\n",
       " 'feature_528': 59486.0078125,\n",
       " 'feature_529': 191470.546875,\n",
       " 'feature_530': 64188.73046875,\n",
       " 'feature_531': 241189.078125,\n",
       " 'feature_532': 64123.1796875,\n",
       " 'feature_533': 61285.8984375,\n",
       " 'feature_534': 142717.84375,\n",
       " 'feature_535': 106849.921875,\n",
       " 'feature_536': 99362.9609375,\n",
       " 'feature_537': 139243.890625,\n",
       " 'feature_538': 53497.8671875,\n",
       " 'feature_539': 198144.5,\n",
       " 'feature_540': 104613.3203125,\n",
       " 'feature_541': 288055.21875,\n",
       " 'feature_542': 46450.14453125,\n",
       " 'feature_543': 140503.484375,\n",
       " 'feature_544': 88700.421875,\n",
       " 'feature_545': 61756.19140625,\n",
       " 'feature_546': 94271.125,\n",
       " 'feature_547': 93908.9296875,\n",
       " 'feature_549': 60565.9609375,\n",
       " 'feature_550': 88564.78125,\n",
       " 'feature_551': 102413.125,\n",
       " 'feature_552': 87763.1875,\n",
       " 'feature_553': 106941.8203125,\n",
       " 'feature_554': 59194.9140625,\n",
       " 'feature_555': 106610.9453125,\n",
       " 'feature_556': 57995.30078125,\n",
       " 'feature_557': 235606.4375,\n",
       " 'feature_558': 55151.26171875,\n",
       " 'feature_559': 62914.6171875,\n",
       " 'feature_560': 88503.7578125,\n",
       " 'feature_561': 137375.796875,\n",
       " 'feature_562': 49668.3671875,\n",
       " 'feature_563': 95052.8671875,\n",
       " 'feature_564': 78303.8671875,\n",
       " 'feature_565': 194233.296875,\n",
       " 'feature_566': 66004.03125,\n",
       " 'feature_567': 137093.75,\n",
       " 'feature_568': 92461.125,\n",
       " 'feature_569': 108976.890625,\n",
       " 'feature_570': 86911.296875,\n",
       " 'feature_571': 45723.77734375,\n",
       " 'feature_572': 134514.84375,\n",
       " 'feature_573': 64601.37890625,\n",
       " 'feature_574': 235221.34375,\n",
       " 'feature_575': 105838.453125,\n",
       " 'feature_576': 80322.59375,\n",
       " 'feature_577': 80023.96875,\n",
       " 'feature_578': 40619.69140625,\n",
       " 'feature_579': 70960.765625,\n",
       " 'feature_580': 72044.6796875,\n",
       " 'feature_581': 101099.515625,\n",
       " 'feature_582': 54686.6484375,\n",
       " 'feature_583': 130235.453125,\n",
       " 'feature_584': 90006.25,\n",
       " 'feature_585': 121952.1171875,\n",
       " 'feature_586': 151331.296875,\n",
       " 'feature_587': 72444.71875,\n",
       " 'feature_588': 94443.1484375,\n",
       " 'feature_589': 47832.0,\n",
       " 'feature_590': 99335.1484375,\n",
       " 'feature_591': 44244.21484375,\n",
       " 'feature_592': 95139.4921875,\n",
       " 'feature_593': 160959.25,\n",
       " 'feature_594': 184938.5,\n",
       " 'feature_595': 79637.1875,\n",
       " 'feature_596': 102349.2109375,\n",
       " 'feature_597': 77638.8359375,\n",
       " 'feature_598': 125744.375,\n",
       " 'feature_599': 134686.53125,\n",
       " 'feature_600': 217177.71875,\n",
       " 'feature_601': 124595.609375,\n",
       " 'feature_602': 158407.46875,\n",
       " 'feature_603': 74136.2734375,\n",
       " 'feature_604': 47946.25390625,\n",
       " 'feature_605': 155765.875,\n",
       " 'feature_606': 96638.625,\n",
       " 'feature_607': 102178.875,\n",
       " 'feature_608': 114616.0859375,\n",
       " 'feature_609': 44201.9765625,\n",
       " 'feature_610': 76889.0390625,\n",
       " 'feature_611': 66171.375,\n",
       " 'feature_612': 50746.1171875,\n",
       " 'feature_613': 84179.71875,\n",
       " 'feature_614': 67675.6875,\n",
       " 'feature_615': 97019.90625,\n",
       " 'feature_616': 88914.7578125,\n",
       " 'feature_618': 101524.859375,\n",
       " 'feature_619': 66464.1640625,\n",
       " 'feature_620': 57991.62890625,\n",
       " 'feature_621': 83498.328125,\n",
       " 'feature_622': 54822.19921875,\n",
       " 'feature_623': 109898.9921875,\n",
       " 'feature_624': 84139.8984375,\n",
       " 'feature_625': 146609.84375,\n",
       " 'feature_626': 97033.8046875,\n",
       " 'feature_627': 86731.203125,\n",
       " 'feature_628': 123744.125,\n",
       " 'feature_629': 135561.203125,\n",
       " 'feature_630': 130018.53125,\n",
       " 'feature_631': 199123.6875,\n",
       " 'feature_632': 158001.859375,\n",
       " 'feature_633': 107570.0,\n",
       " 'feature_634': 96227.6796875,\n",
       " 'feature_635': 55528.61328125,\n",
       " 'feature_636': 136033.96875,\n",
       " 'feature_637': 72517.8359375,\n",
       " 'feature_638': 80943.1953125,\n",
       " 'feature_639': 136395.953125,\n",
       " 'feature_640': 112130.34375,\n",
       " 'feature_641': 99204.1171875,\n",
       " 'feature_642': 37225.58984375,\n",
       " 'feature_643': 130294.46875,\n",
       " 'feature_644': 77555.296875,\n",
       " 'feature_645': 162064.859375,\n",
       " 'feature_646': 116864.4609375,\n",
       " 'feature_647': 83326.296875,\n",
       " 'feature_648': 273786.625,\n",
       " 'feature_649': 98126.6484375,\n",
       " 'feature_650': 72696.0546875,\n",
       " 'feature_651': 166157.4375,\n",
       " 'feature_652': 66957.03125,\n",
       " 'feature_653': 103375.84375,\n",
       " 'feature_654': 97889.5859375,\n",
       " 'feature_655': 70584.078125,\n",
       " 'feature_656': 167701.28125,\n",
       " 'feature_658': 78645.2734375,\n",
       " 'feature_659': 65781.7734375,\n",
       " 'feature_660': 157770.59375,\n",
       " 'feature_661': 168803.578125,\n",
       " 'feature_662': 86131.5078125,\n",
       " 'feature_663': 118164.0546875,\n",
       " 'feature_664': 82392.703125,\n",
       " 'feature_665': 96668.4375,\n",
       " 'feature_666': 85587.9921875,\n",
       " 'feature_667': 79649.921875,\n",
       " 'feature_668': 49488.125,\n",
       " 'feature_669': 209376.625,\n",
       " 'feature_670': 60050.2265625,\n",
       " 'feature_671': 46227.6484375,\n",
       " 'feature_672': 58707.20703125,\n",
       " 'feature_673': 73663.140625,\n",
       " 'feature_674': 64203.83984375,\n",
       " 'feature_675': 388975.90625,\n",
       " 'feature_676': 93079.890625,\n",
       " 'feature_677': 63754.65234375,\n",
       " 'feature_678': 178029.109375,\n",
       " 'feature_679': 95834.84375,\n",
       " 'feature_680': 95325.2734375,\n",
       " 'feature_681': 76248.6171875,\n",
       " 'feature_682': 30329.9140625,\n",
       " 'feature_683': 72038.0,\n",
       " 'feature_684': 189289.84375,\n",
       " 'feature_685': 325099.125,\n",
       " 'feature_686': 63467.1015625,\n",
       " 'feature_687': 85171.0,\n",
       " 'feature_688': 111164.2890625,\n",
       " 'feature_689': 146476.109375,\n",
       " 'feature_690': 67763.8984375,\n",
       " 'feature_691': 204639.0625,\n",
       " 'feature_692': 97576.171875,\n",
       " 'feature_693': 24024.939453125,\n",
       " 'feature_694': 69033.578125,\n",
       " 'feature_695': 81574.515625,\n",
       " 'feature_696': 44040.19140625,\n",
       " 'feature_697': 106439.8125,\n",
       " 'feature_698': 109486.6953125,\n",
       " 'feature_699': 104540.28125,\n",
       " 'feature_700': 298809.59375,\n",
       " 'feature_701': 64099.01171875,\n",
       " 'feature_702': 10970.20703125,\n",
       " 'feature_703': 88289.6484375,\n",
       " 'feature_704': 86773.21875,\n",
       " 'feature_705': 68721.5234375,\n",
       " 'feature_706': 92194.3984375,\n",
       " 'feature_707': 70590.0078125,\n",
       " 'feature_708': 92513.875,\n",
       " 'feature_709': 97052.8046875,\n",
       " 'feature_710': 84665.4140625,\n",
       " 'feature_711': 80243.71875,\n",
       " 'feature_712': 71580.3125,\n",
       " 'feature_713': 55383.4375,\n",
       " 'feature_714': 54072.65234375,\n",
       " 'feature_715': 104416.1328125,\n",
       " 'feature_716': 95579.2109375,\n",
       " 'feature_717': 68335.828125,\n",
       " 'feature_718': 129291.1796875,\n",
       " 'feature_719': 71631.109375,\n",
       " 'feature_720': 168780.40625,\n",
       " 'feature_721': 191610.953125,\n",
       " 'feature_722': 56215.265625,\n",
       " 'feature_723': 61838.5625,\n",
       " 'feature_724': 94122.578125,\n",
       " 'feature_725': 101210.7265625,\n",
       " 'feature_726': 75190.546875,\n",
       " 'feature_727': 87323.9765625,\n",
       " 'feature_728': 141717.046875,\n",
       " 'feature_729': 110037.7890625,\n",
       " 'feature_730': 63837.03125,\n",
       " 'feature_731': 110491.265625,\n",
       " 'feature_732': 120560.8125,\n",
       " 'feature_733': 104064.609375,\n",
       " 'feature_734': 59873.10546875,\n",
       " 'feature_735': 75018.46875,\n",
       " 'feature_736': 136567.8125,\n",
       " 'feature_737': 86927.2734375,\n",
       " 'feature_738': 81328.5,\n",
       " 'feature_739': 139695.171875,\n",
       " 'feature_740': 144876.1875,\n",
       " 'feature_741': 104071.7890625,\n",
       " 'feature_742': 96863.2109375,\n",
       " 'feature_743': 100202.421875,\n",
       " 'feature_744': 63936.34375,\n",
       " 'feature_745': 98903.96875,\n",
       " 'feature_746': 142963.515625,\n",
       " 'feature_747': 108052.75,\n",
       " 'feature_748': 147533.984375,\n",
       " 'feature_749': 130011.453125,\n",
       " 'feature_750': 60434.49609375,\n",
       " 'feature_751': 60285.5625,\n",
       " 'feature_752': 92584.5234375,\n",
       " 'feature_753': 170033.4375,\n",
       " 'feature_754': 82139.7109375,\n",
       " 'feature_755': 43722.9921875,\n",
       " 'feature_756': 146947.15625,\n",
       " 'feature_757': 100320.203125,\n",
       " 'feature_758': 92837.0,\n",
       " 'feature_759': 115452.5078125,\n",
       " 'feature_760': 105086.4609375,\n",
       " 'feature_761': 75691.7890625,\n",
       " 'feature_762': 63942.76171875,\n",
       " 'feature_763': 355772.9375,\n",
       " 'feature_764': 78630.421875,\n",
       " 'feature_765': 103477.84375,\n",
       " 'feature_766': 78068.5390625,\n",
       " 'feature_767': 87055.4921875,\n",
       " 'feature_768': 104842.171875,\n",
       " 'feature_769': 63987.46875,\n",
       " 'feature_770': 52093.3203125,\n",
       " 'feature_771': 57159.66796875,\n",
       " 'feature_772': 95801.6484375,\n",
       " 'feature_773': 54214.98046875,\n",
       " 'feature_774': 101899.1953125,\n",
       " 'feature_775': 49972.8359375,\n",
       " 'feature_776': 54671.83984375,\n",
       " 'feature_777': 99191.515625,\n",
       " 'feature_778': 83583.0546875,\n",
       " 'feature_779': 76286.296875,\n",
       " 'feature_780': 55012.83203125,\n",
       " 'feature_781': 81178.53125,\n",
       " 'feature_782': 84691.859375,\n",
       " 'feature_783': 124747.3984375,\n",
       " 'feature_784': 70114.4375,\n",
       " 'feature_785': 73831.0078125,\n",
       " 'feature_786': 67314.8046875,\n",
       " 'feature_787': 85411.53125,\n",
       " 'feature_788': 60002.61328125,\n",
       " 'feature_789': 178442.140625,\n",
       " 'feature_790': 115885.8125,\n",
       " 'feature_791': 70932.8203125,\n",
       " 'feature_792': 30322.943359375,\n",
       " 'feature_793': 63258.234375,\n",
       " 'feature_794': 83739.1640625,\n",
       " 'feature_795': 210874.375,\n",
       " 'feature_796': 78255.7421875,\n",
       " 'feature_797': 17962.5859375,\n",
       " 'feature_798': 33638.61328125,\n",
       " 'feature_799': 94239.4453125,\n",
       " 'feature_800': 109138.34375,\n",
       " 'feature_801': 86218.34375,\n",
       " 'feature_802': 164589.59375,\n",
       " 'feature_803': 47458.625,\n",
       " 'feature_804': 67187.375,\n",
       " 'feature_806': 211293.09375,\n",
       " 'feature_807': 81788.46875,\n",
       " 'feature_808': 31790.37890625,\n",
       " 'feature_809': 110690.8671875,\n",
       " 'feature_810': 66206.921875,\n",
       " 'feature_811': 90239.25,\n",
       " 'feature_812': 179239.703125,\n",
       " 'feature_813': 89639.765625,\n",
       " 'feature_814': 96367.734375,\n",
       " 'feature_815': 141860.671875,\n",
       " 'feature_816': 68880.5,\n",
       " 'feature_817': 65975.03125,\n",
       " 'feature_818': 81810.640625,\n",
       " 'feature_819': 46078.71875,\n",
       " 'feature_820': 102973.640625,\n",
       " 'feature_821': 32639.666015625,\n",
       " 'feature_822': 70472.515625,\n",
       " 'feature_823': 95608.2890625,\n",
       " 'feature_824': 94429.4375,\n",
       " 'feature_825': 62690.4453125,\n",
       " 'feature_826': 62081.23046875,\n",
       " 'feature_827': 58968.53125,\n",
       " 'feature_828': 87260.4921875,\n",
       " 'feature_829': 28824.9453125,\n",
       " 'feature_830': 81999.6875,\n",
       " 'feature_831': 73089.6171875,\n",
       " 'feature_832': 82474.015625,\n",
       " 'feature_833': 154524.015625,\n",
       " 'feature_834': 3962.2568359375,\n",
       " 'feature_835': 74052.1640625,\n",
       " 'feature_836': 81479.5859375,\n",
       " 'feature_837': 28713.078125,\n",
       " 'feature_838': 101837.28125,\n",
       " 'feature_839': 93526.9296875,\n",
       " 'feature_840': 43435.140625,\n",
       " 'feature_841': 108401.203125,\n",
       " 'feature_842': 83877.78125,\n",
       " 'feature_843': 46323.7109375,\n",
       " 'feature_844': 72942.703125,\n",
       " 'feature_845': 146294.140625,\n",
       " 'feature_846': 181849.34375,\n",
       " 'feature_847': 87315.53125,\n",
       " 'feature_848': 177687.984375,\n",
       " 'feature_849': 72148.421875,\n",
       " 'feature_850': 152753.5,\n",
       " 'feature_851': 89626.390625,\n",
       " 'feature_852': 86392.3984375,\n",
       " 'feature_853': 51763.921875,\n",
       " 'feature_854': 69572.1875,\n",
       " 'feature_855': 135947.71875,\n",
       " 'feature_856': 147944.078125,\n",
       " 'feature_857': 65672.5546875,\n",
       " 'feature_858': 80536.5078125,\n",
       " 'feature_859': 84885.3984375,\n",
       " 'feature_860': 240359.890625,\n",
       " 'feature_861': 63444.89453125,\n",
       " 'feature_862': 57185.25,\n",
       " 'feature_863': 128693.578125,\n",
       " 'feature_864': 54221.51953125,\n",
       " 'feature_865': 105143.125,\n",
       " 'feature_866': 123459.53125,\n",
       " 'feature_867': 63409.23828125,\n",
       " 'feature_868': 105126.9296875,\n",
       " 'feature_869': 117049.6953125,\n",
       " 'feature_870': 175618.921875,\n",
       " 'feature_871': 140327.59375,\n",
       " 'feature_872': 120505.2421875,\n",
       " 'feature_873': 183938.515625,\n",
       " 'feature_874': 135646.859375,\n",
       " 'feature_875': 53107.140625,\n",
       " 'feature_876': 81987.6015625,\n",
       " 'feature_877': 113680.046875,\n",
       " 'feature_878': 66296.578125,\n",
       " 'feature_879': 69249.2734375,\n",
       " 'feature_880': 208898.03125,\n",
       " 'feature_881': 60655.98046875,\n",
       " 'feature_882': 113688.4453125,\n",
       " 'feature_883': 43194.32421875,\n",
       " 'feature_884': 118138.484375,\n",
       " 'feature_885': 102646.328125,\n",
       " 'feature_886': 72357.3359375,\n",
       " 'feature_887': 62077.1953125,\n",
       " 'feature_888': 52089.62890625,\n",
       " 'feature_889': 73952.4453125,\n",
       " 'feature_890': 130494.234375,\n",
       " 'feature_891': 62379.36328125,\n",
       " 'feature_892': 85888.3984375,\n",
       " 'feature_893': 64826.96875,\n",
       " 'feature_894': 89520.4375,\n",
       " 'feature_895': 80978.3984375,\n",
       " 'feature_896': 499911.0,\n",
       " 'feature_897': 68032.984375,\n",
       " 'feature_898': 39698.3984375,\n",
       " 'feature_899': 99379.4140625,\n",
       " 'feature_900': 100228.2265625,\n",
       " 'feature_901': 111803.234375,\n",
       " 'feature_902': 71385.1875,\n",
       " 'feature_903': 254327.25,\n",
       " 'feature_904': 133348.265625,\n",
       " 'feature_905': 69171.609375,\n",
       " 'feature_906': 107338.2109375,\n",
       " 'feature_907': 42082.94140625,\n",
       " 'feature_908': 56953.67578125,\n",
       " 'feature_909': 63318.0,\n",
       " 'feature_910': 88083.921875,\n",
       " 'feature_911': 39633.71484375,\n",
       " 'feature_912': 21603.791015625,\n",
       " 'feature_913': 153908.453125,\n",
       " 'feature_914': 57462.14453125,\n",
       " 'feature_916': 161589.3125,\n",
       " 'feature_917': 60399.6875,\n",
       " 'feature_918': 95819.09375,\n",
       " 'feature_919': 91405.46875,\n",
       " 'feature_920': 102193.75,\n",
       " 'feature_921': 77012.96875,\n",
       " 'feature_922': 183472.515625,\n",
       " 'feature_923': 138409.1875,\n",
       " 'feature_924': 146687.625,\n",
       " 'feature_925': 350385.09375,\n",
       " 'feature_926': 155272.53125,\n",
       " 'feature_927': 152889.859375,\n",
       " 'feature_928': 120033.75,\n",
       " 'feature_929': 56561.43359375,\n",
       " 'feature_930': 97751.2578125,\n",
       " 'feature_931': 134082.28125,\n",
       " 'feature_932': 144131.875,\n",
       " 'feature_933': 90053.1875,\n",
       " 'feature_934': 132969.53125,\n",
       " 'feature_935': 130986.125,\n",
       " 'feature_936': 81737.359375,\n",
       " 'feature_937': 95023.875,\n",
       " 'feature_938': 228560.0,\n",
       " 'feature_939': 86444.015625,\n",
       " 'feature_940': 52236.171875,\n",
       " 'feature_941': 65226.22265625,\n",
       " 'feature_942': 79369.203125,\n",
       " 'feature_943': 99345.65625,\n",
       " 'feature_944': 149607.40625,\n",
       " 'feature_945': 19602.5078125,\n",
       " 'feature_946': 89051.6328125,\n",
       " 'feature_947': 126486.6796875,\n",
       " 'feature_948': 30953.017578125,\n",
       " 'feature_949': 36492.7265625,\n",
       " 'feature_950': 87462.5703125,\n",
       " 'feature_951': 102212.5625,\n",
       " 'feature_952': 81511.875,\n",
       " 'feature_953': 135445.40625,\n",
       " 'feature_954': 55221.546875,\n",
       " 'feature_955': 46463.3125,\n",
       " 'feature_956': 107617.46875,\n",
       " 'feature_957': 52369.11328125,\n",
       " 'feature_958': 76211.1875,\n",
       " 'feature_959': 83428.578125,\n",
       " 'feature_960': 55011.7578125,\n",
       " 'feature_961': 59287.62890625,\n",
       " 'feature_962': 101947.6640625,\n",
       " 'feature_963': 242202.375,\n",
       " 'feature_964': 49748.6953125,\n",
       " 'feature_965': 73934.6015625,\n",
       " 'feature_966': 35946.7265625,\n",
       " 'feature_967': 82840.2578125,\n",
       " 'feature_968': 147591.015625,\n",
       " 'feature_969': 87597.2890625,\n",
       " 'feature_970': 62848.66796875,\n",
       " 'feature_971': 182354.40625,\n",
       " 'feature_972': 146784.96875,\n",
       " 'feature_973': 133935.640625,\n",
       " 'feature_974': 98428.3671875,\n",
       " 'feature_975': 118410.3203125,\n",
       " 'feature_976': 128142.03125,\n",
       " 'feature_977': 192177.890625,\n",
       " 'feature_978': 84830.8125,\n",
       " 'feature_979': 211061.46875,\n",
       " 'feature_980': 77129.6328125,\n",
       " 'feature_981': 74401.9375,\n",
       " 'feature_982': 62101.6953125,\n",
       " 'feature_983': 87658.2109375,\n",
       " 'feature_984': 52474.6875,\n",
       " 'feature_985': 424139.5625,\n",
       " 'feature_986': 176397.03125,\n",
       " 'feature_987': 65694.375,\n",
       " 'feature_988': 313060.46875,\n",
       " 'feature_989': 149875.84375,\n",
       " 'feature_990': 60605.76171875,\n",
       " 'feature_991': 100853.8203125,\n",
       " 'feature_992': 117556.8984375,\n",
       " 'feature_993': 153625.828125,\n",
       " 'feature_994': 65659.4765625,\n",
       " 'feature_995': 210801.3125,\n",
       " 'feature_996': 51739.0546875,\n",
       " 'feature_997': 144607.625,\n",
       " 'feature_998': 276696.75,\n",
       " 'feature_999': 101479.890625}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.get_booster().get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Add\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow doesn't like the modin datasets, so we convert them to pandas here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train._to_pandas()\n",
    "y_train2 = y_train._to_pandas()\n",
    "\n",
    "X_test2 = X_test._to_pandas()\n",
    "y_test2 = y_test._to_pandas()\n",
    "\n",
    "# Convert to NumPy array if it's a Series\n",
    "X_train2 = X_train2.to_numpy() if isinstance(X_train2, pd.DataFrame) else X_train2\n",
    "X_test2 = X_test2.to_numpy() if isinstance(X_test2, pd.DataFrame) else X_test2\n",
    "y_train2 = y_train2.to_numpy() if isinstance(y_train2, pd.Series) else y_train2\n",
    "y_test2 = y_test2.to_numpy() if isinstance(y_test2, pd.Series) else y_test2\n",
    "\n",
    "# Create scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = MinMaxScaler()  # Using MinMaxScaler for target to bound values\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = scaler_X.fit_transform(X_train2)\n",
    "X_test_scaled = scaler_X.transform(X_test2)\n",
    "\n",
    "# Scale target to [0, 1] range\n",
    "y_train_scaled = scaler_y.fit_transform(y_train2.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test2.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(input_dim, learning_rate=1e-4):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(5e-5))(input_layer)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(5e-5))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    # Match the dimensions of residual to x (Dense layer should output 64 units)\n",
    "    residual = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(5e-5))(x)\n",
    "    residual = tf.keras.layers.BatchNormalization()(residual)\n",
    "\n",
    "    # Add the residual connection\n",
    "    output = Add()([x, residual])\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(output)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate \n",
    "            #,clipvalue=1.0 #does this help? not sure.\n",
    "        ),\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_file_name, epochs=250, learning_rate=1e-4, batch_size=32):\n",
    "    # Create and train the model\n",
    "    #model = load_model(file_name)# create_model(X_train_scaled.shape[1])\n",
    "    if model_file_name is not None and os.path.exists(model_file_name):\n",
    "        #import the model\n",
    "        model = load_model(model_file_name) # type: ignore\n",
    "    else:\n",
    "        # Create the model\n",
    "        model = create_model(X_train_scaled.shape[1], learning_rate)\n",
    "\n",
    "\n",
    "    # Early stopping and learning rate reduction\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=25, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=1e-8\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file_name, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        validation_split=0.2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(model_file_name)\n",
    "    \n",
    "    return model, history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, history):\n",
    "    # Predict and inverse transform to get original scale\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # Evaluate on original scale\n",
    "    mae = np.mean(np.abs(y_pred.flatten() - y_test2))\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 4s 17ms/step - loss: 0.0034 - mae: 0.0378 - val_loss: 0.0044 - val_mae: 0.0399 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "file_name=\"models/date_predictor-v16.h5\"\n",
    "model, history = train(file_name, epochs=150, learning_rate=1e-4, batch_size=32)\n",
    "evaluate_model(model, history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
